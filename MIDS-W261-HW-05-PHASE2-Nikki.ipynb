{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS - w261 Machine Learning At Scale\n",
    "__Course Lead:__ Dr James G. Shanahan (__email__ Jimi via  James.Shanahan _AT_ gmail.com)\n",
    "\n",
    "## Assignment - HW5\n",
    "\n",
    "\n",
    "---\n",
    "__Name:__  Carlos Castro   \n",
    "__Class:__ MIDS w261 (Section 2, e.g., Fall 2016 Group 1)     \n",
    "__Email:__  carlosscastro@iSchool.Berkeley.edu     \n",
    "__Week:__   5\n",
    "\n",
    "__Due Time:__ 2 Phases. \n",
    "\n",
    "* __HW5 Phase 1__ \n",
    "This can be done on a local machine (with a unit test on the cloud such as AltaScale's PaaS or on AWS) and is due Tuesday, Week 6 by 8AM (West coast time). It will primarily focus on building a unit/systems and for pairwise similarity calculations pipeline (for stripe documents)\n",
    "\n",
    "* __HW5 Phase 2__ \n",
    "This will require the AltaScale cluster and will be due Tuesday, Week 7 by 8AM (West coast time). \n",
    "The focus of  HW5 Phase 2  will be to scale up the unit/systems tests to the Google 5 gram corpus. This will be a group exercise \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.13 | packaged by conda-forge | (default, May  2 2017, 12:48:11) \\n[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents <a name=\"TOC\"></a> \n",
    "\n",
    "1.  [HW Instructions](#1)   \n",
    "2.  [HW References](#2)\n",
    "3.  [HW Problems](#3)   \n",
    "       \n",
    "    5.4.  [HW5.4](#5.4)    \n",
    "    5.5.  [HW5.5](#5.5)    \n",
    "    5.6.  [HW5.6](#5.6)    \n",
    "    5.7.  [HW5.7](#5.7)    \n",
    "    5.8.  [HW5.8](#5.8)    \n",
    "    5.9.  [HW5.9](#5.9)    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "# 1 Instructions\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "MIDS UC Berkeley, Machine Learning at Scale   \n",
    "DATSCIW261 ASSIGNMENT #5\n",
    "\n",
    "Version 2017-9-2 \n",
    "\n",
    "\n",
    "### IMPORTANT\n",
    "\n",
    "This homework must be completed in the cloud \n",
    "\n",
    "### === INSTRUCTIONS for SUBMISSIONS ===   \n",
    "Follow the instructions for submissions carefully.\n",
    "\n",
    "Each student has a `HW-<user>` repository for all assignments.   \n",
    "\n",
    "Click this link to enable you to create a github repo within the MIDS261 Classroom:   \n",
    "https://classroom.github.com/assignment-invitations/3b1d6c8e58351209f9dd865537111ff8   \n",
    "and follow the instructions to create a HW repo.\n",
    "\n",
    "Push the following to your HW github repo into the master branch:\n",
    "* Your local HW5 directory. Your repo file structure should look like this:\n",
    "\n",
    "```\n",
    "HW-<user>\n",
    "    --HW3\n",
    "       |__MIDS-W261-HW-03-<Student_id>.ipynb\n",
    "       |__MIDS-W261-HW-03-<Student_id>.pdf\n",
    "       |__some other hw3 file\n",
    "    --HW4\n",
    "       |__MIDS-W261-HW-04-<Student_id>.ipynb\n",
    "       |__MIDS-W261-HW-04-<Student_id>.pdf\n",
    "       |__some other hw4 file\n",
    "    etc..\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\">\n",
    "# 2 Useful References\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "* See async and live lectures for this week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\">\n",
    "# 3 HW Problems\n",
    "[Back to Table of Contents](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5.4\"></a> \n",
    "# PHASE 2\n",
    "----------\n",
    "\n",
    "# HW 5.4   \n",
    "## Full-scale experiment on Google N-gram data on the CLOUD\n",
    "__ Once you are happy with your test results __ proceed to generating  your results on the Google n-grams dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.4.0  <a name=\"5.4.0\"></a> Run systems tests on the CLOUD  (PHASE 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Repeat HW5.3.0 (using the same small data sources that were used in HW5.3.0) on ** the cloud** (e.g., AltaScale / AWS/ SoftLayer/ Azure). Make sure all tests give correct results! Good luck out there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing buildStripes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile buildStripes.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import re\n",
    "import mrjob\n",
    "import json\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import itertools\n",
    "import collections\n",
    "import logging\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "class MRbuildStripes(MRJob):\n",
    "  \n",
    "    #START SUDENT CODE531_STRIPES\n",
    "  \n",
    "    MRJob.SORT_VALUES = True\n",
    "    \n",
    "    #def mapper_init(self):\n",
    "    #    return self.start_time = time.time()\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        splits = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "\n",
    "        words = splits[0].lower().split()\n",
    "        count = splits[1]\n",
    "\n",
    "        H = {}\n",
    "        for subset in itertools.combinations(sorted(set(words)), 2):\n",
    "            \n",
    "            # Process combinations in sorted order, i.e. \"hello\",\"tomorrow\"\n",
    "            if subset[0] not in H.keys():\n",
    "                H[subset[0]] = {}\n",
    "                H[subset[0]][subset[1]] = count \n",
    "            elif subset[1] not in H[subset[0]]:\n",
    "                H[subset[0]][subset[1]] = count\n",
    "            else:\n",
    "                H[subset[0]][subset[1]] += count\n",
    "\n",
    "            # Obtain combinations in reverse order, to consider them both ways\n",
    "            # TODO: Should refactor this and the block above, shameless copy-paste\n",
    "            if subset[1] not in H.keys():\n",
    "                H[subset[1]] = {}\n",
    "                H[subset[1]][subset[0]] = count \n",
    "            elif subset[0] not in H[subset[1]]:\n",
    "                H[subset[1]][subset[0]] = count\n",
    "            else:\n",
    "                H[subset[1]][subset[0]] += count\n",
    "        for key in H.keys():\n",
    "            #print \"%s\\t%s\" % (key, json.dumps(H[key]))\n",
    "            yield key, H[key]\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        \n",
    "        counter = {}\n",
    "\n",
    "        for value in values:\n",
    "            \n",
    "            for k, v in value.iteritems():\n",
    "                if k in counter:\n",
    "                    counter[k] += int(v)\n",
    "                else:\n",
    "                    counter[k] = int(v)\n",
    "        \n",
    "        yield key, counter\n",
    "        \n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "\n",
    "            MRStep(#mapper_init=self.mapper_init\n",
    "                   #,\n",
    "                   mapper=self.mapper\n",
    "                   ,\n",
    "                   reducer=self.reducer\n",
    "                  )\n",
    "            ]\n",
    "  #END SUDENT CODE531_STRIPES\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    MRbuildStripes.run()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"\n",
    "    Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing invertedIndex.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile invertedIndex.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "from __future__ import division\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "\n",
    "class MRinvertedIndex(MRJob):\n",
    "    \n",
    "    #START SUDENT CODE531_INV_INDEX\n",
    "  \n",
    "    def mapper(self, _, line):\n",
    "        key, stripeJson = line.strip().split('\\t')\n",
    "        key = key.strip(\"\\\"\")\n",
    "        stripe = json.loads(stripeJson)\n",
    "        \n",
    "        for k, v in stripe.iteritems():\n",
    "            yield k, [key, len(stripe)]\n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "\n",
    "        table = {}\n",
    "        for value in values:\n",
    "            table[value[0]] = value[1]\n",
    "            \n",
    "        yield key, table\n",
    "        \n",
    "    #END SUDENT CODE531_INV_INDEX\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    MRinvertedIndex.run() \n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"\n",
    "    Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing similarity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile similarity.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "#import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import time\n",
    "import logging\n",
    "\n",
    "class MRsimilarity(MRJob):\n",
    "  \n",
    "    #START SUDENT CODE531_SIMILARITY\n",
    "\n",
    "    MRJob.SORT_VALUES = True\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        key, valuesJson = line.strip().split('\\t')\n",
    "        key = key.strip(\"\\\"\")\n",
    "        values = json.loads(valuesJson)\n",
    "\n",
    "        for pair in itertools.combinations(sorted(set(values)), 2):\n",
    "            yield pair, [values[pair[0]], values[pair[1]]]\n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "        intersection = 0\n",
    "        count1 = None\n",
    "        count2 = None\n",
    "        \n",
    "        cosine = 0.0\n",
    "        \n",
    "        # Iterate through the values\n",
    "        for value in values:\n",
    "            # Jaccard, get counts for the intersection, and for each set\n",
    "            intersection += 1\n",
    "            if count1 == None:\n",
    "                count1 = value[0]\n",
    "                count2 = value[1]\n",
    "        \n",
    "            # Cosine\n",
    "            a = 1 / math.sqrt(value[0])\n",
    "            b = 1 / math.sqrt(value[1])\n",
    "            cosine += a * b\n",
    "            \n",
    "        jaccard = float(intersection) / float(count1 + count2 - intersection)\n",
    "        \n",
    "        overlap_coefficient = float(intersection) / min(count1, count2)\n",
    "        \n",
    "        dice_coefficient = float(2 * intersection) / (count1 + count2)\n",
    "        \n",
    "        average = (cosine + jaccard + overlap_coefficient + dice_coefficient) / 4.0\n",
    "        \n",
    "        yield average, [key[0] + ' - ' + key[1], cosine, jaccard, overlap_coefficient, dice_coefficient]\n",
    "            \n",
    "    #END SUDENT CODE531_SIMILARITY\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    MRsimilarity.run()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"\n",
    "    Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\n",
    "A BILL FOR ESTABLISHING RELIGIOUS\t59\t59\t54\n",
    "A Biography of General George\t92\t90\t74\n",
    "A Case Study in Government\t102\t102\t78\n",
    "A Case Study of Female\t447\t447\t327\n",
    "A Case Study of Limited\t55\t55\t43\n",
    "A Child's Christmas in Wales\t1099\t1061\t866\n",
    "A Circumstantial Narrative of the\t62\t62\t50\n",
    "A City by the Sea\t62\t60\t49\n",
    "A Collection of Fairy Tales\t123\t117\t80\n",
    "A Collection of Forms of\t116\t103\t82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing atlas-boon-systems-test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile atlas-boon-systems-test.txt\n",
    "atlas boon\t50\t50\t50\n",
    "boon cava dipped\t10\t10\t10\n",
    "atlas dipped\t15\t15\t15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build stripes for mini-test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `systems_test_stripes_1': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/buildStripes.nhaas.20170618.181106.189023\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nhaas/tmp/mrjob/buildStripes.nhaas.20170618.181106.189023/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob379253321590105247.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1425\n",
      "  Submitted application application_1493936954640_1425\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1425/\n",
      "  Running job: job_1493936954640_1425\n",
      "  Job job_1493936954640_1425 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1425 completed successfully\n",
      "  Output directory: hdfs:///user/nhaas/tmp/mrjob/buildStripes.nhaas.20170618.181106.189023/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=563\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2118\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1042\n",
      "\t\tFILE: Number of bytes written=400887\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1007\n",
      "\t\tHDFS: Number of bytes written=2118\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=12624384\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=59059200\n",
      "\t\tTotal time spent by all map tasks (ms)=8219\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=24657\n",
      "\t\tTotal time spent by all reduce tasks (ms)=23070\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=115350\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8219\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=23070\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2650\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=117\n",
      "\t\tInput split bytes=444\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=3024\n",
      "\t\tMap output materialized bytes=1047\n",
      "\t\tMap output records=49\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1904566272\n",
      "\t\tReduce input groups=49\n",
      "\t\tReduce input records=49\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=1047\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=98\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7760502784\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nhaas/tmp/mrjob/buildStripes.nhaas.20170618.181106.189023/output...\n",
      "Removing HDFS temp directory hdfs:///user/nhaas/tmp/mrjob/buildStripes.nhaas.20170618.181106.189023...\n",
      "Removing temp directory /tmp/buildStripes.nhaas.20170618.181106.189023...\n",
      "WARNING:root:\n",
      "    Elapsed time: 90.4111568928 seconds\n",
      "    In minutes: 1.50685261488 mins\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# Make Stripes from ngrams for systems test 1\n",
    "###########################################################################\n",
    "\n",
    "!hdfs dfs -rm -r systems_test_stripes_1\n",
    "!python buildStripes.py -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt \\\n",
    "    --python-bin=./my_env/bin/python \\\n",
    "    --archive=\"hdfs:///user/nhaas/virtualenv/my_env.zip#my_env\" \\\n",
    "    > systems_test_stripes_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"a\"\t{\"limited\":55,\"female\":447,\"general\":92,\"sea\":62,\"in\":1201,\"religious\":59,\"george\":92,\"biography\":92,\"city\":62,\"for\":59,\"tales\":123,\"child's\":1099,\"forms\":116,\"wales\":1099,\"christmas\":1099,\"government\":102,\"collection\":239,\"by\":62,\"case\":604,\"circumstantial\":62,\"fairy\":123,\"of\":895,\"study\":604,\"bill\":59,\"establishing\":59,\"narrative\":62,\"the\":124}\r\n",
      "\"bill\"\t{\"a\":59,\"religious\":59,\"for\":59,\"establishing\":59}\r\n",
      "\"biography\"\t{\"a\":92,\"of\":92,\"george\":92,\"general\":92}\r\n",
      "\"by\"\t{\"a\":62,\"city\":62,\"the\":62,\"sea\":62}\r\n",
      "\"case\"\t{\"a\":604,\"limited\":55,\"government\":102,\"of\":502,\"study\":604,\"female\":447,\"in\":102}\r\n",
      "\"child's\"\t{\"a\":1099,\"wales\":1099,\"christmas\":1099,\"in\":1099}\r\n",
      "\"christmas\"\t{\"a\":1099,\"wales\":1099,\"in\":1099,\"child's\":1099}\r\n",
      "\"circumstantial\"\t{\"a\":62,\"of\":62,\"the\":62,\"narrative\":62}\r\n",
      "\"city\"\t{\"a\":62,\"the\":62,\"by\":62,\"sea\":62}\r\n",
      "\"collection\"\t{\"a\":239,\"forms\":116,\"fairy\":123,\"tales\":123,\"of\":239}\r\n",
      "\"establishing\"\t{\"a\":59,\"bill\":59,\"religious\":59,\"for\":59}\r\n",
      "\"fairy\"\t{\"a\":123,\"of\":123,\"tales\":123,\"collection\":123}\r\n",
      "\"female\"\t{\"a\":447,\"case\":447,\"study\":447,\"of\":447}\r\n",
      "\"for\"\t{\"a\":59,\"bill\":59,\"religious\":59,\"establishing\":59}\r\n",
      "\"forms\"\t{\"a\":116,\"of\":116,\"collection\":116}\r\n",
      "\"general\"\t{\"a\":92,\"of\":92,\"george\":92,\"biography\":92}\r\n",
      "\"george\"\t{\"a\":92,\"of\":92,\"biography\":92,\"general\":92}\r\n",
      "\"government\"\t{\"a\":102,\"case\":102,\"study\":102,\"in\":102}\r\n",
      "\"in\"\t{\"a\":1201,\"case\":102,\"government\":102,\"study\":102,\"child's\":1099,\"wales\":1099,\"christmas\":1099}\r\n",
      "\"limited\"\t{\"a\":55,\"case\":55,\"study\":55,\"of\":55}\r\n",
      "\"narrative\"\t{\"a\":62,\"of\":62,\"the\":62,\"circumstantial\":62}\r\n",
      "\"of\"\t{\"a\":895,\"case\":502,\"circumstantial\":62,\"george\":92,\"limited\":55,\"tales\":123,\"collection\":239,\"the\":62,\"forms\":116,\"female\":447,\"narrative\":62,\"fairy\":123,\"general\":92,\"study\":502,\"biography\":92}\r\n",
      "\"religious\"\t{\"a\":59,\"bill\":59,\"for\":59,\"establishing\":59}\r\n",
      "\"sea\"\t{\"a\":62,\"city\":62,\"the\":62,\"by\":62}\r\n",
      "\"study\"\t{\"a\":604,\"case\":604,\"limited\":55,\"government\":102,\"of\":502,\"female\":447,\"in\":102}\r\n",
      "\"tales\"\t{\"a\":123,\"of\":123,\"fairy\":123,\"collection\":123}\r\n",
      "\"the\"\t{\"a\":124,\"city\":62,\"circumstantial\":62,\"of\":62,\"sea\":62,\"narrative\":62,\"by\":62}\r\n",
      "\"wales\"\t{\"a\":1099,\"in\":1099,\"christmas\":1099,\"child's\":1099}\r\n"
     ]
    }
   ],
   "source": [
    "!cat systems_test_stripes_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `systems_test_stripes_2': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/buildStripes.nhaas.20170618.181323.003601\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nhaas/tmp/mrjob/buildStripes.nhaas.20170618.181323.003601/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob8102273425500910119.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1426\n",
      "  Submitted application application_1493936954640_1426\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1426/\n",
      "  Running job: job_1493936954640_1426\n",
      "  Job job_1493936954640_1426 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1426 completed successfully\n",
      "  Output directory: hdfs:///user/nhaas/tmp/mrjob/buildStripes.nhaas.20170618.181323.003601/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=101\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=147\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=137\n",
      "\t\tFILE: Number of bytes written=398998\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=471\n",
      "\t\tHDFS: Number of bytes written=147\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=24574464\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=11079680\n",
      "\t\tTotal time spent by all map tasks (ms)=15999\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=47997\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4328\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=21640\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=15999\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4328\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2740\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=91\n",
      "\t\tInput split bytes=370\n",
      "\t\tMap input records=3\n",
      "\t\tMap output bytes=204\n",
      "\t\tMap output materialized bytes=171\n",
      "\t\tMap output records=7\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1903550464\n",
      "\t\tReduce input groups=7\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=4\n",
      "\t\tReduce shuffle bytes=171\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=14\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7768756224\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nhaas/tmp/mrjob/buildStripes.nhaas.20170618.181323.003601/output...\n",
      "Removing HDFS temp directory hdfs:///user/nhaas/tmp/mrjob/buildStripes.nhaas.20170618.181323.003601...\n",
      "Removing temp directory /tmp/buildStripes.nhaas.20170618.181323.003601...\n",
      "WARNING:root:\n",
      "    Elapsed time: 75.2788469791 seconds\n",
      "    In minutes: 1.25464744965 mins\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# Make Stripes from ngrams for systems test 2\n",
    "###########################################################################\n",
    "\n",
    "!hdfs dfs -rm -r systems_test_stripes_2\n",
    "!python buildStripes.py -r hadoop atlas-boon-systems-test.txt \\\n",
    "    --python-bin=./my_env/bin/python \\\n",
    "    --archive=\"hdfs:///user/nhaas/virtualenv/my_env.zip#my_env\" \\\n",
    "    > systems_test_stripes_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"atlas\"\t{\"dipped\":15,\"boon\":50}\r\n",
      "\"boon\"\t{\"atlas\":50,\"dipped\":10,\"cava\":10}\r\n",
      "\"cava\"\t{\"dipped\":10,\"boon\":10}\r\n",
      "\"dipped\"\t{\"atlas\":15,\"boon\":10,\"cava\":10}\r\n"
     ]
    }
   ],
   "source": [
    "!cat systems_test_stripes_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"DocA\"\t{\"X\":20, \"Y\":30, \"Z\":5}\r\n",
      "\"DocB\"\t{\"X\":100, \"Y\":20}\r\n",
      "\"DocC\"\t{\"M\":5, \"N\":20, \"Z\":5, \"Y\":1}\r\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# Stripes for systems test 3 (given, no need to build stripes)\n",
    "########################################################################\n",
    "\n",
    "with open(\"systems_test_stripes_3\", \"w\") as f:\n",
    "    f.writelines([\n",
    "        '\"DocA\"\\t{\"X\":20, \"Y\":30, \"Z\":5}\\n',\n",
    "        '\"DocB\"\\t{\"X\":100, \"Y\":20}\\n',  \n",
    "        '\"DocC\"\\t{\"M\":5, \"N\":20, \"Z\":5, \"Y\":1}\\n'\n",
    "    ])\n",
    "!cat systems_test_stripes_3  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted indices for mini-test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/invertedIndex.nhaas.20170618.181534.271499\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nhaas/tmp/mrjob/invertedIndex.nhaas.20170618.181534.271499/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob8909789328552553258.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1427\n",
      "  Submitted application application_1493936954640_1427\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1427/\n",
      "  Running job: job_1493936954640_1427\n",
      "  Job job_1493936954640_1427 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Task Id : attempt_1493936954640_1427_r_000000_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@76fb0ef5 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1427 completed successfully\n",
      "  Output directory: hdfs:///user/nhaas/tmp/mrjob/invertedIndex.nhaas.20170618.181534.271499/output\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3177\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1904\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1386\n",
      "\t\tFILE: Number of bytes written=400238\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3539\n",
      "\t\tHDFS: Number of bytes written=1904\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tFailed reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=23697408\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=25144320\n",
      "\t\tTotal time spent by all map tasks (ms)=15428\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=46284\n",
      "\t\tTotal time spent by all reduce tasks (ms)=9822\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=49110\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=15428\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=9822\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2820\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=74\n",
      "\t\tInput split bytes=362\n",
      "\t\tMap input records=28\n",
      "\t\tMap output bytes=3150\n",
      "\t\tMap output materialized bytes=1581\n",
      "\t\tMap output records=158\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1883164672\n",
      "\t\tReduce input groups=28\n",
      "\t\tReduce input records=158\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=1581\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=316\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7715913728\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nhaas/tmp/mrjob/invertedIndex.nhaas.20170618.181534.271499/output...\n",
      "Removing HDFS temp directory hdfs:///user/nhaas/tmp/mrjob/invertedIndex.nhaas.20170618.181534.271499...\n",
      "Removing temp directory /tmp/invertedIndex.nhaas.20170618.181534.271499...\n",
      "WARNING:root:\n",
      "    Elapsed time: 74.7592201233 seconds\n",
      "    In minutes: 1.24598700205 mins\n"
     ]
    }
   ],
   "source": [
    "!python invertedIndex.py -r hadoop systems_test_stripes_1 \\\n",
    "    --python-bin=./my_env/bin/python \\\n",
    "    --archive=\"hdfs:///user/nhaas/virtualenv/my_env.zip#my_env\" \\\n",
    "    > systems_test_index_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/invertedIndex.nhaas.20170618.182111.478195\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nhaas/tmp/mrjob/invertedIndex.nhaas.20170618.182111.478195/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob3137388167757883052.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1428\n",
      "  Submitted application application_1493936954640_1428\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1428/\n",
      "  Running job: job_1493936954640_1428\n",
      "  Job job_1493936954640_1428 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "  Task Id : attempt_1493936954640_1428_m_000001_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@3b768f13 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1428_m_000000_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@3b768f13 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1428_m_000001_1, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@2519d039 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1428 completed successfully\n",
      "  Output directory: hdfs:///user/nhaas/tmp/mrjob/invertedIndex.nhaas.20170618.182111.478195/output\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=221\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=137\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=141\n",
      "\t\tFILE: Number of bytes written=397595\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=583\n",
      "\t\tHDFS: Number of bytes written=137\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=3\n",
      "\t\tLaunched map tasks=5\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tOther local map tasks=3\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=27767808\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=22515200\n",
      "\t\tTotal time spent by all map tasks (ms)=18078\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=54234\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8795\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=43975\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=18078\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=8795\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2810\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=100\n",
      "\t\tInput split bytes=362\n",
      "\t\tMap input records=4\n",
      "\t\tMap output bytes=196\n",
      "\t\tMap output materialized bytes=183\n",
      "\t\tMap output records=10\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1904197632\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=4\n",
      "\t\tReduce shuffle bytes=183\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=20\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7773630464\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nhaas/tmp/mrjob/invertedIndex.nhaas.20170618.182111.478195/output...\n",
      "Removing HDFS temp directory hdfs:///user/nhaas/tmp/mrjob/invertedIndex.nhaas.20170618.182111.478195...\n",
      "Removing temp directory /tmp/invertedIndex.nhaas.20170618.182111.478195...\n",
      "WARNING:root:\n",
      "    Elapsed time: 68.2414050102 seconds\n",
      "    In minutes: 1.13735675017 mins\n"
     ]
    }
   ],
   "source": [
    "!python invertedIndex.py -r hadoop systems_test_stripes_2 \\\n",
    "    --python-bin=./my_env/bin/python \\\n",
    "    --archive=\"hdfs:///user/nhaas/virtualenv/my_env.zip#my_env\" \\\n",
    "    > systems_test_index_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/invertedIndex.nhaas.20170618.182248.299673\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nhaas/tmp/mrjob/invertedIndex.nhaas.20170618.182248.299673/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob9135878876131524131.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1429\n",
      "  Submitted application application_1493936954640_1429\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1429/\n",
      "  Running job: job_1493936954640_1429\n",
      "  Job job_1493936954640_1429 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1429 completed successfully\n",
      "  Output directory: hdfs:///user/nhaas/tmp/mrjob/invertedIndex.nhaas.20170618.182248.299673/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=140\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=111\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=92\n",
      "\t\tFILE: Number of bytes written=397488\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=502\n",
      "\t\tHDFS: Number of bytes written=111\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=23972352\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=11896320\n",
      "\t\tTotal time spent by all map tasks (ms)=15607\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=46821\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4647\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=23235\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=15607\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4647\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2530\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=79\n",
      "\t\tInput split bytes=362\n",
      "\t\tMap input records=3\n",
      "\t\tMap output bytes=135\n",
      "\t\tMap output materialized bytes=125\n",
      "\t\tMap output records=9\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1898024960\n",
      "\t\tReduce input groups=5\n",
      "\t\tReduce input records=9\n",
      "\t\tReduce output records=5\n",
      "\t\tReduce shuffle bytes=125\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=18\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7764602880\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nhaas/tmp/mrjob/invertedIndex.nhaas.20170618.182248.299673/output...\n",
      "Removing HDFS temp directory hdfs:///user/nhaas/tmp/mrjob/invertedIndex.nhaas.20170618.182248.299673...\n",
      "Removing temp directory /tmp/invertedIndex.nhaas.20170618.182248.299673...\n",
      "WARNING:root:\n",
      "    Elapsed time: 58.7929999828 seconds\n",
      "    In minutes: 0.979883333047 mins\n"
     ]
    }
   ],
   "source": [
    "!python invertedIndex.py -r hadoop systems_test_stripes_3 \\\n",
    "    --python-bin=./my_env/bin/python \\\n",
    "    --archive=\"hdfs:///user/nhaas/virtualenv/my_env.zip#my_env\" \\\n",
    "    > systems_test_index_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  1  - Inverted Index\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "             \"a\" |          bill 4 |     biography 4 |            by 4\n",
      "          \"bill\" |            a 27 |  establishing 4 |           for 4\n",
      "     \"biography\" |            a 27 |       general 4 |        george 4\n",
      "            \"by\" |            a 27 |          city 4 |           sea 4\n",
      "          \"case\" |            a 27 |        female 4 |    government 4\n",
      "       \"child's\" |            a 27 |     christmas 4 |            in 7\n",
      "     \"christmas\" |            a 27 |       child's 4 |            in 7\n",
      "\"circumstantial\" |            a 27 |     narrative 4 |           of 15\n",
      "          \"city\" |            a 27 |            by 4 |           sea 4\n",
      "    \"collection\" |            a 27 |         fairy 4 |         forms 3\n",
      "  \"establishing\" |            a 27 |          bill 4 |           for 4\n",
      "         \"fairy\" |            a 27 |    collection 5 |           of 15\n",
      "        \"female\" |            a 27 |          case 7 |           of 15\n",
      "           \"for\" |            a 27 |          bill 4 |  establishing 4\n",
      "         \"forms\" |            a 27 |    collection 5 |           of 15\n",
      "       \"general\" |            a 27 |     biography 4 |        george 4\n",
      "        \"george\" |            a 27 |     biography 4 |       general 4\n",
      "    \"government\" |            a 27 |          case 7 |            in 7\n",
      "            \"in\" |            a 27 |          case 7 |       child's 4\n",
      "       \"limited\" |            a 27 |          case 7 |           of 15\n",
      "     \"narrative\" |            a 27 |circumstantial 4 |           of 15\n",
      "            \"of\" |            a 27 |     biography 4 |          case 7\n",
      "     \"religious\" |            a 27 |          bill 4 |  establishing 4\n",
      "           \"sea\" |            a 27 |            by 4 |          city 4\n",
      "         \"study\" |            a 27 |          case 7 |        female 4\n",
      "         \"tales\" |            a 27 |    collection 5 |         fairy 4\n",
      "           \"the\" |            a 27 |            by 4 |circumstantial 4\n",
      "         \"wales\" |            a 27 |       child's 4 |     christmas 4\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  2  - Inverted Index\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "         \"atlas\" |          boon 3 |        dipped 3 |                \n",
      "          \"boon\" |         atlas 2 |          cava 2 |        dipped 3\n",
      "          \"cava\" |          boon 3 |        dipped 3 |                \n",
      "        \"dipped\" |         atlas 2 |          boon 3 |          cava 2\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  3  - Inverted Index\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "             \"M\" |          DocC 4 |                 |                \n",
      "             \"N\" |          DocC 4 |                 |                \n",
      "             \"X\" |          DocA 3 |          DocB 2 |                \n",
      "             \"Y\" |          DocA 3 |          DocB 2 |          DocC 4\n",
      "             \"Z\" |          DocA 3 |          DocC 4 |                \n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "# Pretty print systems tests for generating Inverted Index\n",
    "##########################################################\n",
    "import json\n",
    "\n",
    "for i in range(1,4):\n",
    "    print \"—\"*100\n",
    "    print \"Systems test \",i,\" - Inverted Index\"\n",
    "    print \"—\"*100  \n",
    "    with open(\"systems_test_index_\"+str(i),\"r\") as f:\n",
    "        lines = sorted(f.readlines())\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            word, doc_list = line.split(\"\\t\")\n",
    "            doc_dict = json.loads(doc_list)\n",
    "            stripe=[]\n",
    "            for doc in doc_dict:\n",
    "                stripe.append([doc, doc_dict[doc]])\n",
    "            stripe=sorted(stripe)\n",
    "            stripe.extend([[\"\",\"\"] for _ in xrange(3 - len(stripe))])\n",
    "\n",
    "            print \"{0:>16} |{1:>16} |{2:>16} |{3:>16}\".format(\n",
    "              (word), stripe[0][0]+\" \"+str(stripe[0][1]), stripe[1][0]+\" \"+str(stripe[1][1]), stripe[2][0]+\" \"+str(stripe[2][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarities for mini-test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/similarity.nhaas.20170618.182758.359377\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nhaas/tmp/mrjob/similarity.nhaas.20170618.182758.359377/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob6799646071741313713.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1430\n",
      "  Submitted application application_1493936954640_1430\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1430/\n",
      "  Running job: job_1493936954640_1430\n",
      "  Job job_1493936954640_1430 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1430 completed successfully\n",
      "  Output directory: hdfs:///user/nhaas/tmp/mrjob/similarity.nhaas.20170618.182758.359377/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2856\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=25050\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3654\n",
      "\t\tFILE: Number of bytes written=407058\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3206\n",
      "\t\tHDFS: Number of bytes written=25050\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=12802560\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=12067840\n",
      "\t\tTotal time spent by all map tasks (ms)=8335\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=25005\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4714\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=23570\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8335\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4714\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3120\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=105\n",
      "\t\tInput split bytes=350\n",
      "\t\tMap input records=28\n",
      "\t\tMap output bytes=17893\n",
      "\t\tMap output materialized bytes=4825\n",
      "\t\tMap output records=673\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1909723136\n",
      "\t\tReduce input groups=378\n",
      "\t\tReduce input records=673\n",
      "\t\tReduce output records=378\n",
      "\t\tReduce shuffle bytes=4825\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=1346\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7752478720\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nhaas/tmp/mrjob/similarity.nhaas.20170618.182758.359377/output...\n",
      "Removing HDFS temp directory hdfs:///user/nhaas/tmp/mrjob/similarity.nhaas.20170618.182758.359377...\n",
      "Removing temp directory /tmp/similarity.nhaas.20170618.182758.359377...\n",
      "WARNING:root:\n",
      "    Elapsed time: 57.3262798786 seconds\n",
      "    In minutes: 0.955437997977 mins\n"
     ]
    }
   ],
   "source": [
    "!python similarity.py -r hadoop systems_test_index_1 \\\n",
    "    --python-bin=./my_env/bin/python \\\n",
    "    --archive=\"hdfs:///user/nhaas/virtualenv/my_env.zip#my_env\" \\\n",
    "    > systems_test_similarities_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/similarity.nhaas.20170618.182924.708778\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nhaas/tmp/mrjob/similarity.nhaas.20170618.182924.708778/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob8368095286776363002.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1431\n",
      "  Submitted application application_1493936954640_1431\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1431/\n",
      "  Running job: job_1493936954640_1431\n",
      "  Job job_1493936954640_1431 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1431 completed successfully\n",
      "  Output directory: hdfs:///user/nhaas/tmp/mrjob/similarity.nhaas.20170618.182924.708778/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=206\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=330\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=122\n",
      "\t\tFILE: Number of bytes written=398890\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=556\n",
      "\t\tHDFS: Number of bytes written=330\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=23993856\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=11586560\n",
      "\t\tTotal time spent by all map tasks (ms)=15621\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=46863\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4526\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=22630\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=15621\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4526\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2660\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=124\n",
      "\t\tInput split bytes=350\n",
      "\t\tMap input records=4\n",
      "\t\tMap output bytes=196\n",
      "\t\tMap output materialized bytes=180\n",
      "\t\tMap output records=8\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1908109312\n",
      "\t\tReduce input groups=6\n",
      "\t\tReduce input records=8\n",
      "\t\tReduce output records=6\n",
      "\t\tReduce shuffle bytes=180\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=16\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7774474240\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nhaas/tmp/mrjob/similarity.nhaas.20170618.182924.708778/output...\n",
      "Removing HDFS temp directory hdfs:///user/nhaas/tmp/mrjob/similarity.nhaas.20170618.182924.708778...\n",
      "Removing temp directory /tmp/similarity.nhaas.20170618.182924.708778...\n",
      "WARNING:root:\n",
      "    Elapsed time: 60.3380069733 seconds\n",
      "    In minutes: 1.00563344955 mins\n"
     ]
    }
   ],
   "source": [
    "!python similarity.py -r hadoop systems_test_index_2 \\\n",
    "    --python-bin=./my_env/bin/python \\\n",
    "    --archive=\"hdfs:///user/nhaas/virtualenv/my_env.zip#my_env\" \\\n",
    "    > systems_test_similarities_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/similarity.nhaas.20170618.183035.822188\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nhaas/tmp/mrjob/similarity.nhaas.20170618.183035.822188/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob4534815851229239523.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1432\n",
      "  Submitted application application_1493936954640_1432\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1432/\n",
      "  Running job: job_1493936954640_1432\n",
      "  Job job_1493936954640_1432 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1432 completed successfully\n",
      "  Output directory: hdfs:///user/nhaas/tmp/mrjob/similarity.nhaas.20170618.183035.822188/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=167\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=197\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=74\n",
      "\t\tFILE: Number of bytes written=398759\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=517\n",
      "\t\tHDFS: Number of bytes written=197\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=24390144\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=21701120\n",
      "\t\tTotal time spent by all map tasks (ms)=15879\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=47637\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8477\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=42385\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=15879\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=8477\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2850\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=83\n",
      "\t\tInput split bytes=350\n",
      "\t\tMap input records=5\n",
      "\t\tMap output bytes=115\n",
      "\t\tMap output materialized bytes=106\n",
      "\t\tMap output records=5\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1902333952\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce input records=5\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=106\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=10\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7740493824\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nhaas/tmp/mrjob/similarity.nhaas.20170618.183035.822188/output...\n",
      "Removing HDFS temp directory hdfs:///user/nhaas/tmp/mrjob/similarity.nhaas.20170618.183035.822188...\n",
      "Removing temp directory /tmp/similarity.nhaas.20170618.183035.822188...\n",
      "WARNING:root:\n",
      "    Elapsed time: 62.6306290627 seconds\n",
      "    In minutes: 1.04384381771 mins\n"
     ]
    }
   ],
   "source": [
    "!python similarity.py -r hadoop systems_test_index_3\\\n",
    "    --python-bin=./my_env/bin/python \\\n",
    "    --archive=\"hdfs:///user/nhaas/virtualenv/my_env.zip#my_env\" \\\n",
    "    > systems_test_similarities_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  1  - Similarity measures\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "        average |           pair |         cosine |        jaccard |        overlap |           dice\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "       0.334842 |       a - bill |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |  a - biography |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |         a - by |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.465201 |       a - case |       0.436436 |       0.214286 |       0.857143 |       0.352941\n",
      "       0.334842 |    a - child's |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |  a - christmas |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |a - circumstantial |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |       a - city |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.384281 | a - collection |       0.344265 |       0.142857 |       0.800000 |       0.250000\n",
      "       0.334842 |a - establishing |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |      a - fairy |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |     a - female |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |        a - for |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.273413 |      a - forms |       0.222222 |       0.071429 |       0.666667 |       0.133333\n",
      "       0.334842 |    a - general |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |     a - george |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 | a - government |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.465201 |         a - in |       0.436436 |       0.214286 |       0.857143 |       0.352941\n",
      "       0.334842 |    a - limited |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |  a - narrative |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.698916 |         a - of |       0.695666 |       0.500000 |       0.933333 |       0.666667\n",
      "       0.334842 |  a - religious |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.334842 |        a - sea |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.465201 |      a - study |       0.436436 |       0.214286 |       0.857143 |       0.352941\n",
      "       0.334842 |      a - tales |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.465201 |        a - the |       0.436436 |       0.214286 |       0.857143 |       0.352941\n",
      "       0.334842 |      a - wales |       0.288675 |       0.107143 |       0.750000 |       0.193548\n",
      "       0.223214 |bill - biography |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |      bill - by |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |    bill - case |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 | bill - child's |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |bill - christmas |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |bill - circumstantial |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |    bill - city |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.205207 |bill - collection |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.712500 |bill - establishing |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.223214 |   bill - fairy |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |  bill - female |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.712500 |     bill - for |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.268597 |   bill - forms |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.223214 | bill - general |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |  bill - george |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |bill - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |      bill - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 | bill - limited |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |bill - narrative |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.134980 |      bill - of |       0.129099 |       0.055556 |       0.250000 |       0.105263\n",
      "       0.712500 |bill - religious |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.223214 |     bill - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |   bill - study |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |   bill - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |     bill - the |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |   bill - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 | biography - by |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.365956 |biography - case |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 |biography - child's |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |biography - christmas |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 |biography - circumstantial |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.223214 |biography - city |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.419343 |biography - collection |       0.447214 |       0.285714 |       0.500000 |       0.444444\n",
      "       0.223214 |biography - establishing |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 |biography - fairy |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.458333 |biography - female |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.223214 |biography - for |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.553861 |biography - forms |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
      "       0.712500 |biography - general |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.712500 |biography - george |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.223214 |biography - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 | biography - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.458333 |biography - limited |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.458333 |biography - narrative |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.410147 | biography - of |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.223214 |biography - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |biography - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.365956 |biography - study |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.458333 |biography - tales |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.365956 |biography - the |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 |biography - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |      by - case |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |   by - child's |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 | by - christmas |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 |by - circumstantial |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.712500 |      by - city |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.205207 |by - collection |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.223214 |by - establishing |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |     by - fairy |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |    by - female |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |       by - for |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.268597 |     by - forms |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.223214 |   by - general |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |    by - george |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |by - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |        by - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |   by - limited |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 | by - narrative |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.271593 |        by - of |       0.258199 |       0.117647 |       0.500000 |       0.210526\n",
      "       0.223214 | by - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.712500 |       by - sea |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.180200 |     by - study |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |     by - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.559350 |       by - the |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.223214 |     by - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.365956 | case - child's |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.365956 |case - christmas |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.365956 |case - circumstantial |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.180200 |    case - city |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.317849 |case - collection |       0.338062 |       0.200000 |       0.400000 |       0.333333\n",
      "       0.180200 |case - establishing |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.365956 |   case - fairy |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.559350 |  case - female |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.180200 |     case - for |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.438276 |   case - forms |       0.436436 |       0.250000 |       0.666667 |       0.400000\n",
      "       0.365956 | case - general |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.365956 |  case - george |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.559350 |case - government |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.389610 |      case - in |       0.428571 |       0.272727 |       0.428571 |       0.428571\n",
      "       0.559350 | case - limited |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.365956 |case - narrative |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.386912 |      case - of |       0.390360 |       0.222222 |       0.571429 |       0.363636\n",
      "       0.180200 |case - religious |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.180200 |     case - sea |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.830357 |   case - study |       0.857143 |       0.750000 |       0.857143 |       0.857143\n",
      "       0.365956 |   case - tales |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.255952 |     case - the |       0.285714 |       0.166667 |       0.285714 |       0.285714\n",
      "       0.365956 |   case - wales |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.712500 |child's - christmas |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.223214 |child's - circumstantial |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 | child's - city |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.205207 |child's - collection |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.223214 |child's - establishing |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |child's - fairy |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |child's - female |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |  child's - for |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.268597 |child's - forms |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.223214 |child's - general |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |child's - george |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 |child's - government |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.559350 |   child's - in |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.223214 |child's - limited |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |child's - narrative |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.134980 |   child's - of |       0.129099 |       0.055556 |       0.250000 |       0.105263\n",
      "       0.223214 |child's - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |  child's - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.365956 |child's - study |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 |child's - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |  child's - the |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.712500 |child's - wales |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.223214 |christmas - circumstantial |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |christmas - city |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.205207 |christmas - collection |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.223214 |christmas - establishing |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |christmas - fairy |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |christmas - female |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |christmas - for |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.268597 |christmas - forms |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.223214 |christmas - general |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |christmas - george |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 |christmas - government |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.559350 | christmas - in |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.223214 |christmas - limited |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |christmas - narrative |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.134980 | christmas - of |       0.129099 |       0.055556 |       0.250000 |       0.105263\n",
      "       0.223214 |christmas - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |christmas - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.365956 |christmas - study |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 |christmas - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |christmas - the |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.712500 |christmas - wales |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.458333 |circumstantial - city |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.419343 |circumstantial - collection |       0.447214 |       0.285714 |       0.500000 |       0.444444\n",
      "       0.223214 |circumstantial - establishing |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 |circumstantial - fairy |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.458333 |circumstantial - female |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.223214 |circumstantial - for |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.553861 |circumstantial - forms |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
      "       0.458333 |circumstantial - general |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.458333 |circumstantial - george |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.223214 |circumstantial - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |circumstantial - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.458333 |circumstantial - limited |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.712500 |circumstantial - narrative |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.410147 |circumstantial - of |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.223214 |circumstantial - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 |circumstantial - sea |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.365956 |circumstantial - study |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.458333 |circumstantial - tales |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.559350 |circumstantial - the |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.223214 |circumstantial - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.205207 |city - collection |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.223214 |city - establishing |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |   city - fairy |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |  city - female |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |     city - for |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.268597 |   city - forms |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.223214 | city - general |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |  city - george |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |city - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |      city - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 | city - limited |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 |city - narrative |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.271593 |      city - of |       0.258199 |       0.117647 |       0.500000 |       0.210526\n",
      "       0.223214 |city - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.712500 |     city - sea |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.180200 |   city - study |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |   city - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.559350 |     city - the |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.223214 |   city - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.205207 |collection - establishing |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.646872 |collection - fairy |       0.670820 |       0.500000 |       0.750000 |       0.666667\n",
      "       0.419343 |collection - female |       0.447214 |       0.285714 |       0.500000 |       0.444444\n",
      "       0.205207 |collection - for |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.504099 |collection - forms |       0.516398 |       0.333333 |       0.666667 |       0.500000\n",
      "       0.419343 |collection - general |       0.447214 |       0.285714 |       0.500000 |       0.444444\n",
      "       0.419343 |collection - george |       0.447214 |       0.285714 |       0.500000 |       0.444444\n",
      "       0.205207 |collection - government |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.156652 |collection - in |       0.169031 |       0.090909 |       0.200000 |       0.166667\n",
      "       0.419343 |collection - limited |       0.447214 |       0.285714 |       0.500000 |       0.444444\n",
      "       0.419343 |collection - narrative |       0.447214 |       0.285714 |       0.500000 |       0.444444\n",
      "       0.477970 |collection - of |       0.461880 |       0.250000 |       0.800000 |       0.400000\n",
      "       0.205207 |collection - religious |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.205207 |collection - sea |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.317849 |collection - study |       0.338062 |       0.200000 |       0.400000 |       0.333333\n",
      "       0.646872 |collection - tales |       0.670820 |       0.500000 |       0.750000 |       0.666667\n",
      "       0.317849 |collection - the |       0.338062 |       0.200000 |       0.400000 |       0.333333\n",
      "       0.205207 |collection - wales |       0.223607 |       0.125000 |       0.250000 |       0.222222\n",
      "       0.223214 |establishing - fairy |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |establishing - female |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.712500 |establishing - for |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.268597 |establishing - forms |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.223214 |establishing - general |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |establishing - george |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |establishing - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |establishing - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |establishing - limited |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |establishing - narrative |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.134980 |establishing - of |       0.129099 |       0.055556 |       0.250000 |       0.105263\n",
      "       0.712500 |establishing - religious |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.223214 |establishing - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |establishing - study |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |establishing - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |establishing - the |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |establishing - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 | fairy - female |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.223214 |    fairy - for |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.868292 |  fairy - forms |       0.866025 |       0.750000 |       1.000000 |       0.857143\n",
      "       0.458333 |fairy - general |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.458333 | fairy - george |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.223214 |fairy - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |     fairy - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.458333 |fairy - limited |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.458333 |fairy - narrative |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.410147 |     fairy - of |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.223214 |fairy - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |    fairy - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.365956 |  fairy - study |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.712500 |  fairy - tales |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.365956 |    fairy - the |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 |  fairy - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |   female - for |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.553861 | female - forms |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
      "       0.458333 |female - general |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.458333 |female - george |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.712500 |female - government |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.559350 |    female - in |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       1.000000 |female - limited |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "       0.458333 |female - narrative |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.410147 |    female - of |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.223214 |female - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |   female - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.559350 | female - study |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.458333 | female - tales |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.365956 |   female - the |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 | female - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.268597 |    for - forms |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.223214 |  for - general |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |   for - george |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |for - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |       for - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |  for - limited |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |for - narrative |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.134980 |       for - of |       0.129099 |       0.055556 |       0.250000 |       0.105263\n",
      "       0.712500 |for - religious |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.223214 |      for - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |    for - study |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |    for - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |      for - the |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |    for - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.553861 |forms - general |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
      "       0.553861 | forms - george |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
      "       0.268597 |forms - government |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.215666 |     forms - in |       0.218218 |       0.111111 |       0.333333 |       0.200000\n",
      "       0.553861 |forms - limited |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
      "       0.553861 |forms - narrative |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
      "       0.328008 |     forms - of |       0.298142 |       0.125000 |       0.666667 |       0.222222\n",
      "       0.268597 |forms - religious |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.268597 |    forms - sea |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.438276 |  forms - study |       0.436436 |       0.250000 |       0.666667 |       0.400000\n",
      "       0.868292 |  forms - tales |       0.866025 |       0.750000 |       1.000000 |       0.857143\n",
      "       0.438276 |    forms - the |       0.436436 |       0.250000 |       0.666667 |       0.400000\n",
      "       0.268597 |  forms - wales |       0.288675 |       0.166667 |       0.333333 |       0.285714\n",
      "       0.712500 |general - george |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.223214 |general - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |   general - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.458333 |general - limited |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.458333 |general - narrative |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.410147 |   general - of |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.223214 |general - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |  general - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.365956 |general - study |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.458333 |general - tales |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.365956 |  general - the |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 |general - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |george - government |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |    george - in |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.458333 |george - limited |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.458333 |george - narrative |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.410147 |    george - of |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.223214 |george - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |   george - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.365956 | george - study |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.458333 | george - tales |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.365956 |   george - the |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 | george - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.559350 |government - in |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.712500 |government - limited |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
      "       0.223214 |government - narrative |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.410147 |government - of |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.223214 |government - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |government - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.559350 |government - study |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.223214 |government - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |government - the |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.458333 |government - wales |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.559350 |   in - limited |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.180200 | in - narrative |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.287991 |        in - of |       0.292770 |       0.157895 |       0.428571 |       0.272727\n",
      "       0.180200 | in - religious |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.180200 |       in - sea |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.389610 |     in - study |       0.428571 |       0.272727 |       0.428571 |       0.428571\n",
      "       0.180200 |     in - tales |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.126374 |       in - the |       0.142857 |       0.076923 |       0.142857 |       0.142857\n",
      "       0.559350 |     in - wales |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.458333 |limited - narrative |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.410147 |   limited - of |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.223214 |limited - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.223214 |  limited - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.559350 |limited - study |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.458333 |limited - tales |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.365956 |  limited - the |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 |limited - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.410147 | narrative - of |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.223214 |narrative - religious |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.458333 |narrative - sea |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.365956 |narrative - study |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.458333 |narrative - tales |       0.500000 |       0.333333 |       0.500000 |       0.500000\n",
      "       0.559350 |narrative - the |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.223214 |narrative - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.134980 | of - religious |       0.129099 |       0.055556 |       0.250000 |       0.105263\n",
      "       0.271593 |       of - sea |       0.258199 |       0.117647 |       0.500000 |       0.210526\n",
      "       0.386912 |     of - study |       0.390360 |       0.222222 |       0.571429 |       0.363636\n",
      "       0.410147 |     of - tales |       0.387298 |       0.187500 |       0.750000 |       0.315789\n",
      "       0.287991 |       of - the |       0.292770 |       0.157895 |       0.428571 |       0.272727\n",
      "       0.134980 |     of - wales |       0.129099 |       0.055556 |       0.250000 |       0.105263\n",
      "       0.223214 |religious - sea |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |religious - study |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |religious - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |religious - the |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |religious - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |    sea - study |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "       0.223214 |    sea - tales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.559350 |      sea - the |       0.566947 |       0.375000 |       0.750000 |       0.545455\n",
      "       0.223214 |    sea - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.365956 |  study - tales |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.255952 |    study - the |       0.285714 |       0.166667 |       0.285714 |       0.285714\n",
      "       0.365956 |  study - wales |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.365956 |    tales - the |       0.377964 |       0.222222 |       0.500000 |       0.363636\n",
      "       0.223214 |  tales - wales |       0.250000 |       0.142857 |       0.250000 |       0.250000\n",
      "       0.180200 |    the - wales |       0.188982 |       0.100000 |       0.250000 |       0.181818\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  2  - Similarity measures\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "        average |           pair |         cosine |        jaccard |        overlap |           dice\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "       0.389562 |   atlas - boon |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
      "       1.000000 |   atlas - cava |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "       0.389562 | atlas - dipped |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
      "       0.389562 |    boon - cava |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
      "       0.625000 |  boon - dipped |       0.666667 |       0.500000 |       0.666667 |       0.666667\n",
      "       0.389562 |  cava - dipped |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  3  - Similarity measures\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "        average |           pair |         cosine |        jaccard |        overlap |           dice\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "       0.820791 |    DocA - DocB |       0.816497 |       0.666667 |       1.000000 |       0.800000\n",
      "       0.553861 |    DocA - DocC |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
      "       0.346722 |    DocB - DocC |       0.353553 |       0.200000 |       0.500000 |       0.333333\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# Pretty print systems tests\n",
    "############################################\n",
    "\n",
    "import json\n",
    "for i in range(1,4):\n",
    "  print '—'*110\n",
    "  print \"Systems test \",i,\" - Similarity measures\"\n",
    "  print '—'*110\n",
    "  print \"{0:>15} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "          \"average\", \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\")\n",
    "  print '-'*110\n",
    "\n",
    "  with open(\"systems_test_similarities_\"+str(i),\"r\") as f:\n",
    "      lines = f.readlines()\n",
    "      for line in lines:\n",
    "          line = line.strip()\n",
    "          avg,stripe = line.split(\"\\t\")\n",
    "          stripe = json.loads(stripe)\n",
    "\n",
    "          print \"{0:>15f} |{1:>15} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "              float(avg), stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.4.1 <a name=\"5.4.1\"></a>Full-scale experiment: EDA of Google n-grams dataset (PHASE 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- A. Longest 5-gram (number of characters)\n",
    "- B. Top 10 most frequent words (please use the count information), i.e., unigrams\n",
    "- C. 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n",
    "- D. Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.4.1 - A. Longest 5-gram (number of characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing longest5gram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile longest5gram.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import logging\n",
    "import time\n",
    "\n",
    "class longest5gram(MRJob):\n",
    "    \n",
    "    # START STUDENT CODE 5.4.1.A\n",
    "    \n",
    "    MRJob.SORT_VALUES = True\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(longest5gram, self).__init__(args)\n",
    "        self.max_count = 0\n",
    "        self.max_grams = []\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # Split line\n",
    "        splits = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        words = splits[0].lower().split()\n",
    "        \n",
    "        char_count = 0\n",
    "        \n",
    "        # Count characters\n",
    "        for word in words:\n",
    "            char_count += len(word)\n",
    "        \n",
    "        # Optimization: we track the max count local to the current mapper instance. If records\n",
    "        # have higher count than the max, we output them and update the max. We don't yield\n",
    "        # records that are smaller than the local max. \n",
    "        # Even some non-max records are passed on, the good thing about this is that it is extremely memory efficient\n",
    "        # in that it uses constant memory.\n",
    "        if char_count > self.max_count:\n",
    "            self.max_count = char_count\n",
    "            yield (words), char_count\n",
    "        elif char_count == self.max_count:\n",
    "            yield (words), char_count\n",
    "            \n",
    "    \n",
    "    def combiner(self, ngram, char_counts):\n",
    "        current_max = max(char_counts)\n",
    "        \n",
    "        # Optimization: we track the max count local to the current combiner instance. If records\n",
    "        # have higher count than the max, we output them and update the max. We don't yield\n",
    "        # records that are smaller than the local max, drastically reducing work on shuffling and sorting\n",
    "        # Even some non-max or local max records are passed on, the good thing about this is that it is extremely \n",
    "        # memory efficient in that it uses constant memory (just 1 integer :)\n",
    "        if current_max > self.max_count:\n",
    "            self.max_count = current_max\n",
    "            yield ngram, current_max\n",
    "        elif current_max == self.max_count:\n",
    "            yield ngram, current_max\n",
    "    \n",
    "    def reducer(self, ngram, char_counts):\n",
    "            \n",
    "        current_count = max(char_counts)\n",
    "\n",
    "        # Track in max_grams the n-grams with the max count of words\n",
    "        if current_count > self.max_count:\n",
    "            self.max_count = current_count\n",
    "            self.max_grams = [(current_count, ngram)]\n",
    "        elif current_count == self.max_count:\n",
    "            self.max_grams.append((current_count, ngram))\n",
    "\n",
    "    def reducer_final(self):\n",
    "        # Once\n",
    "        for gram in self.max_grams:\n",
    "            yield gram[0], gram[1]\n",
    "\n",
    "\n",
    "    def steps(self):\n",
    "        \n",
    "        # We need 1 reducer for this approach. However the optimizations in the mappers and combiners\n",
    "        # help us ensure that a small percentage of records get to the reducer\n",
    "        custom_jobconf = {\n",
    "            'stream.num.map.output.key.fields':'1',\n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k2,2nr',\n",
    "            'mapred.reduce.tasks': '1'\n",
    "        }\n",
    "\n",
    "        return [\n",
    "                MRStep(jobconf=custom_jobconf,\n",
    "                    mapper=self.mapper,\n",
    "                    reducer=self.reducer,\n",
    "                    combiner = self.combiner,\n",
    "                    reducer_final = self.reducer_final\n",
    "                      )\n",
    "                 ]\n",
    "\n",
    "    # END STUDENT CODE 5.4.1.A\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    longest5gram.run()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"\n",
    "    Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On test data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `systems_test_stripes_5.4.1.a_1': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/longest5gram.nhaas.20170618.183617.038656\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nhaas/tmp/mrjob/longest5gram.nhaas.20170618.183617.038656/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob175076408620936324.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1433\n",
      "  Submitted application application_1493936954640_1433\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1433/\n",
      "  Running job: job_1493936954640_1433\n",
      "  Job job_1493936954640_1433 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "  Task Id : attempt_1493936954640_1433_m_000000_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@33ceb74d rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1433_m_000001_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@33ceb74d rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1433 completed successfully\n",
      "  Output directory: hdfs:///user/nhaas/tmp/mrjob/longest5gram.nhaas.20170618.183617.038656/output\n",
      "Counters: 52\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=563\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=98\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=157\n",
      "\t\tFILE: Number of bytes written=401398\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1007\n",
      "\t\tHDFS: Number of bytes written=98\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=2\n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=4\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tOther local map tasks=2\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=23417856\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=11706880\n",
      "\t\tTotal time spent by all map tasks (ms)=15246\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=45738\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4573\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=22865\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=15246\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4573\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3060\n",
      "\t\tCombine input records=3\n",
      "\t\tCombine output records=3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=118\n",
      "\t\tInput split bytes=444\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=142\n",
      "\t\tMap output materialized bytes=173\n",
      "\t\tMap output records=3\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1901543424\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce input records=3\n",
      "\t\tReduce output records=2\n",
      "\t\tReduce shuffle bytes=173\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=6\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7773741056\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nhaas/tmp/mrjob/longest5gram.nhaas.20170618.183617.038656/output...\n",
      "Removing HDFS temp directory hdfs:///user/nhaas/tmp/mrjob/longest5gram.nhaas.20170618.183617.038656...\n",
      "Removing temp directory /tmp/longest5gram.nhaas.20170618.183617.038656...\n",
      "WARNING:root:\n",
      "    Elapsed time: 62.5210649967 seconds\n",
      "    In minutes: 1.04201774995 mins\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r systems_test_stripes_5.4.1.a_1\n",
    "!python longest5gram.py -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt \\\n",
    "    --python-bin=./my_env/bin/python \\\n",
    "    --archive=\"hdfs:///user/nhaas/virtualenv/my_env.zip#my_env\" \\\n",
    "    > systems_test_stripes_5.4.1.a_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\t[\"a\",\"bill\",\"for\",\"establishing\",\"religious\"]\r\n",
      "29\t[\"a\",\"circumstantial\",\"narrative\",\"of\",\"the\"]\r\n"
     ]
    }
   ],
   "source": [
    "!cat systems_test_stripes_5.4.1.a_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On full data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `full_stripes_5.4.1.a': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/longest5gram.nhaas.20170618.184043.586315\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nhaas/tmp/mrjob/longest5gram.nhaas.20170618.184043.586315/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob396577900789759386.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 190\n",
      "  number of splits:190\n",
      "  Submitting tokens for job: job_1493936954640_1434\n",
      "  Submitted application application_1493936954640_1434\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1434/\n",
      "  Running job: job_1493936954640_1434\n",
      "  Job job_1493936954640_1434 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "  Task Id : attempt_1493936954640_1434_m_000132_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@45c8cb42 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1434_m_000134_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@45c8cb42 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1434_m_000135_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@45c8cb42 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1434_m_000131_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@45c8cb42 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1434_m_000137_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1679fa07 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1434_m_000140_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1679fa07 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1434_m_000133_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@7a365e54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1434_m_000136_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@7a365e54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1434_m_000138_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@7a365e54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1434_m_000150_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@7a365e54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1434_m_000144_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@7a365e54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1434_m_000146_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@7a365e54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1434_m_000153_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@7a365e54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1434_m_000143_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@7a365e54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1434_m_000155_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@7a365e54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1434_m_000149_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@7a365e54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1434_m_000145_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@7a365e54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1434_m_000142_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@7a365e54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1434_m_000148_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@7a365e54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1434_m_000151_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@7a365e54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1434_m_000139_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@7a365e54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1434_m_000141_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@7a365e54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1434_m_000152_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@7a365e54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1434_m_000147_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@7a365e54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1434_m_000156_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@7a365e54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1434_m_000154_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@7a365e54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1434_m_000157_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4270f4ef rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1434_m_000158_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4270f4ef rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1434_m_000159_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4270f4ef rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "   map 1% reduce 0%\n",
      "   map 2% reduce 0%\n",
      "   map 3% reduce 0%\n",
      "   map 6% reduce 0%\n",
      "   map 9% reduce 0%\n",
      "   map 13% reduce 0%\n",
      "   map 15% reduce 0%\n",
      "   map 17% reduce 0%\n",
      "   map 18% reduce 0%\n",
      "   map 21% reduce 0%\n",
      "   map 23% reduce 0%\n",
      "   map 25% reduce 0%\n",
      "   map 30% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 40% reduce 0%\n",
      "   map 47% reduce 0%\n",
      "   map 54% reduce 0%\n",
      "   map 58% reduce 0%\n",
      "   map 61% reduce 0%\n",
      "   map 69% reduce 0%\n",
      "   map 70% reduce 0%\n",
      "   map 71% reduce 0%\n",
      "   map 74% reduce 0%\n",
      "   map 79% reduce 0%\n",
      "   map 80% reduce 0%\n",
      "   map 81% reduce 0%\n",
      "   map 82% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 84% reduce 0%\n",
      "   map 87% reduce 0%\n",
      "   map 89% reduce 0%\n",
      "   map 90% reduce 0%\n",
      "   map 95% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1434 completed successfully\n",
      "  Output directory: hdfs:///user/nhaas/tmp/mrjob/longest5gram.nhaas.20170618.184043.586315/output\n",
      "Counters: 52\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=352\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=37013\n",
      "\t\tFILE: Number of bytes written=25619187\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2156101116\n",
      "\t\tHDFS: Number of bytes written=352\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=573\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=29\n",
      "\t\tKilled map tasks=2\n",
      "\t\tLaunched map tasks=221\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tOther local map tasks=33\n",
      "\t\tRack-local map tasks=188\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=11472514560\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=13388800\n",
      "\t\tTotal time spent by all map tasks (ms)=7469085\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=22407255\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5230\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=26150\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=7469085\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5230\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=564710\n",
      "\t\tCombine input records=2699\n",
      "\t\tCombine output records=1108\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=32985\n",
      "\t\tInput split bytes=32000\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=161994\n",
      "\t\tMap output materialized bytes=63404\n",
      "\t\tMap output records=2699\n",
      "\t\tMerged Map outputs=190\n",
      "\t\tPhysical memory (bytes) snapshot=153901260800\n",
      "\t\tReduce input groups=1108\n",
      "\t\tReduce input records=1108\n",
      "\t\tReduce output records=2\n",
      "\t\tReduce shuffle bytes=63404\n",
      "\t\tShuffled Maps =190\n",
      "\t\tSpilled Records=2216\n",
      "\t\tTotal committed heap usage (bytes)=299992875008\n",
      "\t\tVirtual memory (bytes) snapshot=420946317312\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nhaas/tmp/mrjob/longest5gram.nhaas.20170618.184043.586315/output...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing HDFS temp directory hdfs:///user/nhaas/tmp/mrjob/longest5gram.nhaas.20170618.184043.586315...\n",
      "Removing temp directory /tmp/longest5gram.nhaas.20170618.184043.586315...\n",
      "WARNING:root:\n",
      "    Elapsed time: 123.621688128 seconds\n",
      "    In minutes: 2.06036146879 mins\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r full_stripes_5.4.1.a\n",
    "!python longest5gram.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ \\\n",
    "    --python-bin=./my_env/bin/python \\\n",
    "    --archive=\"hdfs:///user/nhaas/virtualenv/my_env.zip#my_env\" \\\n",
    "    > full_stripes_5.4.1.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155\t[\"roplezimpredastrodonbraslpklson\",\"yhroaclmparcheyxmmioudavesaurus\",\"piofpilocowersuruasogetsesnegcp\",\"tyravopsifengoquapialloboskenuo\",\"owinfuyaiokenecksasxhyilpoynuat\"]\r\n",
      "155\t[\"aiopjumrxuyvaslyhypsibemapodikr\",\"ufrydiuuolbigasuaurusrexlisnaye\",\"rnoondqsrunsubunougrabberyairtc\",\"utahraptoredileipmilbdummyuveri\",\"syevrahvelocyallosauruslinrotsr\"]\r\n"
     ]
    }
   ],
   "source": [
    "!cat full_stripes_5.4.1.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Stats: \n",
    "## example: \n",
    "## Longest 5grams MR stats\n",
    "\n",
    "    ec2_instance_type: m3.xlarge\n",
    "    num_ec2_instances: 15\n",
    "\n",
    "__Step 1:__  \n",
    "\n",
    "    RUNNING for 107.0s ~= 2 minutes  \n",
    "    Reduce tasks = 16 \n",
    "    \n",
    "__Step 2:__   \n",
    "\n",
    "    RUNNING for 108.8s ~= 2 minutes\n",
    "    Reduce tasks = 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.4.1 - B. Top 10 most frequent words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mostFrequentWords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mostFrequentWords.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import logging\n",
    "import time\n",
    "\n",
    "class mostFrequentWords(MRJob):\n",
    "\n",
    "    # START STUDENT CODE 5.4.1.B\n",
    "            \n",
    "    MRJob.SORT_VALUES = True\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # Split line\n",
    "        splits = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        words = splits[0].lower().split()\n",
    "        count = int(splits[1])\n",
    "        \n",
    "        for word in words:\n",
    "            yield word, count\n",
    "            \n",
    "    \n",
    "    def combiner(self, word, counts):\n",
    "        total = sum(count for count in counts)\n",
    "        yield word, total\n",
    "    \n",
    "    def reducer(self, word, counts):\n",
    "        total = sum(count for count in counts)\n",
    "        yield total, word\n",
    "    \n",
    "    def max_reducer(self, count, words):\n",
    "        for word in words:\n",
    "            yield word, count\n",
    "\n",
    "    def steps(self):\n",
    "        \n",
    "        custom_jobconf = {\n",
    "            'stream.num.map.output.key.fields':'2',\n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k1,1nr',\n",
    "            'mapred.reduce.tasks': '1'\n",
    "        }\n",
    "\n",
    "        return [\n",
    "                MRStep(mapper=self.mapper,\n",
    "                    reducer=self.reducer,\n",
    "                    combiner = self.combiner),\n",
    "                MRStep(jobconf=custom_jobconf,\n",
    "                       reducer=self.max_reducer)\n",
    "                 ]\n",
    "\n",
    "    # END STUDENT CODE 5.4.1.B\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    mostFrequentWords.run()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"\n",
    "    Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the test data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `systems_test_stripes_5.4.1.b_1': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/mostFrequentWords.nhaas.20170618.184417.128909\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nhaas/tmp/mrjob/mostFrequentWords.nhaas.20170618.184417.128909/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob2384880195566234157.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1435\n",
      "  Submitted application application_1493936954640_1435\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1435/\n",
      "  Running job: job_1493936954640_1435\n",
      "  Job job_1493936954640_1435 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1435 completed successfully\n",
      "  Output directory: hdfs:///user/nhaas/tmp/mrjob/mostFrequentWords.nhaas.20170618.184417.128909/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=563\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=357\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=430\n",
      "\t\tFILE: Number of bytes written=401153\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1017\n",
      "\t\tHDFS: Number of bytes written=357\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=24419328\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=12024320\n",
      "\t\tTotal time spent by all map tasks (ms)=15898\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=47694\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4697\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=23485\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=15898\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4697\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2960\n",
      "\t\tCombine input records=50\n",
      "\t\tCombine output records=31\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=109\n",
      "\t\tInput split bytes=454\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=602\n",
      "\t\tMap output materialized bytes=458\n",
      "\t\tMap output records=50\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1897758720\n",
      "\t\tReduce input groups=28\n",
      "\t\tReduce input records=31\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=458\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=62\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7742881792\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob4829533011563623770.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1436\n",
      "  Submitted application application_1493936954640_1436\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1436/\n",
      "  Running job: job_1493936954640_1436\n",
      "  Job job_1493936954640_1436 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1436 completed successfully\n",
      "  Output directory: hdfs:///user/nhaas/tmp/mrjob/mostFrequentWords.nhaas.20170618.184417.128909/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=536\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=357\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=396\n",
      "\t\tFILE: Number of bytes written=400513\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=904\n",
      "\t\tHDFS: Number of bytes written=357\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=19481088\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=11993600\n",
      "\t\tTotal time spent by all map tasks (ms)=12683\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=38049\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4685\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=23425\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=12683\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4685\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2690\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=109\n",
      "\t\tInput split bytes=368\n",
      "\t\tMap input records=28\n",
      "\t\tMap output bytes=385\n",
      "\t\tMap output materialized bytes=437\n",
      "\t\tMap output records=28\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1893158912\n",
      "\t\tReduce input groups=28\n",
      "\t\tReduce input records=28\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=437\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=56\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7737966592\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nhaas/tmp/mrjob/mostFrequentWords.nhaas.20170618.184417.128909/output...\n",
      "Removing HDFS temp directory hdfs:///user/nhaas/tmp/mrjob/mostFrequentWords.nhaas.20170618.184417.128909...\n",
      "Removing temp directory /tmp/mostFrequentWords.nhaas.20170618.184417.128909...\n",
      "WARNING:root:\n",
      "    Elapsed time: 98.0454440117 seconds\n",
      "    In minutes: 1.63409073353 mins\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r systems_test_stripes_5.4.1.b_1\n",
    "!python mostFrequentWords.py -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt \\\n",
    "    --python-bin=./my_env/bin/python \\\n",
    "    --archive=\"hdfs:///user/nhaas/virtualenv/my_env.zip#my_env\" \\\n",
    "    > systems_test_stripes_5.4.1.b_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"a\"\t2217\r\n",
      "\"in\"\t1201\r\n",
      "\"wales\"\t1099\r\n",
      "\"christmas\"\t1099\r\n",
      "\"child's\"\t1099\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 systems_test_stripes_5.4.1.b_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the full data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `full_mostFrequentWords_5.4.1.b': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/mostFrequentWords.nhaas.20170618.184724.820057\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nhaas/tmp/mrjob/mostFrequentWords.nhaas.20170618.184724.820057/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob1123122455035904051.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 190\n",
      "  number of splits:190\n",
      "  Submitting tokens for job: job_1493936954640_1438\n",
      "  Submitted application application_1493936954640_1438\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1438/\n",
      "  Running job: job_1493936954640_1438\n",
      "  Job job_1493936954640_1438 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "  Task Id : attempt_1493936954640_1438_m_000002_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@640bb179 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000004_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@640bb179 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000008_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4b8afcd2 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000009_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000001_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000029_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "   map 16% reduce 0%\n",
      "  Task Id : attempt_1493936954640_1438_m_000003_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000006_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4b8afcd2 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000000_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000038_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000035_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000011_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000033_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000027_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000025_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000005_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "   map 1% reduce 0%\n",
      "  Task Id : attempt_1493936954640_1438_m_000040_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000020_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000031_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000019_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000007_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000015_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000022_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000042_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000017_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000026_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "   map 0% reduce 0%\n",
      "  Task Id : attempt_1493936954640_1438_m_000036_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000034_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000028_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000024_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000021_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000013_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000010_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000030_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000037_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000012_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000023_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000016_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000018_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000014_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000041_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000032_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000158_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6506f9f7 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000157_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6506f9f7 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000043_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000039_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@38e8fd54 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000161_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@11ac70ba rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1438_m_000164_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Task Id : attempt_1493936954640_1438_m_000174_1000, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1438_m_000175_1000, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1438_m_000168_1000, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1438_m_000162_1000, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1438_m_000159_1000, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1438_m_000165_1000, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1438_m_000171_1000, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1438_m_000160_1000, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1438_m_000163_1000, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1438_m_000176_1000, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1438_m_000166_1000, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1438_m_000178_1000, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1438_m_000186_1000, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1438_m_000179_1000, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1438_m_000172_1000, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1438_m_000170_1000, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1438_m_000182_1000, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1438_m_000002_1001, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1438_m_000184_1000, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1438_m_000185_1000, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1438_m_000183_1000, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1438_m_000177_1000, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1438_m_000181_1000, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1438_m_000173_1000, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1438_m_000169_1000, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1438_m_000187_1000, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1438_m_000004_1001, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1438_m_000167_1000, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "  Task Id : attempt_1493936954640_1438_m_000180_1000, Status : FAILED\r\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6f7b518c rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\r\n",
      "\r\n",
      "   map 1% reduce 0%\r\n",
      "   map 2% reduce 0%\r\n",
      "   map 3% reduce 0%\r\n",
      "   map 5% reduce 0%\r\n",
      "   map 6% reduce 0%\r\n",
      "   map 7% reduce 0%\r\n",
      "   map 8% reduce 0%\r\n",
      "   map 10% reduce 0%\r\n",
      "   map 11% reduce 0%\r\n",
      "   map 12% reduce 0%\r\n",
      "   map 13% reduce 0%\r\n",
      "   map 14% reduce 0%\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   map 16% reduce 0%\n",
      "   map 17% reduce 0%\n",
      "   map 19% reduce 0%\n",
      "   map 20% reduce 0%\n",
      "   map 22% reduce 0%\n",
      "   map 23% reduce 0%\n",
      "   map 25% reduce 0%\n",
      "   map 27% reduce 0%\n",
      "   map 28% reduce 0%\n",
      "   map 29% reduce 0%\n",
      "   map 31% reduce 0%\n",
      "   map 32% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 34% reduce 0%\n",
      "   map 35% reduce 0%\n",
      "   map 37% reduce 0%\n",
      "   map 38% reduce 0%\n",
      "   map 39% reduce 0%\n",
      "   map 40% reduce 0%\n",
      "   map 41% reduce 0%\n",
      "   map 42% reduce 0%\n",
      "   map 43% reduce 0%\n",
      "   map 44% reduce 0%\n",
      "   map 45% reduce 0%\n",
      "   map 46% reduce 0%\n",
      "   map 47% reduce 0%\n",
      "   map 48% reduce 0%\n",
      "   map 49% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 51% reduce 0%\n",
      "   map 52% reduce 0%\n",
      "   map 53% reduce 0%\n",
      "   map 54% reduce 0%\n",
      "   map 56% reduce 0%\n",
      "   map 57% reduce 0%\n",
      "   map 59% reduce 0%\n",
      "   map 60% reduce 0%\n",
      "   map 61% reduce 0%\n",
      "   map 62% reduce 0%\n",
      "   map 63% reduce 0%\n",
      "   map 64% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 66% reduce 0%\n",
      "   map 68% reduce 0%\n",
      "   map 71% reduce 0%\n",
      "   map 74% reduce 0%\n",
      "   map 76% reduce 0%\n",
      "   map 77% reduce 0%\n",
      "   map 80% reduce 0%\n",
      "   map 81% reduce 0%\n",
      "   map 82% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 84% reduce 0%\n",
      "   map 85% reduce 0%\n",
      "   map 87% reduce 0%\n",
      "   map 88% reduce 0%\n",
      "   map 89% reduce 0%\n",
      "   map 90% reduce 0%\n",
      "   map 91% reduce 0%\n",
      "   map 92% reduce 0%\n",
      "   map 93% reduce 0%\n",
      "   map 95% reduce 0%\n",
      "   map 96% reduce 0%\n",
      "   map 97% reduce 0%\n",
      "   map 98% reduce 0%\n",
      "   map 99% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 67%\n",
      "   map 100% reduce 71%\n",
      "   map 100% reduce 77%\n",
      "   map 100% reduce 83%\n",
      "   map 100% reduce 88%\n",
      "   map 100% reduce 94%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1438 completed successfully\n",
      "  Output directory: hdfs:///user/nhaas/tmp/mrjob/mostFrequentWords.nhaas.20170618.184724.820057/step-output/0000\n",
      "Counters: 52\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=4158739\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=39015541\n",
      "\t\tFILE: Number of bytes written=138284058\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2156101116\n",
      "\t\tHDFS: Number of bytes written=4158739\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=573\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=77\n",
      "\t\tKilled map tasks=2\n",
      "\t\tLaunched map tasks=268\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tOther local map tasks=80\n",
      "\t\tRack-local map tasks=188\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=21287586816\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=70865920\n",
      "\t\tTotal time spent by all map tasks (ms)=13859106\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=41577318\n",
      "\t\tTotal time spent by all reduce tasks (ms)=27682\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=138410\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=13859106\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=27682\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=5193040\n",
      "\t\tCombine input records=293411330\n",
      "\t\tCombine output records=6822745\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=82457\n",
      "\t\tInput split bytes=32000\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=3430141090\n",
      "\t\tMap output materialized bytes=73800744\n",
      "\t\tMap output records=293411330\n",
      "\t\tMerged Map outputs=190\n",
      "\t\tPhysical memory (bytes) snapshot=154849239040\n",
      "\t\tReduce input groups=269339\n",
      "\t\tReduce input records=6822745\n",
      "\t\tReduce output records=269339\n",
      "\t\tReduce shuffle bytes=73800744\n",
      "\t\tShuffled Maps =190\n",
      "\t\tSpilled Records=13645490\n",
      "\t\tTotal committed heap usage (bytes)=299931533312\n",
      "\t\tVirtual memory (bytes) snapshot=421320278016\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob5525939383323576955.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1439\n",
      "  Submitted application application_1493936954640_1439\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1439/\n",
      "  Running job: job_1493936954640_1439\n",
      "  Job job_1493936954640_1439 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1439 completed successfully\n",
      "  Output directory: hdfs:///user/nhaas/tmp/mrjob/mostFrequentWords.nhaas.20170618.184724.820057/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=4176522\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=4158739\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2953963\n",
      "\t\tFILE: Number of bytes written=6322903\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4176890\n",
      "\t\tHDFS: Number of bytes written=4158739\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=13529088\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=16522240\n",
      "\t\tTotal time spent by all map tasks (ms)=8808\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=26424\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6454\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=32270\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8808\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6454\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=8810\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=248\n",
      "\t\tInput split bytes=368\n",
      "\t\tMap input records=269339\n",
      "\t\tMap output bytes=4428078\n",
      "\t\tMap output materialized bytes=2969260\n",
      "\t\tMap output records=269339\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1945366528\n",
      "\t\tReduce input groups=269339\n",
      "\t\tReduce input records=269339\n",
      "\t\tReduce output records=269339\n",
      "\t\tReduce shuffle bytes=2969260\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=538678\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7750213632\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nhaas/tmp/mrjob/mostFrequentWords.nhaas.20170618.184724.820057/output...\n",
      "Removing HDFS temp directory hdfs:///user/nhaas/tmp/mrjob/mostFrequentWords.nhaas.20170618.184724.820057...\n",
      "Removing temp directory /tmp/mostFrequentWords.nhaas.20170618.184724.820057...\n",
      "WARNING:root:\n",
      "    Elapsed time: 239.674644947 seconds\n",
      "    In minutes: 3.99457741578 mins\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r full_mostFrequentWords_5.4.1.b\n",
    "!python mostFrequentWords.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ \\\n",
    "    --python-bin=./my_env/bin/python \\\n",
    "    --archive=\"hdfs:///user/nhaas/virtualenv/my_env.zip#my_env\" \\\n",
    "    > full_mostFrequentWords_5.4.1.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"the\"\t5490815394\r\n",
      "\"of\"\t3698583299\r\n",
      "\"to\"\t2227866570\r\n",
      "\"in\"\t1421312776\r\n",
      "\"a\"\t1361123022\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 full_mostFrequentWords_5.4.1.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most frequent words MR stats\n",
    "    \n",
    "    ec2_instance_type: m3.xlarge\n",
    "    num_ec2_instances: 15\n",
    "    \n",
    "__Step 1:__   \n",
    "\n",
    "    RUNNING for 590.7s ~= 10 minutes   \n",
    "    Launched map tasks=191  \n",
    "    Launched reduce tasks=57   \n",
    "\n",
    "__Step 2:__  \n",
    "\n",
    "    RUNNING for 76.6s   \n",
    "    Launched map tasks=110\n",
    "    Launched reduce tasks=16  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.4.1 - C. 20 Most/Least densely appearing words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mostLeastDenseWords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mostLeastDenseWords.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import division\n",
    "import re\n",
    "import numpy as np\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import time\n",
    "import logging\n",
    "\n",
    "class mostLeastDenseWords(MRJob):\n",
    "    \n",
    "    # START STUDENT CODE 5.4.1.C\n",
    "           \n",
    "    MRJob.SORT_VALUES = True\n",
    "        \n",
    "    def __init__(self, args):\n",
    "        super(mostLeastDenseWords, self).__init__(args)\n",
    "        self.total_word_count = None\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # Split line\n",
    "        splits = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        words = splits[0].lower().split()\n",
    "        count = int(splits[1])\n",
    "        \n",
    "        for word in words:\n",
    "            yield \"*\", count\n",
    "            yield word, count\n",
    "            \n",
    "    \n",
    "    def combiner(self, word, counts):\n",
    "        total = sum(count for count in counts)\n",
    "        yield word, total\n",
    "    \n",
    "    def reducer(self, word, counts):\n",
    "    \n",
    "        total = sum(count for count in counts)\n",
    "        \n",
    "        if word == \"*\":\n",
    "            self.total_word_count = total\n",
    "        else:\n",
    "            yield float(total) / float(self.total_word_count), word\n",
    "    \n",
    "    def max_reducer(self, count, words):\n",
    "        for word in words:\n",
    "            yield word, count\n",
    "\n",
    "    def steps(self):\n",
    "        \n",
    "        custom_jobconf = {\n",
    "            'stream.num.map.output.key.fields':'2',\n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-g -k1,1nr',\n",
    "            'mapred.reduce.tasks': '1'\n",
    "        }\n",
    "\n",
    "        return [\n",
    "                MRStep(\n",
    "                    mapper=self.mapper,\n",
    "                    reducer=self.reducer,\n",
    "                    combiner = self.combiner),\n",
    "                MRStep(jobconf=custom_jobconf,\n",
    "                       reducer=self.max_reducer)\n",
    "                 ]\n",
    "    \n",
    "    # END STUDENT CODE 5.4.1.C\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    mostLeastDenseWords.run()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"\n",
    "    Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the test data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `systems_test_stripes_5.4.1.c_1': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/mostLeastDenseWords.nhaas.20170618.185307.431994\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nhaas/tmp/mrjob/mostLeastDenseWords.nhaas.20170618.185307.431994/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob6118156754056839672.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1440\n",
      "  Submitted application application_1493936954640_1440\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1440/\n",
      "  Running job: job_1493936954640_1440\n",
      "  Job job_1493936954640_1440 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1440 completed successfully\n",
      "  Output directory: hdfs:///user/nhaas/tmp/mrjob/mostLeastDenseWords.nhaas.20170618.185307.431994/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=563\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=604\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=447\n",
      "\t\tFILE: Number of bytes written=401285\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1021\n",
      "\t\tHDFS: Number of bytes written=604\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=26204160\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=11686400\n",
      "\t\tTotal time spent by all map tasks (ms)=17060\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=51180\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4565\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=22825\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=17060\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4565\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2780\n",
      "\t\tCombine input records=100\n",
      "\t\tCombine output records=33\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=68\n",
      "\t\tInput split bytes=458\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=1032\n",
      "\t\tMap output materialized bytes=477\n",
      "\t\tMap output records=100\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1907466240\n",
      "\t\tReduce input groups=29\n",
      "\t\tReduce input records=33\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=477\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=66\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7730454528\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob8934195949017961851.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1441\n",
      "  Submitted application application_1493936954640_1441\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1441/\n",
      "  Running job: job_1493936954640_1441\n",
      "  Job job_1493936954640_1441 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1441 completed successfully\n",
      "  Output directory: hdfs:///user/nhaas/tmp/mrjob/mostLeastDenseWords.nhaas.20170618.185307.431994/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=906\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=604\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=523\n",
      "\t\tFILE: Number of bytes written=400900\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1278\n",
      "\t\tHDFS: Number of bytes written=604\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=19564032\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=13035520\n",
      "\t\tTotal time spent by all map tasks (ms)=12737\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=38211\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5092\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=25460\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=12737\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5092\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3010\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=140\n",
      "\t\tInput split bytes=372\n",
      "\t\tMap input records=28\n",
      "\t\tMap output bytes=632\n",
      "\t\tMap output materialized bytes=613\n",
      "\t\tMap output records=28\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1898176512\n",
      "\t\tReduce input groups=28\n",
      "\t\tReduce input records=28\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=613\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=56\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7730274304\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nhaas/tmp/mrjob/mostLeastDenseWords.nhaas.20170618.185307.431994/output...\n",
      "Removing HDFS temp directory hdfs:///user/nhaas/tmp/mrjob/mostLeastDenseWords.nhaas.20170618.185307.431994...\n",
      "Removing temp directory /tmp/mostLeastDenseWords.nhaas.20170618.185307.431994...\n",
      "WARNING:root:\n",
      "    Elapsed time: 91.9328680038 seconds\n",
      "    In minutes: 1.53221446673 mins\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r systems_test_stripes_5.4.1.c_1\n",
    "!python mostLeastDenseWords.py -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt \\\n",
    "    --python-bin=./my_env/bin/python \\\n",
    "    --archive=\"hdfs:///user/nhaas/virtualenv/my_env.zip#my_env\" \\\n",
    "    > systems_test_stripes_5.4.1.c_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"a\"\t0.2\r\n",
      "\"in\"\t0.1083446098\r\n",
      "\"wales\"\t0.099142986\r\n",
      "\"christmas\"\t0.099142986\r\n",
      "\"child's\"\t0.099142986\r\n",
      "\"of\"\t0.0912043302\r\n",
      "\"study\"\t0.0544880469\r\n",
      "\"case\"\t0.0544880469\r\n",
      "\"female\"\t0.0403247632\r\n",
      "\"collection\"\t0.0215606676\r\n",
      "\"the\"\t0.0111862878\r\n",
      "\"tales\"\t0.0110960758\r\n",
      "\"fairy\"\t0.0110960758\r\n",
      "\"forms\"\t0.0104645918\r\n",
      "\"government\"\t0.0092016238\r\n",
      "\"george\"\t0.0082995038\r\n",
      "\"general\"\t0.0082995038\r\n",
      "\"biography\"\t0.0082995038\r\n",
      "\"city\"\t0.0055931439\r\n",
      "\"circumstantial\"\t0.0055931439\r\n",
      "\"by\"\t0.0055931439\r\n",
      "\"sea\"\t0.0055931439\r\n",
      "\"narrative\"\t0.0055931439\r\n",
      "\"religious\"\t0.0053225079\r\n",
      "\"establishing\"\t0.0053225079\r\n",
      "\"for\"\t0.0053225079\r\n",
      "\"bill\"\t0.0053225079\r\n",
      "\"limited\"\t0.0049616599\r\n"
     ]
    }
   ],
   "source": [
    "!cat systems_test_stripes_5.4.1.c_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the full data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `full_mostLeastDenseWords_5.4.1.c': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/mostLeastDenseWords.nhaas.20170618.185655.804094\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nhaas/tmp/mrjob/mostLeastDenseWords.nhaas.20170618.185655.804094/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob371614955361885526.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 190\n",
      "  number of splits:190\n",
      "  Submitting tokens for job: job_1493936954640_1442\n",
      "  Submitted application application_1493936954640_1442\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1442/\n",
      "  Running job: job_1493936954640_1442\n",
      "  Job job_1493936954640_1442 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "  Task Id : attempt_1493936954640_1442_m_000052_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6025ece5 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000046_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6025ece5 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000044_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6025ece5 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000045_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6025ece5 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000048_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6025ece5 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000047_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6025ece5 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000050_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@3b777bd5 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000075_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000053_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@3b777bd5 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000056_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000077_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000079_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000081_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000054_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000064_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000057_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "   map 2% reduce 0%\n",
      "  Task Id : attempt_1493936954640_1442_m_000082_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000049_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000071_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000087_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000067_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000058_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000062_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000078_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000083_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000069_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "   map 0% reduce 0%\n",
      "  Task Id : attempt_1493936954640_1442_m_000080_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000051_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000076_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000060_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000068_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000073_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000055_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000065_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000074_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000084_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000061_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000070_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000066_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000072_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000063_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000059_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000086_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000085_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@36d39242 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000159_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6aa12a9b rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000158_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6aa12a9b rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1442_m_000157_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6aa12a9b rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "   map 1% reduce 0%\n",
      "   map 2% reduce 0%\n",
      "   map 3% reduce 0%\n",
      "   map 4% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 6% reduce 0%\n",
      "   map 7% reduce 0%\n",
      "   map 8% reduce 0%\n",
      "   map 9% reduce 0%\n",
      "   map 10% reduce 0%\n",
      "   map 11% reduce 0%\n",
      "   map 12% reduce 0%\n",
      "   map 13% reduce 0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   map 14% reduce 0%\n",
      "   map 15% reduce 0%\n",
      "   map 16% reduce 0%\n",
      "   map 17% reduce 0%\n",
      "   map 18% reduce 0%\n",
      "   map 19% reduce 0%\n",
      "   map 20% reduce 0%\n",
      "   map 21% reduce 0%\n",
      "   map 22% reduce 0%\n",
      "   map 23% reduce 0%\n",
      "   map 24% reduce 0%\n",
      "   map 25% reduce 0%\n",
      "   map 26% reduce 0%\n",
      "   map 27% reduce 0%\n",
      "   map 28% reduce 0%\n",
      "   map 29% reduce 0%\n",
      "   map 30% reduce 0%\n",
      "   map 31% reduce 0%\n",
      "   map 32% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 34% reduce 0%\n",
      "   map 35% reduce 0%\n",
      "   map 36% reduce 0%\n",
      "   map 37% reduce 0%\n",
      "   map 38% reduce 0%\n",
      "   map 39% reduce 0%\n",
      "   map 40% reduce 0%\n",
      "   map 41% reduce 0%\n",
      "   map 42% reduce 0%\n",
      "   map 43% reduce 0%\n",
      "   map 44% reduce 0%\n",
      "   map 45% reduce 0%\n",
      "   map 46% reduce 0%\n",
      "   map 47% reduce 0%\n",
      "   map 48% reduce 0%\n",
      "   map 49% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 51% reduce 0%\n",
      "   map 52% reduce 0%\n",
      "   map 53% reduce 0%\n",
      "   map 54% reduce 0%\n",
      "   map 55% reduce 0%\n",
      "   map 56% reduce 0%\n",
      "   map 57% reduce 0%\n",
      "   map 59% reduce 0%\n",
      "   map 60% reduce 0%\n",
      "   map 61% reduce 0%\n",
      "   map 63% reduce 0%\n",
      "   map 64% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 66% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 68% reduce 0%\n",
      "   map 69% reduce 0%\n",
      "   map 70% reduce 0%\n",
      "   map 72% reduce 0%\n",
      "   map 74% reduce 0%\n",
      "   map 75% reduce 0%\n",
      "   map 77% reduce 0%\n",
      "   map 78% reduce 0%\n",
      "   map 79% reduce 0%\n",
      "   map 80% reduce 0%\n",
      "   map 81% reduce 0%\n",
      "   map 82% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 84% reduce 0%\n",
      "   map 85% reduce 0%\n",
      "   map 86% reduce 0%\n",
      "   map 87% reduce 0%\n",
      "   map 88% reduce 0%\n",
      "   map 89% reduce 0%\n",
      "   map 90% reduce 0%\n",
      "   map 91% reduce 0%\n",
      "   map 92% reduce 0%\n",
      "   map 93% reduce 0%\n",
      "   map 94% reduce 0%\n",
      "   map 95% reduce 0%\n",
      "   map 96% reduce 0%\n",
      "   map 97% reduce 0%\n",
      "   map 98% reduce 0%\n",
      "   map 99% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 67%\n",
      "   map 100% reduce 71%\n",
      "   map 100% reduce 76%\n",
      "   map 100% reduce 82%\n",
      "   map 100% reduce 87%\n",
      "   map 100% reduce 92%\n",
      "   map 100% reduce 98%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1442 completed successfully\n",
      "  Output directory: hdfs:///user/nhaas/tmp/mrjob/mostLeastDenseWords.nhaas.20170618.185655.804094/step-output/0000\n",
      "Counters: 52\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=6511551\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=39016573\n",
      "\t\tFILE: Number of bytes written=138293429\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2156101116\n",
      "\t\tHDFS: Number of bytes written=6511551\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=573\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=47\n",
      "\t\tKilled map tasks=2\n",
      "\t\tLaunched map tasks=239\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tOther local map tasks=51\n",
      "\t\tRack-local map tasks=188\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=31154754048\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=73377280\n",
      "\t\tTotal time spent by all map tasks (ms)=20283043\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=60849129\n",
      "\t\tTotal time spent by all reduce tasks (ms)=28663\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=143315\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=20283043\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=28663\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=9078560\n",
      "\t\tCombine input records=586822660\n",
      "\t\tCombine output records=6822933\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=119240\n",
      "\t\tInput split bytes=32000\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=5878243690\n",
      "\t\tMap output materialized bytes=73804117\n",
      "\t\tMap output records=586822660\n",
      "\t\tMerged Map outputs=190\n",
      "\t\tPhysical memory (bytes) snapshot=154915774464\n",
      "\t\tReduce input groups=269340\n",
      "\t\tReduce input records=6822933\n",
      "\t\tReduce output records=269339\n",
      "\t\tReduce shuffle bytes=73804117\n",
      "\t\tShuffled Maps =190\n",
      "\t\tSpilled Records=13645866\n",
      "\t\tTotal committed heap usage (bytes)=297861120000\n",
      "\t\tVirtual memory (bytes) snapshot=421247385600\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob1412229802811820157.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1443\n",
      "  Submitted application application_1493936954640_1443\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1443/\n",
      "  Running job: job_1493936954640_1443\n",
      "  Job job_1493936954640_1443 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1443 completed successfully\n",
      "  Output directory: hdfs:///user/nhaas/tmp/mrjob/mostLeastDenseWords.nhaas.20170618.185655.804094/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=6532576\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=6511551\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3064653\n",
      "\t\tFILE: Number of bytes written=6538821\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6532948\n",
      "\t\tHDFS: Number of bytes written=6511551\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=13767168\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=24819200\n",
      "\t\tTotal time spent by all map tasks (ms)=8963\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=26889\n",
      "\t\tTotal time spent by all reduce tasks (ms)=9695\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=48475\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8963\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=9695\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=8830\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=232\n",
      "\t\tInput split bytes=372\n",
      "\t\tMap input records=269339\n",
      "\t\tMap output bytes=6780890\n",
      "\t\tMap output materialized bytes=3074404\n",
      "\t\tMap output records=269339\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1958100992\n",
      "\t\tReduce input groups=269339\n",
      "\t\tReduce input records=269339\n",
      "\t\tReduce output records=269339\n",
      "\t\tReduce shuffle bytes=3074404\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=538678\n",
      "\t\tTotal committed heap usage (bytes)=5202509824\n",
      "\t\tVirtual memory (bytes) snapshot=7761498112\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nhaas/tmp/mrjob/mostLeastDenseWords.nhaas.20170618.185655.804094/output...\n",
      "Removing HDFS temp directory hdfs:///user/nhaas/tmp/mrjob/mostLeastDenseWords.nhaas.20170618.185655.804094...\n",
      "Removing temp directory /tmp/mostLeastDenseWords.nhaas.20170618.185655.804094...\n",
      "WARNING:root:\n",
      "    Elapsed time: 299.635922909 seconds\n",
      "    In minutes: 4.99393204848 mins\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r full_mostLeastDenseWords_5.4.1.c\n",
    "!python mostLeastDenseWords.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ \\\n",
    "    --python-bin=./my_env/bin/python \\\n",
    "    --archive=\"hdfs:///user/nhaas/virtualenv/my_env.zip#my_env\" \\\n",
    "    > full_mostLeastDenseWords_5.4.1.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=192\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Highest frequency words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"the\"\t0.1108347829\r\n",
      "\"of\"\t0.0746577052\r\n",
      "\"to\"\t0.0449705718\r\n",
      "\"in\"\t0.0286898906\r\n",
      "\"a\"\t0.0274749311\r\n",
      "\"and\"\t0.0232047813\r\n",
      "\"that\"\t0.0162073544\r\n",
      "\"is\"\t0.0153072361\r\n",
      "\"be\"\t0.0139018888\r\n",
      "\"as\"\t0.0099346975\r\n",
      "\"it\"\t0.00983431\r\n",
      "\"was\"\t0.0094945978\r\n",
      "\"for\"\t0.0093389722\r\n",
      "\"not\"\t0.0080856977\r\n",
      "\"with\"\t0.0075914555\r\n",
      "\"on\"\t0.0072231848\r\n",
      "\"by\"\t0.0070078767\r\n",
      "\"he\"\t0.0064625413\r\n",
      "\"have\"\t0.0064079076\r\n",
      "\"which\"\t0.0056949527\r\n"
     ]
    }
   ],
   "source": [
    "!head -20 full_mostLeastDenseWords_5.4.1.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowest frequency words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"instillest\"\t0.0000000008\r\n",
      "\"glycasme\"\t0.0000000008\r\n",
      "\"dantonists\"\t0.0000000008\r\n",
      "\"jurisfirma\"\t0.0000000008\r\n",
      "\"danton's\"\t0.0000000008\r\n",
      "\"glur\"\t0.0000000008\r\n",
      "\"alcayaga\"\t0.0000000008\r\n",
      "\"istedetfor\"\t0.0000000008\r\n",
      "\"frugall\"\t0.0000000008\r\n",
      "\"fixin's\"\t0.0000000008\r\n",
      "\"alcarria\"\t0.0000000008\r\n",
      "\"highheeled\"\t0.0000000008\r\n",
      "\"insterburg\"\t0.0000000008\r\n",
      "\"fixin\"\t0.0000000008\r\n",
      "\"farallones\"\t0.0000000008\r\n",
      "\"juro\"\t0.0000000008\r\n",
      "\"fructum\"\t0.0000000008\r\n",
      "\"arroquhar\"\t0.0000000008\r\n",
      "\"frsenum\"\t0.0000000008\r\n",
      "\"glumdalia\"\t0.0000000008\r\n"
     ]
    }
   ],
   "source": [
    "!tail -20 full_mostLeastDenseWords_5.4.1.c\n",
    "#TODO revert order probably"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word density MR stats\n",
    "\n",
    "    ec2_instance_type: m3.xlarge\n",
    "    num_ec2_instances: 15\n",
    "    \n",
    "__Step 1:__ \n",
    "\n",
    "    RUNNING for 649.2s  ~= 10 minutes      \n",
    "    Launched map tasks=190   \n",
    "    Launched reduce tasks=57     \n",
    "\n",
    "__Step 2:__  \n",
    "\n",
    "    RUNNING for 74.4s  ~= 1 minute    \n",
    "    Launched map tasks=110   \n",
    "    Launched reduce tasks=20   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.4.1 - D. Distribution of 5-gram sizes (character length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing distribution.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile distribution.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import time\n",
    "import logging\n",
    "\n",
    "class distribution(MRJob):\n",
    "    \n",
    "    # START STUDENT CODE 5.4.1.D\n",
    "    \n",
    "    MRJob.SORT_VALUES = True\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # Split line\n",
    "        splits = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        words = splits[0].lower().split()\n",
    "        \n",
    "        char_count = 0\n",
    "        \n",
    "        # Count characters\n",
    "        for word in words:\n",
    "            char_count += len(word)\n",
    "        \n",
    "        yield char_count, 1\n",
    "            \n",
    "    \n",
    "    def combiner(self, ngram_size, counts):\n",
    "        yield ngram_size, sum(count for count in counts)\n",
    "    \n",
    "    def reducer(self, ngram_size, counts):\n",
    "        yield ngram_size, sum(count for count in counts)\n",
    "\n",
    "    def steps(self):\n",
    "        \n",
    "        custom_jobconf = {\n",
    "            'stream.num.map.output.key.fields':'2',\n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k1,1n',\n",
    "            'mapred.reduce.tasks': '1'\n",
    "        }\n",
    "\n",
    "        return [\n",
    "                MRStep(\n",
    "                    jobconf=custom_jobconf,\n",
    "                    mapper=self.mapper,\n",
    "                    reducer=self.reducer,\n",
    "                    combiner = self.combiner)\n",
    "        ]\n",
    "\n",
    "    # END STUDENT CODE 5.4.1.D\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    distribution.run()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"\n",
    "    Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__On the test data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `5.3distributions_test/part-00000': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/distribution.nhaas.20170618.190220.067124\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nhaas/tmp/mrjob/distribution.nhaas.20170618.190220.067124/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob7181367499357633081.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1444\n",
      "  Submitted application application_1493936954640_1444\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1444/\n",
      "  Running job: job_1493936954640_1444\n",
      "  Job job_1493936954640_1444 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1444 completed successfully\n",
      "  Output directory: hdfs:///user/nhaas/tmp/mrjob/distribution.nhaas.20170618.190220.067124/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=563\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=45\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=68\n",
      "\t\tFILE: Number of bytes written=401224\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1007\n",
      "\t\tHDFS: Number of bytes written=45\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=15077376\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=12044800\n",
      "\t\tTotal time spent by all map tasks (ms)=9816\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=29448\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4705\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=23525\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=9816\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4705\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3060\n",
      "\t\tCombine input records=10\n",
      "\t\tCombine output records=10\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=108\n",
      "\t\tInput split bytes=444\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=60\n",
      "\t\tMap output materialized bytes=88\n",
      "\t\tMap output records=10\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1915658240\n",
      "\t\tReduce input groups=9\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=9\n",
      "\t\tReduce shuffle bytes=88\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=20\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7775621120\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nhaas/tmp/mrjob/distribution.nhaas.20170618.190220.067124/output...\n",
      "Removing HDFS temp directory hdfs:///user/nhaas/tmp/mrjob/distribution.nhaas.20170618.190220.067124...\n",
      "Removing temp directory /tmp/distribution.nhaas.20170618.190220.067124...\n",
      "WARNING:root:\n",
      "    Elapsed time: 56.0780830383 seconds\n",
      "    In minutes: 0.934634717306 mins\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r 5.3distributions_test/part-00000\n",
    "!python distribution.py -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt \\\n",
    "    --python-bin=./my_env/bin/python \\\n",
    "    --archive=\"hdfs:///user/nhaas/virtualenv/my_env.zip#my_env\" \\\n",
    "    > 5.3distributions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\t1\r\n",
      "18\t1\r\n",
      "19\t1\r\n",
      "20\t1\r\n",
      "22\t1\r\n",
      "23\t1\r\n",
      "24\t1\r\n",
      "25\t1\r\n",
      "29\t2\r\n"
     ]
    }
   ],
   "source": [
    "!cat 5.3distributions_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Histogram 10-line test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+MAAAGuCAYAAAD73ddLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWR0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f117cd2f710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "results_A = []\n",
    "for line in open(\"5.3distributions_test\").readlines():\n",
    "    line = line.strip()\n",
    "    X,Y = line.split(\"\\t\")\n",
    "    results_A.append([int(X),int(Y)])\n",
    "\n",
    "items = (np.array(results_A)[::-1].T)\n",
    "fig = pl.figure(figsize=(17,7))\n",
    "ax = pl.subplot(111)\n",
    "width=0.8\n",
    "ax.bar(range(len(items[0])), items[1], width=width)\n",
    "ax.set_xticks(np.arange(len(items[0])) + width/2)\n",
    "ax.set_xticklabels(items[0], rotation=90)\n",
    "ax.invert_xaxis()\n",
    "\n",
    "\n",
    "pl.title(\"Distributions of 5 Gram lengths in 10-line test\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the full data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `full_distribution_5.4.1.d': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/distribution.nhaas.20170618.190550.484166\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nhaas/tmp/mrjob/distribution.nhaas.20170618.190550.484166/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob96303865093154153.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 190\n",
      "  number of splits:190\n",
      "  Submitting tokens for job: job_1493936954640_1446\n",
      "  Submitted application application_1493936954640_1446\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1446/\n",
      "  Running job: job_1493936954640_1446\n",
      "  Job job_1493936954640_1446 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "  Task Id : attempt_1493936954640_1446_m_000050_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6c7fec65 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000049_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6c7fec65 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000047_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6c7fec65 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000045_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6c7fec65 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000048_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6c7fec65 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000044_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6c7fec65 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000051_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6c7fec65 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000046_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6c7fec65 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000057_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000053_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@582c2bdc rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000054_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000056_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000052_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@582c2bdc rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000086_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000075_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000062_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000077_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000071_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000158_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4d8fdd4e rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000084_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000073_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000065_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000060_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "   map 7% reduce 0%\n",
      "  Task Id : attempt_1493936954640_1446_m_000067_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000161_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4d8fdd4e rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000080_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000058_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000076_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000063_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000069_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000078_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000157_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4d8fdd4e rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000163_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4d8fdd4e rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "   map 0% reduce 0%\n",
      "  Task Id : attempt_1493936954640_1446_m_000059_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000159_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4d8fdd4e rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000082_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000072_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000066_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000085_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000055_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000087_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000162_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4d8fdd4e rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000061_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000074_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000064_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000070_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000160_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4d8fdd4e rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000083_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000164_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4d8fdd4e rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Task Id : attempt_1493936954640_1446_m_000081_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000079_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1446_m_000068_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@4f67101a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "   map 1% reduce 0%\n",
      "   map 2% reduce 0%\n",
      "   map 3% reduce 0%\n",
      "   map 4% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 6% reduce 0%\n",
      "   map 7% reduce 0%\n",
      "   map 9% reduce 0%\n",
      "   map 10% reduce 0%\n",
      "   map 11% reduce 0%\n",
      "   map 13% reduce 0%\n",
      "   map 14% reduce 0%\n",
      "   map 16% reduce 0%\n",
      "   map 18% reduce 0%\n",
      "   map 19% reduce 0%\n",
      "   map 21% reduce 0%\n",
      "   map 23% reduce 0%\n",
      "   map 24% reduce 0%\n",
      "   map 26% reduce 0%\n",
      "   map 28% reduce 0%\n",
      "   map 29% reduce 0%\n",
      "   map 31% reduce 0%\n",
      "   map 32% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 34% reduce 0%\n",
      "   map 35% reduce 0%\n",
      "   map 37% reduce 0%\n",
      "   map 39% reduce 0%\n",
      "   map 40% reduce 0%\n",
      "   map 42% reduce 0%\n",
      "   map 43% reduce 0%\n",
      "   map 44% reduce 0%\n",
      "   map 45% reduce 0%\n",
      "   map 46% reduce 0%\n",
      "   map 48% reduce 0%\n",
      "   map 49% reduce 0%\n",
      "   map 51% reduce 0%\n",
      "   map 53% reduce 0%\n",
      "   map 55% reduce 0%\n",
      "   map 57% reduce 0%\n",
      "   map 58% reduce 0%\n",
      "   map 59% reduce 0%\n",
      "   map 60% reduce 0%\n",
      "   map 61% reduce 0%\n",
      "   map 62% reduce 0%\n",
      "   map 63% reduce 0%\n",
      "   map 64% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 69% reduce 0%\n",
      "   map 70% reduce 0%\n",
      "   map 71% reduce 0%\n",
      "   map 72% reduce 0%\n",
      "   map 73% reduce 0%\n",
      "   map 74% reduce 0%\n",
      "   map 76% reduce 0%\n",
      "   map 77% reduce 0%\n",
      "   map 80% reduce 0%\n",
      "   map 81% reduce 0%\n",
      "   map 82% reduce 0%\n",
      "   map 84% reduce 0%\n",
      "   map 85% reduce 0%\n",
      "   map 86% reduce 0%\n",
      "   map 87% reduce 0%\n",
      "   map 88% reduce 0%\n",
      "   map 89% reduce 0%\n",
      "   map 90% reduce 0%\n",
      "   map 91% reduce 0%\n",
      "   map 92% reduce 0%\n",
      "   map 93% reduce 0%\n",
      "   map 95% reduce 0%\n",
      "   map 96% reduce 0%\n",
      "   map 97% reduce 0%\n",
      "   map 99% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1446 completed successfully\n",
      "  Output directory: hdfs:///user/nhaas/tmp/mrjob/distribution.nhaas.20170618.190550.484166/output\n",
      "Counters: 52\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=619\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=29296\n",
      "\t\tFILE: Number of bytes written=25556234\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2156101116\n",
      "\t\tHDFS: Number of bytes written=619\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=573\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=52\n",
      "\t\tKilled map tasks=2\n",
      "\t\tLaunched map tasks=243\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tOther local map tasks=55\n",
      "\t\tRack-local map tasks=188\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=17805316608\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=12930560\n",
      "\t\tTotal time spent by all map tasks (ms)=11592003\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=34776009\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5051\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=25255\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=11592003\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5051\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=4015120\n",
      "\t\tCombine input records=58682266\n",
      "\t\tCombine output records=9172\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=87145\n",
      "\t\tInput split bytes=32000\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=352088828\n",
      "\t\tMap output materialized bytes=79220\n",
      "\t\tMap output records=58682266\n",
      "\t\tMerged Map outputs=190\n",
      "\t\tPhysical memory (bytes) snapshot=154531594240\n",
      "\t\tReduce input groups=80\n",
      "\t\tReduce input records=9172\n",
      "\t\tReduce output records=80\n",
      "\t\tReduce shuffle bytes=79220\n",
      "\t\tShuffled Maps =190\n",
      "\t\tSpilled Records=18344\n",
      "\t\tTotal committed heap usage (bytes)=299992875008\n",
      "\t\tVirtual memory (bytes) snapshot=421026402304\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nhaas/tmp/mrjob/distribution.nhaas.20170618.190550.484166/output...\n",
      "Removing HDFS temp directory hdfs:///user/nhaas/tmp/mrjob/distribution.nhaas.20170618.190550.484166...\n",
      "Removing temp directory /tmp/distribution.nhaas.20170618.190550.484166...\n",
      "WARNING:root:\n",
      "    Elapsed time: 165.414644003 seconds\n",
      "    In minutes: 2.75691073338 mins\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r full_distribution_5.4.1.d\n",
    "!python distribution.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > full_distribution_5.4.1.d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### something here that cats out from the previous directory the mapper and reducer tasks\n",
    "### or, we can declare them and call them as args then print out the args\n",
    "### in this case:\n",
    "### Launched map tasks=2\n",
    "### Launched reduce tasks=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution MRJob stats\n",
    "\n",
    "__Step 1:__ \n",
    "\n",
    "    RUNNING for 157.8s ~= 2.6 minutes  \n",
    "    Launched map tasks=191  \n",
    "    Launched reduce tasks=16   \n",
    "    \n",
    "__Step 2:__  \n",
    "\n",
    "    RUNNING for 115.0s ~= 2 minutes   \n",
    "    Launched map tasks=139\n",
    "\tLaunched reduce tasks=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/kAAAG1CAYAAACiQDQyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRkKSDwAAACMhyQcAAICRkOQDAADASPw/IFBk1NJO8uYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f117c740c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "results_A = []\n",
    "for line in open(\"full_distribution_5.4.1.d\").readlines():\n",
    "    line = line.strip()\n",
    "    X,Y = line.split(\"\\t\")\n",
    "    results_A.append([int(X),int(Y)])\n",
    "\n",
    "items = (np.array(results_A)[::-1].T)\n",
    "fig = pl.figure(figsize=(17,7))\n",
    "ax = pl.subplot(111)\n",
    "width=0.8\n",
    "ax.bar(range(len(items[0])), items[1], width=width)\n",
    "ax.set_xticks(np.arange(len(items[0])) + width/2)\n",
    "ax.set_xticklabels(items[0], rotation=90)\n",
    "ax.invert_xaxis()\n",
    "\n",
    "\n",
    "pl.title(\"Distributions of 5 Gram lengths\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO revert order of counts in chart\n",
    "### NH - done, used ax.invert_xaxis()\n",
    "### http://matplotlib.org/devdocs/api/_as_gen/matplotlib.axes.Axes.invert_xaxis.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.4.2 <a name=\"5.4.2\"></a>OPTIONAL Question: log-log plots (PHASE 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?\n",
    "\n",
    "For more background see:\n",
    "- https://en.wikipedia.org/wiki/Log%E2%80%93log_plot\n",
    "- https://en.wikipedia.org/wiki/Power_law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.5  <a name=\"5.5\"></a> Synonym detection over 2Gig of Data with extra Preprocessing steps (HW5.3 plus some preprocessing)   (Phase 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "For the remainder of this assignment please feel free to eliminate stop words from your analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">There is also a corpus of stopwords, that is, high-frequency words like \"the\", \"to\" and \"also\" that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts. Python's nltk comes with a prebuilt list of stopwords (see below). Using this stopword list filter out these tokens from your analysis and rerun the experiments in 5.5 and disucuss the results of using a stopword list and without using a stopword list.\n",
    "\n",
    "> from nltk.corpus import stopwords\n",
    " stopwords.words('english')\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: A large subset of the Google n-grams dataset as was described above\n",
    "\n",
    "For each HW 5.4 -5.5.1 Please unit test and system test your code with respect \n",
    "to SYSTEMS TEST DATASET and show the results. \n",
    "Please compute the expected answer by hand and show your hand calculations for the \n",
    "SYSTEMS TEST DATASET. Then show the results you get with your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the assignment we will focus on developing methods for detecting synonyms, using the Google 5-grams dataset. At a high level:\n",
    "\n",
    "\n",
    "1. remove stopwords\n",
    "2. get 10,0000 most frequent\n",
    "3. get 1000 (9001-10000) features\n",
    "3. build stripes\n",
    "\n",
    "To accomplish this you must script two main tasks using MRJob:\n",
    "\n",
    "\n",
    "__TASK (1)__ Build stripes for the most frequent 10,000 words using cooccurence information based on\n",
    "the words ranked from 9001,-10,000 as a basis/vocabulary (drop stopword-like terms),\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "\n",
    "__TASK (2)__ Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "#### Design notes for TASK (1)\n",
    "For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "<*word,count>\n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for TASK (2).\n",
    "\n",
    "#### Design notes for _TASK (2)_\n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Jaccard\n",
    "- Cosine similarity\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Kendall correlation\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary, \n",
    "since your choice of association will be symmetric.\n",
    "\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. \n",
    "\n",
    "Please report the size of the cluster used and the amount of time it takes to run for the index construction task and for the synonym calculation task. How many pairs need to be processed (HINT: use the posting list length to calculate directly)? Report your  Cluster configuration!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example MR stats: (report times!)\n",
    "    took ~11 minutes on 5 m3.xlarge nodes\n",
    "    Data-local map tasks=188\n",
    "\tLaunched map tasks=190\n",
    "\tLaunched reduce tasks=15\n",
    "\tOther local map tasks=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# START STUDENT CODE 5.5\n",
    "# ADD OR REMOVE CELLS AS NEEDED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Frequency ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frequencies5_5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile frequencies5_5.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import time\n",
    "import logging\n",
    "\n",
    "class frequencies(MRJob):\n",
    "\n",
    "    # START STUDENT CODE 5.4.1.B\n",
    "    \n",
    "    MRJob.SORT_VALUES = True\n",
    "        \n",
    "    def __init__(self, args):\n",
    "        super(frequencies, self).__init__(args)\n",
    "        #self.min_rank = 9001\n",
    "        #self.max_rank = 10000 \n",
    "        self.current_rank = 0\n",
    "\n",
    "    def configure_options(self): \n",
    "        super(frequencies, self).configure_options() \n",
    "        self.add_passthrough_option('--min_rank', dest='min_rank', type='int', default=9001) \n",
    "        self.add_passthrough_option('--max_rank', dest='max_rank', type='int', default=10000) \n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "        # Split line\n",
    "        splits = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        words = splits[0].lower().split()\n",
    "        count = int(splits[1])\n",
    "        \n",
    "        for word in words:\n",
    "            yield word, count\n",
    "            \n",
    "    \n",
    "    def combiner(self, word, counts):\n",
    "        total = sum(count for count in counts)\n",
    "        yield word, total\n",
    "    \n",
    "    def reducer(self, word, counts):\n",
    "        total = sum(count for count in counts)\n",
    "        yield total, word\n",
    "    \n",
    "    def max_reducer(self, count, words):\n",
    "        \n",
    "        # Words come in frequency descending order here\n",
    "        # Only yield the words that are within the min and max frequency ranking desired\n",
    "        \n",
    "        for word in words:\n",
    "            self.current_rank += 1\n",
    "            \n",
    "            if self.current_rank >= self.options.min_rank and self.current_rank <= self.options.max_rank:\n",
    "                yield word, count\n",
    "\n",
    "    def steps(self):\n",
    "        \n",
    "        custom_jobconf = {\n",
    "            'stream.num.map.output.key.fields':'2',\n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k1,1nr',\n",
    "            'mapred.reduce.tasks': '1'\n",
    "        }\n",
    "\n",
    "        return [\n",
    "                MRStep(mapper=self.mapper,\n",
    "                    reducer=self.reducer,\n",
    "                    combiner = self.combiner),\n",
    "                MRStep(jobconf=custom_jobconf,\n",
    "                       reducer=self.max_reducer)\n",
    "                 ]\n",
    "\n",
    "    # END STUDENT CODE 5.4.1.B\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    frequencies.run()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"\n",
    "    Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency ranking on 10-line test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `frequencies_test5.5': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/frequencies5_5.nhaas.20170618.191041.546428\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nhaas/tmp/mrjob/frequencies5_5.nhaas.20170618.191041.546428/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob8695505202649490987.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1450\n",
      "  Submitted application application_1493936954640_1450\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1450/\n",
      "  Running job: job_1493936954640_1450\n",
      "  Job job_1493936954640_1450 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1450 completed successfully\n",
      "  Output directory: hdfs:///user/nhaas/tmp/mrjob/frequencies5_5.nhaas.20170618.191041.546428/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=563\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=357\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=430\n",
      "\t\tFILE: Number of bytes written=401243\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1011\n",
      "\t\tHDFS: Number of bytes written=357\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=14854656\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=18296320\n",
      "\t\tTotal time spent by all map tasks (ms)=9671\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=29013\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7147\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=35735\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=9671\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=7147\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2740\n",
      "\t\tCombine input records=50\n",
      "\t\tCombine output records=31\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=135\n",
      "\t\tInput split bytes=448\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=602\n",
      "\t\tMap output materialized bytes=458\n",
      "\t\tMap output records=50\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1894670336\n",
      "\t\tReduce input groups=28\n",
      "\t\tReduce input records=31\n",
      "\t\tReduce output records=28\n",
      "\t\tReduce shuffle bytes=458\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=62\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7737335808\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob1601828692919124958.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1493936954640_1452\n",
      "  Submitted application application_1493936954640_1452\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1452/\n",
      "  Running job: job_1493936954640_1452\n",
      "  Job job_1493936954640_1452 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Task Id : attempt_1493936954640_1452_r_000000_0, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@3e5adb0 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1452_r_000000_1, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@393da3a5 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1452_r_000000_2, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@1d727d07 rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "   map 100% reduce 100%\n",
      "  Job job_1493936954640_1452 completed successfully\n",
      "  Output directory: hdfs:///user/nhaas/tmp/mrjob/frequencies5_5.nhaas.20170618.191041.546428/output\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=536\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=40\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=396\n",
      "\t\tFILE: Number of bytes written=400456\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=898\n",
      "\t\tHDFS: Number of bytes written=40\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tFailed reduce tasks=3\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=21430272\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=32043520\n",
      "\t\tTotal time spent by all map tasks (ms)=13952\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=41856\n",
      "\t\tTotal time spent by all reduce tasks (ms)=12517\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=62585\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=13952\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=12517\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2590\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=117\n",
      "\t\tInput split bytes=362\n",
      "\t\tMap input records=28\n",
      "\t\tMap output bytes=385\n",
      "\t\tMap output materialized bytes=437\n",
      "\t\tMap output records=28\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1913962496\n",
      "\t\tReduce input groups=28\n",
      "\t\tReduce input records=28\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=437\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=56\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7759028224\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nhaas/tmp/mrjob/frequencies5_5.nhaas.20170618.191041.546428/output...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing HDFS temp directory hdfs:///user/nhaas/tmp/mrjob/frequencies5_5.nhaas.20170618.191041.546428...\n",
      "Removing temp directory /tmp/frequencies5_5.nhaas.20170618.191041.546428...\n",
      "WARNING:root:\n",
      "    Elapsed time: 108.236572981 seconds\n",
      "    In minutes: 1.80394288301 mins\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r frequencies_test5.5\n",
    "!python frequencies5_5.py --min_rank 2 --max_rank 4 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt \\\n",
    "    --python-bin=./my_env/bin/python \\\n",
    "    --archive=\"hdfs:///user/nhaas/virtualenv/my_env.zip#my_env\" \\\n",
    "    > frequencies_test5.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"in\"\t1201\r\n",
      "\"wales\"\t1099\r\n",
      "\"christmas\"\t1099\r\n"
     ]
    }
   ],
   "source": [
    "!cat frequencies_test5.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency ranking on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `frequencies5.5': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/frequencies5_5.nhaas.20170618.191353.087423\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nhaas/tmp/mrjob/frequencies5_5.nhaas.20170618.191353.087423/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob2533593669063093394.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 190\n",
      "  number of splits:190\n",
      "  Submitting tokens for job: job_1493936954640_1456\n",
      "  Submitted application application_1493936954640_1456\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1493936954640_1456/\n",
      "  Running job: job_1493936954640_1456\n",
      "  Job job_1493936954640_1456 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "  Task Id : attempt_1493936954640_1456_m_000006_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@2464e88a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1456_m_000001_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@2464e88a rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1456_m_000003_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6187f8fd rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1456_m_000009_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6187f8fd rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1456_m_000023_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6187f8fd rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1456_m_000012_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6187f8fd rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1456_m_000005_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6187f8fd rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1456_m_000008_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6187f8fd rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1456_m_000010_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6187f8fd rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1456_m_000020_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6187f8fd rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1456_m_000018_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6187f8fd rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1456_m_000011_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6187f8fd rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1456_m_000024_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6187f8fd rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1456_m_000014_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6187f8fd rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1456_m_000022_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6187f8fd rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1456_m_000004_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6187f8fd rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1456_m_000013_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6187f8fd rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n",
      "  Task Id : attempt_1493936954640_1456_m_000000_1000, Status : FAILED\n",
      "Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@6187f8fd rejected from java.util.concurrent.ThreadPoolExecutor@32a6cf5b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 30]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r frequencies5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ \\\n",
    "    --python-bin=./my_env/bin/python \\\n",
    "    --archive=\"hdfs:///user/nhaas/virtualenv/my_env.zip#my_env\" \\\n",
    "    > frequencies5.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"susceptibility\"\t878\r\n",
      "\"stooges\"\t878\r\n",
      "\"tribunals\"\t877\r\n",
      "\"parable\"\t877\r\n",
      "\"sanctuary\"\t877\r\n",
      "\"coupling\"\t876\r\n",
      "\"generic\"\t876\r\n",
      "\"collagen\"\t876\r\n",
      "\"clubs\"\t875\r\n",
      "\"fulness\"\t875\r\n",
      "\"se\"\t875\r\n",
      "\"pine\"\t875\r\n",
      "\"shelves\"\t874\r\n",
      "\"pigs\"\t874\r\n",
      "\"procuring\"\t874\r\n",
      "\"masterly\"\t874\r\n",
      "\"homogeneity\"\t874\r\n",
      "\"brigadier\"\t873\r\n",
      "\"humanities\"\t873\r\n",
      "\"mammalian\"\t873\r\n",
      "\"prolongation\"\t872\r\n",
      "\"rightly\"\t872\r\n",
      "\"instability\"\t872\r\n",
      "\"concerts\"\t871\r\n",
      "\"extracted\"\t871\r\n",
      "\"taylor\"\t871\r\n",
      "\"likeness\"\t871\r\n",
      "\"lee\"\t870\r\n",
      "\"tens\"\t870\r\n",
      "\"accepts\"\t870\r\n",
      "\"believers\"\t869\r\n",
      "\"deceive\"\t869\r\n",
      "\"exit\"\t869\r\n",
      "\"shipment\"\t869\r\n",
      "\"salient\"\t869\r\n",
      "\"lesbian\"\t869\r\n",
      "\"ragged\"\t868\r\n",
      "\"treachery\"\t868\r\n",
      "\"donor\"\t868\r\n",
      "\"au\"\t868\r\n",
      "\"guy\"\t868\r\n",
      "\"antique\"\t868\r\n",
      "\"asset\"\t868\r\n",
      "\"affective\"\t867\r\n",
      "\"cart\"\t867\r\n",
      "\"joys\"\t867\r\n",
      "\"arrives\"\t867\r\n",
      "\"rumor\"\t867\r\n",
      "\"specifics\"\t867\r\n",
      "\"nacional\"\t867\r\n",
      "\"surveys\"\t866\r\n",
      "\"label\"\t866\r\n",
      "\"momentous\"\t866\r\n",
      "\"preventive\"\t866\r\n",
      "\"eu\"\t866\r\n",
      "\"hampshire\"\t866\r\n",
      "\"disagree\"\t866\r\n",
      "\"fairy\"\t865\r\n",
      "\"commence\"\t865\r\n",
      "\"attaining\"\t865\r\n",
      "\"troublesome\"\t865\r\n",
      "\"weakened\"\t865\r\n",
      "\"taxed\"\t864\r\n",
      "\"rectus\"\t864\r\n",
      "\"cashier\"\t864\r\n",
      "\"driver\"\t864\r\n",
      "\"acceptor\"\t864\r\n",
      "\"juliet\"\t864\r\n",
      "\"compliment\"\t863\r\n",
      "\"forthcoming\"\t863\r\n",
      "\"maria\"\t863\r\n",
      "\"subdued\"\t863\r\n",
      "\"symptomatic\"\t863\r\n",
      "\"signifies\"\t863\r\n",
      "\"scenery\"\t862\r\n",
      "\"maid\"\t862\r\n",
      "\"clusters\"\t862\r\n",
      "\"goodbye\"\t862\r\n",
      "\"abhorrence\"\t862\r\n",
      "\"rash\"\t861\r\n",
      "\"samuel\"\t861\r\n",
      "\"strides\"\t861\r\n",
      "\"pharmacological\"\t861\r\n",
      "\"ok\"\t860\r\n",
      "\"occurrences\"\t860\r\n",
      "\"surviving\"\t860\r\n",
      "\"insofar\"\t860\r\n",
      "\"electorate\"\t859\r\n",
      "\"elite\"\t859\r\n",
      "\"proxy\"\t859\r\n",
      "\"refrigerator\"\t859\r\n",
      "\"mom\"\t859\r\n",
      "\"sore\"\t859\r\n",
      "\"stately\"\t858\r\n",
      "\"symposium\"\t858\r\n",
      "\"endanger\"\t858\r\n",
      "\"hailed\"\t858\r\n",
      "\"headings\"\t857\r\n",
      "\"earmarks\"\t857\r\n",
      "\"hinges\"\t857\r\n",
      "\"regiments\"\t857\r\n",
      "\"reverend\"\t856\r\n",
      "\"cochlear\"\t856\r\n",
      "\"heap\"\t856\r\n",
      "\"cruisers\"\t856\r\n",
      "\"consolidate\"\t856\r\n",
      "\"facilitated\"\t856\r\n",
      "\"impunity\"\t855\r\n",
      "\"bite\"\t855\r\n",
      "\"deposition\"\t855\r\n",
      "\"styloid\"\t855\r\n",
      "\"pink\"\t855\r\n",
      "\"professors\"\t855\r\n",
      "\"yourselves\"\t854\r\n",
      "\"solidarity\"\t854\r\n",
      "\"arched\"\t854\r\n",
      "\"infliction\"\t854\r\n",
      "\"inferences\"\t853\r\n",
      "\"hypertrophy\"\t853\r\n",
      "\"irregular\"\t853\r\n",
      "\"parentheses\"\t853\r\n",
      "\"seizing\"\t852\r\n",
      "\"rearing\"\t852\r\n",
      "\"sun's\"\t852\r\n",
      "\"restless\"\t852\r\n",
      "\"jutting\"\t852\r\n",
      "\"suffers\"\t852\r\n",
      "\"exertion\"\t852\r\n",
      "\"avowed\"\t852\r\n",
      "\"contacts\"\t851\r\n",
      "\"handkerchief\"\t851\r\n",
      "\"englishmen\"\t851\r\n",
      "\"infidel\"\t851\r\n",
      "\"subjective\"\t851\r\n",
      "\"secures\"\t851\r\n",
      "\"radiant\"\t850\r\n",
      "\"suez\"\t850\r\n",
      "\"weaknesses\"\t850\r\n",
      "\"fingertips\"\t850\r\n",
      "\"deferred\"\t850\r\n",
      "\"heading\"\t849\r\n",
      "\"combed\"\t849\r\n",
      "\"dublin\"\t849\r\n",
      "\"brush\"\t849\r\n",
      "\"professes\"\t849\r\n",
      "\"sur\"\t849\r\n",
      "\"proprietor\"\t849\r\n",
      "\"scared\"\t848\r\n",
      "\"objected\"\t848\r\n",
      "\"wife's\"\t848\r\n",
      "\"uncovered\"\t848\r\n",
      "\"bronchioles\"\t848\r\n",
      "\"benefited\"\t847\r\n",
      "\"bloodiest\"\t847\r\n",
      "\"fountain\"\t847\r\n",
      "\"hem\"\t847\r\n",
      "\"relish\"\t847\r\n",
      "\"swedish\"\t846\r\n",
      "\"unfit\"\t846\r\n",
      "\"thereon\"\t846\r\n",
      "\"foreseen\"\t846\r\n",
      "\"erie\"\t846\r\n",
      "\"closet\"\t846\r\n",
      "\"intolerable\"\t845\r\n",
      "\"dissatisfaction\"\t845\r\n",
      "\"chatted\"\t845\r\n",
      "\"foreigners\"\t845\r\n",
      "\"mail\"\t845\r\n",
      "\"reversals\"\t845\r\n",
      "\"pharisees\"\t845\r\n",
      "\"sigma\"\t845\r\n",
      "\"submucosa\"\t844\r\n",
      "\"wells\"\t844\r\n",
      "\"mutation\"\t844\r\n",
      "\"absurdity\"\t844\r\n",
      "\"expectancy\"\t844\r\n",
      "\"adolescent\"\t844\r\n",
      "\"disparity\"\t844\r\n",
      "\"denomination\"\t844\r\n",
      "\"await\"\t843\r\n",
      "\"callosum\"\t843\r\n",
      "\"accustom\"\t843\r\n",
      "\"subsidiary\"\t843\r\n",
      "\"packet\"\t842\r\n",
      "\"slot\"\t842\r\n",
      "\"simplification\"\t842\r\n",
      "\"maturation\"\t842\r\n",
      "\"ascent\"\t842\r\n",
      "\"gentry\"\t842\r\n",
      "\"contended\"\t842\r\n",
      "\"cannon\"\t842\r\n",
      "\"capabilities\"\t841\r\n",
      "\"distilled\"\t841\r\n",
      "\"bethlehem\"\t841\r\n",
      "\"inflict\"\t841\r\n",
      "\"exemplified\"\t841\r\n",
      "\"silly\"\t841\r\n",
      "\"po\"\t841\r\n",
      "\"sanitary\"\t841\r\n",
      "\"rumors\"\t841\r\n",
      "\"macedonians\"\t841\r\n",
      "\"sanctions\"\t841\r\n",
      "\"overlap\"\t840\r\n",
      "\"resumption\"\t840\r\n",
      "\"mills\"\t840\r\n",
      "\"subjection\"\t840\r\n",
      "\"placement\"\t840\r\n",
      "\"scotia\"\t840\r\n",
      "\"copied\"\t840\r\n",
      "\"chihuahua\"\t839\r\n",
      "\"pendulum\"\t839\r\n",
      "\"reproductive\"\t839\r\n",
      "\"laborer\"\t839\r\n",
      "\"sophisticated\"\t839\r\n",
      "\"wretch\"\t839\r\n",
      "\"parting\"\t838\r\n",
      "\"styles\"\t838\r\n",
      "\"disregard\"\t838\r\n",
      "\"chemicals\"\t838\r\n",
      "\"boom\"\t838\r\n",
      "\"constancy\"\t838\r\n",
      "\"amended\"\t837\r\n",
      "\"belize\"\t837\r\n",
      "\"challenging\"\t837\r\n",
      "\"baptism\"\t837\r\n",
      "\"surveillance\"\t837\r\n",
      "\"owes\"\t836\r\n",
      "\"peece\"\t836\r\n",
      "\"drowned\"\t836\r\n",
      "\"dramatically\"\t835\r\n",
      "\"charitable\"\t835\r\n",
      "\"antigens\"\t835\r\n",
      "\"strand\"\t835\r\n",
      "\"warnings\"\t835\r\n",
      "\"unification\"\t835\r\n",
      "\"locations\"\t834\r\n",
      "\"subversion\"\t834\r\n",
      "\"arduous\"\t834\r\n",
      "\"innovations\"\t834\r\n",
      "\"hereunto\"\t834\r\n",
      "\"discrete\"\t834\r\n",
      "\"andean\"\t833\r\n",
      "\"contradictory\"\t833\r\n",
      "\"repeatedly\"\t833\r\n",
      "\"vocabulary\"\t833\r\n",
      "\"swearing\"\t832\r\n",
      "\"vault\"\t832\r\n",
      "\"reinforcement\"\t832\r\n",
      "\"waived\"\t832\r\n",
      "\"constrained\"\t832\r\n",
      "\"advocated\"\t831\r\n",
      "\"effectually\"\t831\r\n",
      "\"excommunicated\"\t831\r\n",
      "\"sights\"\t831\r\n",
      "\"norfolk\"\t831\r\n",
      "\"resumed\"\t831\r\n",
      "\"alluded\"\t830\r\n",
      "\"cooled\"\t830\r\n",
      "\"anxiously\"\t830\r\n",
      "\"aliquot\"\t829\r\n",
      "\"designer\"\t829\r\n",
      "\"cataract\"\t829\r\n",
      "\"taxpayer\"\t829\r\n",
      "\"sessions\"\t829\r\n",
      "\"provoked\"\t829\r\n",
      "\"stigma\"\t829\r\n",
      "\"woke\"\t828\r\n",
      "\"assaults\"\t828\r\n",
      "\"denotes\"\t828\r\n",
      "\"decorated\"\t828\r\n",
      "\"mainland\"\t827\r\n",
      "\"spectacle\"\t827\r\n",
      "\"photographs\"\t826\r\n",
      "\"unlimited\"\t826\r\n",
      "\"manipulation\"\t826\r\n",
      "\"bladder\"\t826\r\n",
      "\"iliac\"\t826\r\n",
      "\"displaced\"\t825\r\n",
      "\"brushed\"\t825\r\n",
      "\"stump\"\t825\r\n",
      "\"oscillation\"\t825\r\n",
      "\"sang\"\t825\r\n",
      "\"denying\"\t824\r\n",
      "\"cottage\"\t823\r\n",
      "\"caution\"\t823\r\n",
      "\"backed\"\t823\r\n",
      "\"coil\"\t823\r\n",
      "\"contests\"\t823\r\n",
      "\"brigands\"\t823\r\n",
      "\"anti\"\t823\r\n",
      "\"pit\"\t823\r\n",
      "\"rs\"\t823\r\n",
      "\"tasting\"\t823\r\n",
      "\"accompaniment\"\t822\r\n",
      "\"conduction\"\t822\r\n",
      "\"gaping\"\t822\r\n",
      "\"hasten\"\t822\r\n",
      "\"feathers\"\t821\r\n",
      "\"fetch\"\t821\r\n",
      "\"cardinals\"\t821\r\n",
      "\"dined\"\t821\r\n",
      "\"peritoneal\"\t821\r\n",
      "\"trademarks\"\t820\r\n",
      "\"korean\"\t820\r\n",
      "\"retrieve\"\t820\r\n",
      "\"inconvenient\"\t820\r\n",
      "\"curved\"\t820\r\n",
      "\"aggravated\"\t820\r\n",
      "\"atrium\"\t820\r\n",
      "\"entity\"\t820\r\n",
      "\"authenticity\"\t819\r\n",
      "\"impurities\"\t819\r\n",
      "\"indicative\"\t819\r\n",
      "\"simplified\"\t819\r\n",
      "\"nazareth\"\t819\r\n",
      "\"summons\"\t819\r\n",
      "\"serotonin\"\t819\r\n",
      "\"splendor\"\t819\r\n",
      "\"shifting\"\t819\r\n",
      "\"aboard\"\t818\r\n",
      "\"flanders\"\t818\r\n",
      "\"careers\"\t818\r\n",
      "\"falsehood\"\t818\r\n",
      "\"amazement\"\t817\r\n",
      "\"iowa\"\t817\r\n",
      "\"jew\"\t817\r\n",
      "\"collector\"\t817\r\n",
      "\"locality\"\t817\r\n",
      "\"untrue\"\t817\r\n",
      "\"peered\"\t817\r\n",
      "\"midpoint\"\t817\r\n",
      "\"molten\"\t816\r\n",
      "\"surveyed\"\t816\r\n",
      "\"norwegian\"\t816\r\n",
      "\"vie\"\t816\r\n",
      "\"missions\"\t815\r\n",
      "\"suppressed\"\t815\r\n",
      "\"predictor\"\t815\r\n",
      "\"theorem\"\t815\r\n",
      "\"nearby\"\t815\r\n",
      "\"seizure\"\t815\r\n",
      "\"inclusive\"\t815\r\n",
      "\"curvature\"\t815\r\n",
      "\"bows\"\t815\r\n",
      "\"bail\"\t815\r\n",
      "\"binary\"\t814\r\n",
      "\"virtual\"\t814\r\n",
      "\"politically\"\t813\r\n",
      "\"microcosm\"\t813\r\n",
      "\"hiv\"\t813\r\n",
      "\"groans\"\t813\r\n",
      "\"foreword\"\t813\r\n",
      "\"employing\"\t813\r\n",
      "\"beheld\"\t813\r\n",
      "\"damaged\"\t813\r\n",
      "\"flank\"\t813\r\n",
      "\"grove\"\t812\r\n",
      "\"designate\"\t812\r\n",
      "\"furtherance\"\t812\r\n",
      "\"aunt\"\t812\r\n",
      "\"football\"\t812\r\n",
      "\"trait\"\t812\r\n",
      "\"reproducing\"\t812\r\n",
      "\"sober\"\t812\r\n",
      "\"saturation\"\t812\r\n",
      "\"translate\"\t810\r\n",
      "\"respectively\"\t810\r\n",
      "\"provisional\"\t810\r\n",
      "\"refractory\"\t810\r\n",
      "\"craft\"\t810\r\n",
      "\"aiding\"\t810\r\n",
      "\"cherished\"\t810\r\n",
      "\"inquiring\"\t809\r\n",
      "\"endued\"\t809\r\n",
      "\"imminent\"\t809\r\n",
      "\"stumbled\"\t809\r\n",
      "\"vile\"\t809\r\n",
      "\"pearl\"\t808\r\n",
      "\"entities\"\t808\r\n",
      "\"benign\"\t808\r\n",
      "\"afterward\"\t808\r\n",
      "\"industrializing\"\t808\r\n",
      "\"alienating\"\t808\r\n",
      "\"adhesive\"\t808\r\n",
      "\"admits\"\t807\r\n",
      "\"rainfall\"\t807\r\n",
      "\"plaintiff's\"\t807\r\n",
      "\"reigns\"\t807\r\n",
      "\"researcher\"\t807\r\n",
      "\"polish\"\t806\r\n",
      "\"underdeveloped\"\t806\r\n",
      "\"judicious\"\t806\r\n",
      "\"bacilli\"\t806\r\n",
      "\"adjourned\"\t805\r\n",
      "\"dilatation\"\t805\r\n",
      "\"sampling\"\t805\r\n",
      "\"pave\"\t805\r\n",
      "\"pretends\"\t805\r\n",
      "\"vestiges\"\t805\r\n",
      "\"tidings\"\t805\r\n",
      "\"terrorist\"\t805\r\n",
      "\"symbolism\"\t804\r\n",
      "\"terrace\"\t804\r\n",
      "\"rarity\"\t804\r\n",
      "\"unprecedented\"\t804\r\n",
      "\"paramount\"\t804\r\n",
      "\"grotesque\"\t804\r\n",
      "\"governance\"\t803\r\n",
      "\"elegant\"\t803\r\n",
      "\"coarse\"\t803\r\n",
      "\"franciscans\"\t803\r\n",
      "\"programmes\"\t803\r\n",
      "\"sings\"\t803\r\n",
      "\"uneasiness\"\t802\r\n",
      "\"listener\"\t802\r\n",
      "\"meals\"\t802\r\n",
      "\"polls\"\t802\r\n",
      "\"relegated\"\t802\r\n",
      "\"ingenuity\"\t802\r\n",
      "\"embarrassed\"\t801\r\n",
      "\"epsilon\"\t801\r\n",
      "\"automatic\"\t801\r\n",
      "\"privacy\"\t801\r\n",
      "\"sometime\"\t801\r\n",
      "\"vogue\"\t800\r\n",
      "\"lays\"\t800\r\n",
      "\"racism\"\t800\r\n",
      "\"perpetually\"\t800\r\n",
      "\"gracious\"\t800\r\n",
      "\"analyzing\"\t800\r\n",
      "\"drives\"\t800\r\n",
      "\"fortified\"\t799\r\n",
      "\"nominate\"\t799\r\n",
      "\"heal\"\t798\r\n",
      "\"invented\"\t798\r\n",
      "\"inspector\"\t797\r\n",
      "\"hesitation\"\t797\r\n",
      "\"pharynx\"\t797\r\n",
      "\"overtaken\"\t796\r\n",
      "\"deduced\"\t796\r\n",
      "\"expressive\"\t796\r\n",
      "\"compares\"\t796\r\n",
      "\"coverage\"\t796\r\n",
      "\"arabia\"\t796\r\n",
      "\"advertisement\"\t796\r\n",
      "\"encroached\"\t795\r\n",
      "\"lewis\"\t795\r\n",
      "\"socialists\"\t795\r\n",
      "\"reporter\"\t795\r\n",
      "\"vacant\"\t795\r\n",
      "\"rings\"\t794\r\n",
      "\"recueil\"\t794\r\n",
      "\"lois\"\t794\r\n",
      "\"answering\"\t794\r\n",
      "\"clues\"\t794\r\n",
      "\"fossil\"\t794\r\n",
      "\"funded\"\t794\r\n",
      "\"chill\"\t794\r\n",
      "\"anciennes\"\t794\r\n",
      "\"challenged\"\t794\r\n",
      "\"reinforce\"\t793\r\n",
      "\"tedious\"\t793\r\n",
      "\"papal\"\t793\r\n",
      "\"soweth\"\t792\r\n",
      "\"sterile\"\t792\r\n",
      "\"restrained\"\t792\r\n",
      "\"eventual\"\t792\r\n",
      "\"fibre\"\t792\r\n",
      "\"composer\"\t792\r\n",
      "\"hastened\"\t791\r\n",
      "\"blaze\"\t791\r\n",
      "\"configuration\"\t791\r\n",
      "\"danish\"\t791\r\n",
      "\"economists\"\t791\r\n",
      "\"virtuous\"\t791\r\n",
      "\"reflex\"\t791\r\n",
      "\"pepper\"\t791\r\n",
      "\"patrons\"\t790\r\n",
      "\"condolence\"\t790\r\n",
      "\"calculus\"\t790\r\n",
      "\"fugitives\"\t790\r\n",
      "\"chalk\"\t789\r\n",
      "\"israeli\"\t789\r\n",
      "\"abstraction\"\t789\r\n",
      "\"coolidge\"\t789\r\n",
      "\"executors\"\t789\r\n",
      "\"pilgrim\"\t789\r\n",
      "\"rearranged\"\t789\r\n",
      "\"platelet\"\t789\r\n",
      "\"silica\"\t789\r\n",
      "\"malaysian\"\t788\r\n",
      "\"crowned\"\t788\r\n",
      "\"emigration\"\t788\r\n",
      "\"envisaged\"\t787\r\n",
      "\"distinguishes\"\t787\r\n",
      "\"graceful\"\t787\r\n",
      "\"hoover\"\t787\r\n",
      "\"infinitesimal\"\t787\r\n",
      "\"velvet\"\t787\r\n",
      "\"stringent\"\t786\r\n",
      "\"rejoiced\"\t786\r\n",
      "\"notices\"\t786\r\n",
      "\"enlarge\"\t786\r\n",
      "\"cooperative\"\t786\r\n",
      "\"informing\"\t785\r\n",
      "\"complexes\"\t785\r\n",
      "\"hateful\"\t785\r\n",
      "\"clearness\"\t785\r\n",
      "\"experiencing\"\t785\r\n",
      "\"tightly\"\t785\r\n",
      "\"wolf\"\t785\r\n",
      "\"overhead\"\t785\r\n",
      "\"westerly\"\t784\r\n",
      "\"antagonistic\"\t784\r\n",
      "\"inhibition\"\t784\r\n",
      "\"destination\"\t784\r\n",
      "\"castles\"\t783\r\n",
      "\"damaging\"\t783\r\n",
      "\"anastomosis\"\t783\r\n",
      "\"negation\"\t783\r\n",
      "\"rationalism\"\t783\r\n",
      "\"supplications\"\t783\r\n",
      "\"tv\"\t782\r\n",
      "\"necks\"\t782\r\n",
      "\"defences\"\t782\r\n",
      "\"dexterity\"\t782\r\n",
      "\"grounded\"\t782\r\n",
      "\"iranian\"\t781\r\n",
      "\"allotted\"\t781\r\n",
      "\"downs\"\t781\r\n",
      "\"ram\"\t781\r\n",
      "\"preamble\"\t781\r\n",
      "\"vigour\"\t781\r\n",
      "\"nationalities\"\t781\r\n",
      "\"rupture\"\t780\r\n",
      "\"listens\"\t780\r\n",
      "\"rugged\"\t780\r\n",
      "\"transfers\"\t780\r\n",
      "\"desolate\"\t780\r\n",
      "\"campus\"\t780\r\n",
      "\"chorus\"\t780\r\n",
      "\"cherish\"\t780\r\n",
      "\"consul\"\t780\r\n",
      "\"excesses\"\t779\r\n",
      "\"eloquence\"\t779\r\n",
      "\"incentives\"\t779\r\n",
      "\"excel\"\t779\r\n",
      "\"crying\"\t779\r\n",
      "\"consented\"\t779\r\n",
      "\"adjunct\"\t779\r\n",
      "\"ratify\"\t779\r\n",
      "\"sovereigns\"\t779\r\n",
      "\"vastly\"\t779\r\n",
      "\"scraps\"\t778\r\n",
      "\"hepatic\"\t778\r\n",
      "\"cumulative\"\t778\r\n",
      "\"contradict\"\t777\r\n",
      "\"doubled\"\t777\r\n",
      "\"destroying\"\t777\r\n",
      "\"volcano\"\t777\r\n",
      "\"zur\"\t777\r\n",
      "\"sub\"\t776\r\n",
      "\"sorrows\"\t776\r\n",
      "\"literal\"\t776\r\n",
      "\"circumscribed\"\t776\r\n",
      "\"damp\"\t775\r\n",
      "\"cardiogenic\"\t775\r\n",
      "\"drops\"\t775\r\n",
      "\"determinations\"\t774\r\n",
      "\"abrupt\"\t774\r\n",
      "\"authorize\"\t774\r\n",
      "\"lighter\"\t774\r\n",
      "\"veneration\"\t774\r\n",
      "\"prospective\"\t774\r\n",
      "\"sentimental\"\t774\r\n",
      "\"ss\"\t773\r\n",
      "\"sunlight\"\t773\r\n",
      "\"faded\"\t773\r\n",
      "\"extinguished\"\t773\r\n",
      "\"infamous\"\t773\r\n",
      "\"drafts\"\t773\r\n",
      "\"disclosure\"\t773\r\n",
      "\"insulted\"\t773\r\n",
      "\"irrigation\"\t773\r\n",
      "\"induction\"\t773\r\n",
      "\"backbone\"\t773\r\n",
      "\"alienation\"\t773\r\n",
      "\"formulating\"\t772\r\n",
      "\"fifths\"\t772\r\n",
      "\"dean\"\t772\r\n",
      "\"resorted\"\t772\r\n",
      "\"revealing\"\t772\r\n",
      "\"presiding\"\t771\r\n",
      "\"peru\"\t771\r\n",
      "\"reservoir\"\t771\r\n",
      "\"comic\"\t771\r\n",
      "\"carbohydrate\"\t771\r\n",
      "\"extant\"\t771\r\n",
      "\"eloquent\"\t771\r\n",
      "\"empires\"\t771\r\n",
      "\"addicted\"\t771\r\n",
      "\"complimentary\"\t771\r\n",
      "\"imaging\"\t770\r\n",
      "\"civic\"\t770\r\n",
      "\"thirties\"\t770\r\n",
      "\"mushrooms\"\t770\r\n",
      "\"supporter\"\t770\r\n",
      "\"poorer\"\t769\r\n",
      "\"sustainable\"\t769\r\n",
      "\"speedily\"\t769\r\n",
      "\"gallant\"\t769\r\n",
      "\"divers\"\t768\r\n",
      "\"fin\"\t768\r\n",
      "\"com\"\t768\r\n",
      "\"doubling\"\t768\r\n",
      "\"angola\"\t768\r\n",
      "\"pernicious\"\t768\r\n",
      "\"lingering\"\t768\r\n",
      "\"overtake\"\t768\r\n",
      "\"ounce\"\t768\r\n",
      "\"galleys\"\t767\r\n",
      "\"inspire\"\t767\r\n",
      "\"apology\"\t767\r\n",
      "\"anomalies\"\t767\r\n",
      "\"judiciary\"\t767\r\n",
      "\"bankruptcy\"\t766\r\n",
      "\"thronged\"\t766\r\n",
      "\"perseverance\"\t765\r\n",
      "\"brother's\"\t765\r\n",
      "\"brook\"\t765\r\n",
      "\"huron\"\t764\r\n",
      "\"holiness\"\t763\r\n",
      "\"distention\"\t763\r\n",
      "\"directory\"\t763\r\n",
      "\"endurance\"\t763\r\n",
      "\"vertebra\"\t763\r\n",
      "\"stuart\"\t763\r\n",
      "\"reflecting\"\t763\r\n",
      "\"warrior\"\t763\r\n",
      "\"namely\"\t763\r\n",
      "\"vanished\"\t762\r\n",
      "\"profitably\"\t762\r\n",
      "\"roused\"\t762\r\n",
      "\"circumference\"\t762\r\n",
      "\"capricious\"\t762\r\n",
      "\"coronation\"\t762\r\n",
      "\"creativity\"\t762\r\n",
      "\"discontent\"\t762\r\n",
      "\"impatience\"\t761\r\n",
      "\"embody\"\t761\r\n",
      "\"bags\"\t761\r\n",
      "\"logically\"\t761\r\n",
      "\"underworld\"\t761\r\n",
      "\"visions\"\t761\r\n",
      "\"papua\"\t761\r\n",
      "\"pretense\"\t760\r\n",
      "\"oft\"\t760\r\n",
      "\"infallibility\"\t760\r\n",
      "\"cadillac\"\t760\r\n",
      "\"cunning\"\t760\r\n",
      "\"freud's\"\t759\r\n",
      "\"dominance\"\t759\r\n",
      "\"contaminated\"\t759\r\n",
      "\"eminence\"\t759\r\n",
      "\"trivial\"\t759\r\n",
      "\"strove\"\t759\r\n",
      "\"utilize\"\t759\r\n",
      "\"musket\"\t759\r\n",
      "\"penetrated\"\t758\r\n",
      "\"zippor\"\t758\r\n",
      "\"wrongs\"\t758\r\n",
      "\"specially\"\t758\r\n",
      "\"unpopular\"\t758\r\n",
      "\"balak\"\t758\r\n",
      "\"inaccessible\"\t758\r\n",
      "\"homologous\"\t757\r\n",
      "\"attracting\"\t757\r\n",
      "\"camps\"\t757\r\n",
      "\"babies\"\t757\r\n",
      "\"deliberations\"\t757\r\n",
      "\"mellitus\"\t757\r\n",
      "\"premier\"\t757\r\n",
      "\"notch\"\t756\r\n",
      "\"chromosome\"\t756\r\n",
      "\"disconcerted\"\t756\r\n",
      "\"judith\"\t756\r\n",
      "\"bleed\"\t756\r\n",
      "\"harnessed\"\t755\r\n",
      "\"reinforced\"\t755\r\n",
      "\"manifesto\"\t755\r\n",
      "\"sends\"\t755\r\n",
      "\"sculpture\"\t755\r\n",
      "\"tremble\"\t754\r\n",
      "\"weakening\"\t754\r\n",
      "\"monstrous\"\t754\r\n",
      "\"crane\"\t754\r\n",
      "\"fourier\"\t754\r\n",
      "\"den\"\t753\r\n",
      "\"conveying\"\t753\r\n",
      "\"albert\"\t753\r\n",
      "\"educating\"\t753\r\n",
      "\"basically\"\t752\r\n",
      "\"characterize\"\t752\r\n",
      "\"destitute\"\t752\r\n",
      "\"relics\"\t752\r\n",
      "\"quicker\"\t752\r\n",
      "\"tough\"\t752\r\n",
      "\"pangs\"\t752\r\n",
      "\"needy\"\t752\r\n",
      "\"lodging\"\t752\r\n",
      "\"upheld\"\t751\r\n",
      "\"perpetuate\"\t751\r\n",
      "\"rabbit\"\t751\r\n",
      "\"blockade\"\t751\r\n",
      "\"bridges\"\t750\r\n",
      "\"retina\"\t750\r\n",
      "\"sift\"\t750\r\n",
      "\"vienna\"\t749\r\n",
      "\"wandering\"\t749\r\n",
      "\"microscope\"\t749\r\n",
      "\"assemblies\"\t749\r\n",
      "\"incur\"\t749\r\n",
      "\"formative\"\t749\r\n",
      "\"standardization\"\t748\r\n",
      "\"traditionally\"\t748\r\n",
      "\"que\"\t748\r\n",
      "\"obstinate\"\t747\r\n",
      "\"widening\"\t747\r\n",
      "\"israelites\"\t747\r\n",
      "\"identifying\"\t747\r\n",
      "\"causally\"\t747\r\n",
      "\"distorted\"\t747\r\n",
      "\"grandchildren\"\t747\r\n",
      "\"conscientious\"\t746\r\n",
      "\"hygiene\"\t746\r\n",
      "\"monastic\"\t746\r\n",
      "\"qualify\"\t746\r\n",
      "\"railways\"\t746\r\n",
      "\"memorials\"\t746\r\n",
      "\"preclude\"\t745\r\n",
      "\"deliberately\"\t745\r\n",
      "\"individuality\"\t745\r\n",
      "\"birmingham\"\t745\r\n",
      "\"executrix\"\t745\r\n",
      "\"escaping\"\t744\r\n",
      "\"castro\"\t744\r\n",
      "\"frederick\"\t744\r\n",
      "\"absolve\"\t744\r\n",
      "\"diametrically\"\t744\r\n",
      "\"santo\"\t744\r\n",
      "\"token\"\t744\r\n",
      "\"tavern\"\t744\r\n",
      "\"offender\"\t744\r\n",
      "\"specification\"\t744\r\n",
      "\"topical\"\t744\r\n",
      "\"simon\"\t744\r\n",
      "\"longed\"\t743\r\n",
      "\"trick\"\t743\r\n",
      "\"terrors\"\t743\r\n",
      "\"baltimore\"\t743\r\n",
      "\"cyrus\"\t743\r\n",
      "\"insulation\"\t743\r\n",
      "\"brains\"\t743\r\n",
      "\"accomplishments\"\t742\r\n",
      "\"toe\"\t742\r\n",
      "\"speculated\"\t742\r\n",
      "\"sells\"\t742\r\n",
      "\"scots\"\t742\r\n",
      "\"throws\"\t741\r\n",
      "\"suorum\"\t741\r\n",
      "\"parium\"\t741\r\n",
      "\"surety\"\t741\r\n",
      "\"legale\"\t741\r\n",
      "\"recognizing\"\t741\r\n",
      "\"highlight\"\t741\r\n",
      "\"backgrounds\"\t741\r\n",
      "\"judicium\"\t741\r\n",
      "\"adrenaline\"\t740\r\n",
      "\"backing\"\t740\r\n",
      "\"edict\"\t740\r\n",
      "\"inducing\"\t740\r\n",
      "\"excluding\"\t739\r\n",
      "\"eleventh\"\t739\r\n",
      "\"heavier\"\t739\r\n",
      "\"fruition\"\t739\r\n",
      "\"hemodialysis\"\t739\r\n",
      "\"mohamad\"\t739\r\n",
      "\"mahathir\"\t739\r\n",
      "\"norway\"\t739\r\n",
      "\"rebuilt\"\t738\r\n",
      "\"soils\"\t738\r\n",
      "\"surrounds\"\t738\r\n",
      "\"dumb\"\t738\r\n",
      "\"invaders\"\t738\r\n",
      "\"delimitation\"\t738\r\n",
      "\"barons\"\t738\r\n",
      "\"deficient\"\t738\r\n",
      "\"clothe\"\t737\r\n",
      "\"decency\"\t737\r\n",
      "\"belligerents\"\t737\r\n",
      "\"dictionary\"\t737\r\n",
      "\"revenge\"\t737\r\n",
      "\"storming\"\t737\r\n",
      "\"silicon\"\t737\r\n",
      "\"upshot\"\t736\r\n",
      "\"shower\"\t736\r\n",
      "\"ventilation\"\t736\r\n",
      "\"mists\"\t736\r\n",
      "\"withdrawing\"\t736\r\n",
      "\"banker\"\t736\r\n",
      "\"honours\"\t736\r\n",
      "\"aspiration\"\t735\r\n",
      "\"inhibitors\"\t735\r\n",
      "\"apprehensive\"\t735\r\n",
      "\"fibrous\"\t735\r\n",
      "\"concur\"\t735\r\n",
      "\"abruptly\"\t735\r\n",
      "\"reciprocal\"\t735\r\n",
      "\"plead\"\t735\r\n",
      "\"spinning\"\t735\r\n",
      "\"vatican\"\t735\r\n",
      "\"tanzania\"\t734\r\n",
      "\"couched\"\t734\r\n",
      "\"declarations\"\t734\r\n",
      "\"hosts\"\t734\r\n",
      "\"aptitude\"\t733\r\n",
      "\"breaker\"\t733\r\n",
      "\"confederates\"\t733\r\n",
      "\"hull\"\t733\r\n",
      "\"toy\"\t733\r\n",
      "\"remembering\"\t732\r\n",
      "\"undisturbed\"\t732\r\n",
      "\"trifle\"\t732\r\n",
      "\"miraculous\"\t732\r\n",
      "\"coordination\"\t732\r\n",
      "\"affixed\"\t732\r\n",
      "\"conclusively\"\t732\r\n",
      "\"farming\"\t731\r\n",
      "\"humanly\"\t731\r\n",
      "\"elizabethan\"\t731\r\n",
      "\"adjustments\"\t731\r\n",
      "\"stanley\"\t731\r\n",
      "\"pre\"\t731\r\n",
      "\"rails\"\t731\r\n",
      "\"purport\"\t731\r\n",
      "\"scratch\"\t731\r\n",
      "\"mammals\"\t730\r\n",
      "\"who's\"\t730\r\n",
      "\"multitudes\"\t730\r\n",
      "\"florence\"\t730\r\n",
      "\"augmentation\"\t730\r\n",
      "\"indefinite\"\t729\r\n",
      "\"demon\"\t729\r\n",
      "\"mould\"\t729\r\n",
      "\"prussian\"\t729\r\n",
      "\"stove\"\t729\r\n",
      "\"simulation\"\t728\r\n",
      "\"subordinated\"\t728\r\n",
      "\"secretly\"\t728\r\n",
      "\"locate\"\t728\r\n",
      "\"glowing\"\t728\r\n",
      "\"insulin\"\t727\r\n",
      "\"chronology\"\t727\r\n",
      "\"cured\"\t727\r\n",
      "\"allocated\"\t727\r\n",
      "\"melt\"\t727\r\n",
      "\"singly\"\t727\r\n",
      "\"realism\"\t727\r\n",
      "\"pleaded\"\t727\r\n",
      "\"touches\"\t726\r\n",
      "\"numbered\"\t726\r\n",
      "\"organization's\"\t726\r\n",
      "\"glories\"\t726\r\n",
      "\"goat\"\t725\r\n",
      "\"fashioned\"\t725\r\n",
      "\"deference\"\t725\r\n",
      "\"hymns\"\t725\r\n",
      "\"steer\"\t725\r\n",
      "\"triple\"\t725\r\n",
      "\"spectator\"\t725\r\n",
      "\"reconstructed\"\t724\r\n",
      "\"twin\"\t724\r\n",
      "\"liberate\"\t724\r\n",
      "\"derogation\"\t724\r\n",
      "\"constituents\"\t724\r\n",
      "\"atmospheric\"\t724\r\n",
      "\"choir\"\t724\r\n",
      "\"interrupt\"\t723\r\n",
      "\"hawaiian\"\t723\r\n",
      "\"irregularity\"\t723\r\n",
      "\"fitness\"\t723\r\n",
      "\"costa\"\t723\r\n",
      "\"opponent\"\t723\r\n",
      "\"purest\"\t723\r\n",
      "\"needful\"\t723\r\n",
      "\"thrive\"\t722\r\n",
      "\"precipitation\"\t722\r\n",
      "\"shawl\"\t722\r\n",
      "\"calvin\"\t722\r\n",
      "\"beats\"\t722\r\n",
      "\"ac\"\t722\r\n",
      "\"conditioned\"\t722\r\n",
      "\"dispassionate\"\t722\r\n",
      "\"heroine\"\t722\r\n",
      "\"allude\"\t721\r\n",
      "\"articulate\"\t721\r\n",
      "\"variability\"\t721\r\n",
      "\"switzerland\"\t721\r\n",
      "\"temperance\"\t721\r\n",
      "\"shedding\"\t721\r\n",
      "\"tomography\"\t721\r\n",
      "\"lithium\"\t720\r\n",
      "\"suburb\"\t720\r\n",
      "\"sofa\"\t720\r\n",
      "\"framing\"\t720\r\n",
      "\"debilitating\"\t720\r\n",
      "\"anchored\"\t719\r\n",
      "\"chromosomes\"\t719\r\n",
      "\"neuropathy\"\t719\r\n",
      "\"wrists\"\t719\r\n",
      "\"thunder\"\t719\r\n",
      "\"lighting\"\t719\r\n",
      "\"politician\"\t718\r\n",
      "\"lap\"\t718\r\n",
      "\"roger\"\t718\r\n",
      "\"ischemia\"\t718\r\n",
      "\"butler\"\t717\r\n",
      "\"innovative\"\t717\r\n",
      "\"discriminated\"\t717\r\n",
      "\"manly\"\t717\r\n",
      "\"vigor\"\t717\r\n",
      "\"tranquillity\"\t717\r\n",
      "\"nuclei\"\t716\r\n",
      "\"washings\"\t716\r\n",
      "\"plague\"\t716\r\n",
      "\"wits\"\t716\r\n",
      "\"shouted\"\t716\r\n",
      "\"franklin\"\t716\r\n",
      "\"enumerated\"\t716\r\n",
      "\"controversies\"\t715\r\n",
      "\"bloom\"\t715\r\n",
      "\"heroism\"\t715\r\n",
      "\"enlightenment\"\t715\r\n",
      "\"gloomy\"\t715\r\n",
      "\"dentist\"\t715\r\n",
      "\"harmless\"\t715\r\n",
      "\"sanguine\"\t715\r\n",
      "\"kentucky\"\t715\r\n",
      "\"triangular\"\t715\r\n",
      "\"malawi\"\t714\r\n",
      "\"solemnly\"\t714\r\n",
      "\"heel\"\t714\r\n",
      "\"forgiveness\"\t714\r\n",
      "\"fixation\"\t714\r\n",
      "\"fist\"\t713\r\n",
      "\"episcopal\"\t713\r\n",
      "\"caudal\"\t713\r\n",
      "\"anarchy\"\t713\r\n",
      "\"appropriated\"\t713\r\n",
      "\"meridian\"\t713\r\n",
      "\"regulatory\"\t713\r\n",
      "\"ovum\"\t713\r\n",
      "\"prized\"\t713\r\n",
      "\"ontario\"\t712\r\n",
      "\"productivity\"\t712\r\n",
      "\"pushing\"\t712\r\n",
      "\"pamphlets\"\t712\r\n",
      "\"spy\"\t712\r\n",
      "\"html\"\t712\r\n",
      "\"inquired\"\t712\r\n",
      "\"cytoplasm\"\t712\r\n",
      "\"herbs\"\t712\r\n",
      "\"cancel\"\t712\r\n",
      "\"habitually\"\t711\r\n",
      "\"drafted\"\t711\r\n",
      "\"ethic\"\t711\r\n",
      "\"mock\"\t711\r\n",
      "\"supplier\"\t710\r\n",
      "\"tartar\"\t710\r\n",
      "\"indicted\"\t710\r\n",
      "\"fairer\"\t710\r\n",
      "\"hamilton\"\t710\r\n",
      "\"deducted\"\t709\r\n",
      "\"exporting\"\t709\r\n",
      "\"interpolation\"\t709\r\n",
      "\"hungary\"\t709\r\n",
      "\"crawl\"\t709\r\n",
      "\"robes\"\t709\r\n",
      "\"researches\"\t709\r\n",
      "\"processor\"\t709\r\n",
      "\"prerogative\"\t709\r\n",
      "\"streak\"\t709\r\n",
      "\"tyrant\"\t708\r\n",
      "\"probabilities\"\t708\r\n",
      "\"secessionist\"\t708\r\n",
      "\"historiography\"\t708\r\n",
      "\"gower\"\t708\r\n",
      "\"exalted\"\t708\r\n",
      "\"fills\"\t708\r\n",
      "\"emancipation\"\t708\r\n"
     ]
    }
   ],
   "source": [
    "!cat frequencies5.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stripes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stripes5_5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stripes5_5.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import re\n",
    "import mrjob\n",
    "import json\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import itertools\n",
    "import collections\n",
    "import time\n",
    "import logging\n",
    "\n",
    "class stripes(MRJob):\n",
    "  \n",
    "    #START SUDENT CODE531_STRIPES\n",
    "  \n",
    "    MRJob.SORT_VALUES = True\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        self.valid_words = set()\n",
    "        super(stripes, self).__init__(args)\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        # Load the left table in the init so all mappers get this info\n",
    "        with open(\"frequencies5.5\", 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                x = line.strip().split(\"\\t\")\n",
    "                self.valid_words.add(x[0].strip(\"\\\"\"))\n",
    "            \n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        splits = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "\n",
    "        words = splits[0].lower().split()\n",
    "        count = splits[1]\n",
    "\n",
    "        H = {}\n",
    "        for subset in itertools.combinations(sorted(set(words)), 2):\n",
    "            \n",
    "            if subset[0] in self.valid_words and subset[1] in self.valid_words:\n",
    "\n",
    "                # Process combinations in sorted order, i.e. \"hello\",\"tomorrow\"\n",
    "                if subset[0] not in H.keys():\n",
    "                    H[subset[0]] = {}\n",
    "                    H[subset[0]][subset[1]] = count \n",
    "                elif subset[1] not in H[subset[0]]:\n",
    "                    H[subset[0]][subset[1]] = count\n",
    "                else:\n",
    "                    H[subset[0]][subset[1]] += count\n",
    "\n",
    "                # Obtain combinations in reverse order, to consider them both ways\n",
    "                # TODO: Should refactor this and the block above, shameless copy-paste\n",
    "                if subset[1] not in H.keys():\n",
    "                    H[subset[1]] = {}\n",
    "                    H[subset[1]][subset[0]] = count \n",
    "                elif subset[0] not in H[subset[1]]:\n",
    "                    H[subset[1]][subset[0]] = count\n",
    "                else:\n",
    "                    H[subset[1]][subset[0]] += count\n",
    "        for key in H.keys():\n",
    "            #print \"%s\\t%s\" % (key, json.dumps(H[key]))\n",
    "            yield key, H[key]\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        \n",
    "        counter = {}\n",
    "\n",
    "        for value in values:\n",
    "            \n",
    "            for k, v in value.iteritems():\n",
    "                if k in counter:\n",
    "                    counter[k] += int(v)\n",
    "                else:\n",
    "                    counter[k] = int(v)\n",
    "        \n",
    "        yield key, counter\n",
    "\n",
    "  #END SUDENT CODE531_STRIPES\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    stripes.run()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `stripes5.5': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/stripes5_5.root.20170616.223242.125605\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/stripes5_5.root.20170616.223242.125605/files/...\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob5118814237556475014.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497651454196_0006\n",
      "  Submitted application application_1497651454196_0006\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1497651454196_0006/\n",
      "  Running job: job_1497651454196_0006\n",
      "  Job job_1497651454196_0006 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497651454196_0006 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/stripes5_5.root.20170616.223242.125605/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=11448710\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3741\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4617\n",
      "\t\tFILE: Number of bytes written=365729\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=11449108\n",
      "\t\tHDFS: Number of bytes written=3741\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8843264\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2507776\n",
      "\t\tTotal time spent by all map tasks (ms)=8636\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8636\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2449\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2449\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8636\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2449\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1610\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=91\n",
      "\t\tInput split bytes=398\n",
      "\t\tMap input records=311614\n",
      "\t\tMap output bytes=4341\n",
      "\t\tMap output materialized bytes=4623\n",
      "\t\tMap output records=135\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=804491264\n",
      "\t\tReduce input groups=135\n",
      "\t\tReduce input records=135\n",
      "\t\tReduce output records=123\n",
      "\t\tReduce shuffle bytes=4623\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=270\n",
      "\t\tTotal committed heap usage (bytes)=907542528\n",
      "\t\tVirtual memory (bytes) snapshot=4107857920\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/stripes5_5.root.20170616.223242.125605/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/stripes5_5.root.20170616.223242.125605...\n",
      "Removing temp directory /tmp/stripes5_5.root.20170616.223242.125605...\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r stripes5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python stripes5_5.py --file=frequencies5.5 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > stripes5.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"accepts\"\t{\"embody\": 382}\r\n",
      "\"adjustments\"\t{\"placement\": 46}\r\n",
      "\"albert\"\t{\"lee\": 57}\r\n",
      "\"anastomosis\"\t{\"donor\": 52}\r\n",
      "\"anciennes\"\t{\"recueil\": 794, \"lois\": 794}\r\n",
      "\"anti\"\t{\"goat\": 72}\r\n",
      "\"arrives\"\t{\"pilgrim\": 321}\r\n",
      "\"atmospheric\"\t{\"fixation\": 313}\r\n",
      "\"au\"\t{\"vie\": 272}\r\n",
      "\"aunt\"\t{\"maria\": 43}\r\n",
      "\"automatic\"\t{\"ventilation\": 411}\r\n",
      "\"balak\"\t{\"zippor\": 758}\r\n",
      "\"brook\"\t{\"rugged\": 95}\r\n",
      "\"brother's\"\t{\"vacant\": 46}\r\n",
      "\"calculus\"\t{\"probabilities\": 115, \"infinitesimal\": 412}\r\n",
      "\"calvin\"\t{\"hoover\": 680, \"coolidge\": 680}\r\n",
      "\"cancel\"\t{\"ok\": 589}\r\n",
      "\"cannon\"\t{\"thunder\": 80}\r\n",
      "\"cardinals\"\t{\"seizing\": 43}\r\n",
      "\"caudal\"\t{\"fin\": 60}\r\n",
      "\"chromosome\"\t{\"chromosomes\": 44}\r\n",
      "\"chromosomes\"\t{\"chromosome\": 44}\r\n",
      "\"clusters\"\t{\"creativity\": 63}\r\n",
      "\"compliment\"\t{\"graceful\": 44, \"relish\": 44}\r\n",
      "\"conditioned\"\t{\"inhibition\": 70}\r\n",
      "\"constrained\"\t{\"gentry\": 40}\r\n",
      "\"coolidge\"\t{\"hoover\": 680, \"calvin\": 680}\r\n",
      "\"crane\"\t{\"necks\": 67}\r\n",
      "\"crawl\"\t{\"strove\": 56}\r\n",
      "\"creativity\"\t{\"clusters\": 63, \"ingenuity\": 229}\r\n",
      "\"cyrus\"\t{\"edict\": 67}\r\n",
      "\"desolate\"\t{\"wandering\": 73}\r\n",
      "\"distilled\"\t{\"sterile\": 170}\r\n",
      "\"donor\"\t{\"anastomosis\": 52}\r\n",
      "\"edict\"\t{\"preamble\": 53, \"cyrus\": 67}\r\n",
      "\"embody\"\t{\"accepts\": 382}\r\n",
      "\"englishmen\"\t{\"subjection\": 52}\r\n",
      "\"epsilon\"\t{\"sigma\": 801}\r\n",
      "\"erie\"\t{\"huron\": 93}\r\n",
      "\"excel\"\t{\"virtuous\": 225}\r\n",
      "\"falsehood\"\t{\"needful\": 74, \"vanished\": 127, \"mock\": 248}\r\n",
      "\"fin\"\t{\"caudal\": 60}\r\n",
      "\"fixation\"\t{\"atmospheric\": 313}\r\n",
      "\"gentry\"\t{\"constrained\": 40}\r\n",
      "\"goat\"\t{\"anti\": 72}\r\n",
      "\"graceful\"\t{\"compliment\": 44}\r\n",
      "\"grotesque\"\t{\"visions\": 149}\r\n",
      "\"heavier\"\t{\"rails\": 67}\r\n",
      "\"hepatic\"\t{\"inducing\": 115}\r\n",
      "\"hoover\"\t{\"coolidge\": 680, \"calvin\": 680}\r\n",
      "\"huron\"\t{\"erie\": 93}\r\n",
      "\"incentives\"\t{\"manipulation\": 49}\r\n",
      "\"inducing\"\t{\"hepatic\": 115}\r\n",
      "\"infinitesimal\"\t{\"calculus\": 412}\r\n",
      "\"ingenuity\"\t{\"creativity\": 229}\r\n",
      "\"inhibition\"\t{\"serotonin\": 53, \"conditioned\": 70}\r\n",
      "\"inhibitors\"\t{\"serotonin\": 580}\r\n",
      "\"inspector\"\t{\"sanitary\": 46}\r\n",
      "\"insulted\"\t{\"mock\": 60}\r\n",
      "\"intolerable\"\t{\"pangs\": 44}\r\n",
      "\"invaders\"\t{\"vigour\": 53}\r\n",
      "\"judicium\"\t{\"legale\": 741, \"parium\": 741, \"suorum\": 741}\r\n",
      "\"korean\"\t{\"unification\": 74}\r\n",
      "\"lee\"\t{\"albert\": 57}\r\n",
      "\"legale\"\t{\"parium\": 741, \"judicium\": 741, \"suorum\": 741}\r\n",
      "\"lois\"\t{\"recueil\": 794, \"anciennes\": 794}\r\n",
      "\"mahathir\"\t{\"mohamad\": 739, \"malaysian\": 739}\r\n",
      "\"malaysian\"\t{\"mahathir\": 739, \"mohamad\": 739}\r\n",
      "\"manipulation\"\t{\"incentives\": 49}\r\n",
      "\"maria\"\t{\"aunt\": 43}\r\n",
      "\"mock\"\t{\"falsehood\": 248, \"insulted\": 60}\r\n",
      "\"mohamad\"\t{\"mahathir\": 739, \"malaysian\": 739}\r\n",
      "\"monastic\"\t{\"vestiges\": 76}\r\n",
      "\"necks\"\t{\"crane\": 67}\r\n",
      "\"needful\"\t{\"falsehood\": 74}\r\n",
      "\"norfolk\"\t{\"presiding\": 57}\r\n",
      "\"ok\"\t{\"cancel\": 589}\r\n",
      "\"pangs\"\t{\"intolerable\": 44, \"parting\": 111}\r\n",
      "\"parium\"\t{\"legale\": 741, \"judicium\": 741, \"suorum\": 741}\r\n",
      "\"parting\"\t{\"pangs\": 111}\r\n",
      "\"pearl\"\t{\"sells\": 42}\r\n",
      "\"pernicious\"\t{\"tyrant\": 81}\r\n",
      "\"pilgrim\"\t{\"arrives\": 321}\r\n",
      "\"placement\"\t{\"adjustments\": 46}\r\n",
      "\"preamble\"\t{\"edict\": 53}\r\n",
      "\"presiding\"\t{\"norfolk\": 57}\r\n",
      "\"probabilities\"\t{\"calculus\": 115}\r\n",
      "\"prospective\"\t{\"seizure\": 66}\r\n",
      "\"que\"\t{\"se\": 56}\r\n",
      "\"rails\"\t{\"heavier\": 67}\r\n",
      "\"recueil\"\t{\"lois\": 794, \"anciennes\": 794}\r\n",
      "\"reinforcement\"\t{\"speedily\": 62}\r\n",
      "\"relish\"\t{\"compliment\": 44}\r\n",
      "\"rugged\"\t{\"brook\": 95}\r\n",
      "\"sanitary\"\t{\"inspector\": 46}\r\n",
      "\"scared\"\t{\"who's\": 60}\r\n",
      "\"se\"\t{\"que\": 56}\r\n",
      "\"seizing\"\t{\"cardinals\": 43}\r\n",
      "\"seizure\"\t{\"prospective\": 66}\r\n",
      "\"sells\"\t{\"pearl\": 42}\r\n",
      "\"serotonin\"\t{\"inhibition\": 53, \"inhibitors\": 580}\r\n",
      "\"sigma\"\t{\"epsilon\": 801}\r\n",
      "\"speedily\"\t{\"reinforcement\": 62}\r\n",
      "\"sterile\"\t{\"distilled\": 170}\r\n",
      "\"strand\"\t{\"triple\": 117}\r\n",
      "\"strove\"\t{\"crawl\": 56}\r\n",
      "\"subjection\"\t{\"englishmen\": 52}\r\n",
      "\"suorum\"\t{\"legale\": 741, \"judicium\": 741, \"parium\": 741}\r\n",
      "\"thunder\"\t{\"cannon\": 80}\r\n",
      "\"triple\"\t{\"strand\": 117}\r\n",
      "\"tyrant\"\t{\"pernicious\": 81}\r\n",
      "\"unification\"\t{\"korean\": 74}\r\n",
      "\"vacant\"\t{\"brother's\": 46}\r\n",
      "\"vanished\"\t{\"falsehood\": 127}\r\n",
      "\"ventilation\"\t{\"automatic\": 411}\r\n",
      "\"vestiges\"\t{\"monastic\": 76}\r\n",
      "\"vie\"\t{\"au\": 272}\r\n",
      "\"vigour\"\t{\"invaders\": 53}\r\n",
      "\"virtuous\"\t{\"excel\": 225}\r\n",
      "\"visions\"\t{\"grotesque\": 149}\r\n",
      "\"wandering\"\t{\"desolate\": 73}\r\n",
      "\"who's\"\t{\"scared\": 60}\r\n",
      "\"zippor\"\t{\"balak\": 758}\r\n"
     ]
    }
   ],
   "source": [
    "!cat stripes5.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting index5_5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile index5_5.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "from __future__ import division\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "\n",
    "class index(MRJob):\n",
    "    \n",
    "    #START SUDENT CODE531_INV_INDEX\n",
    "  \n",
    "    def mapper(self, _, line):\n",
    "        key, stripeJson = line.strip().split('\\t')\n",
    "        key = key.strip(\"\\\"\")\n",
    "        stripe = json.loads(stripeJson)\n",
    "        \n",
    "        for k, v in stripe.iteritems():\n",
    "            yield k, [key, len(stripe)]\n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "\n",
    "        table = {}\n",
    "        for value in values:\n",
    "            table[value[0]] = value[1]\n",
    "            \n",
    "        yield key, table\n",
    "        \n",
    "    #END SUDENT CODE531_INV_INDEX\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    index.run() \n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `index5.5': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/index5_5.root.20170616.223655.035482\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/index5_5.root.20170616.223655.035482/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob2385366140107570970.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497651454196_0008\n",
      "  Submitted application application_1497651454196_0008\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1497651454196_0008/\n",
      "  Running job: job_1497651454196_0008\n",
      "  Job job_1497651454196_0008 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497651454196_0008 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/index5_5.root.20170616.223655.035482/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5612\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3521\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4130\n",
      "\t\tFILE: Number of bytes written=361785\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=5926\n",
      "\t\tHDFS: Number of bytes written=3521\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6293504\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2485248\n",
      "\t\tTotal time spent by all map tasks (ms)=6146\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6146\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2427\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2427\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6146\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2427\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1140\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=56\n",
      "\t\tInput split bytes=314\n",
      "\t\tMap input records=123\n",
      "\t\tMap output bytes=3824\n",
      "\t\tMap output materialized bytes=4136\n",
      "\t\tMap output records=150\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=735563776\n",
      "\t\tReduce input groups=123\n",
      "\t\tReduce input records=150\n",
      "\t\tReduce output records=123\n",
      "\t\tReduce shuffle bytes=4136\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=300\n",
      "\t\tTotal committed heap usage (bytes)=750780416\n",
      "\t\tVirtual memory (bytes) snapshot=4077588480\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/index5_5.root.20170616.223655.035482/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/index5_5.root.20170616.223655.035482...\n",
      "Removing temp directory /tmp/index5_5.root.20170616.223655.035482...\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r index5.5\n",
    "!python index5_5.py -r hadoop stripes5.5 > index5.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"accepts\"\t{\"embody\": 1}\r\n",
      "\"adjustments\"\t{\"placement\": 1}\r\n",
      "\"albert\"\t{\"lee\": 1}\r\n",
      "\"anastomosis\"\t{\"donor\": 1}\r\n",
      "\"anciennes\"\t{\"recueil\": 2, \"lois\": 2}\r\n",
      "\"anti\"\t{\"goat\": 1}\r\n",
      "\"arrives\"\t{\"pilgrim\": 1}\r\n",
      "\"atmospheric\"\t{\"fixation\": 1}\r\n",
      "\"au\"\t{\"vie\": 1}\r\n",
      "\"aunt\"\t{\"maria\": 1}\r\n",
      "\"automatic\"\t{\"ventilation\": 1}\r\n",
      "\"balak\"\t{\"zippor\": 1}\r\n",
      "\"brook\"\t{\"rugged\": 1}\r\n",
      "\"brother's\"\t{\"vacant\": 1}\r\n",
      "\"calculus\"\t{\"probabilities\": 1, \"infinitesimal\": 1}\r\n",
      "\"calvin\"\t{\"hoover\": 2, \"coolidge\": 2}\r\n",
      "\"cancel\"\t{\"ok\": 1}\r\n",
      "\"cannon\"\t{\"thunder\": 1}\r\n",
      "\"cardinals\"\t{\"seizing\": 1}\r\n",
      "\"caudal\"\t{\"fin\": 1}\r\n",
      "\"chromosome\"\t{\"chromosomes\": 1}\r\n",
      "\"chromosomes\"\t{\"chromosome\": 1}\r\n",
      "\"clusters\"\t{\"creativity\": 2}\r\n",
      "\"compliment\"\t{\"graceful\": 1, \"relish\": 1}\r\n",
      "\"conditioned\"\t{\"inhibition\": 2}\r\n",
      "\"constrained\"\t{\"gentry\": 1}\r\n",
      "\"coolidge\"\t{\"hoover\": 2, \"calvin\": 2}\r\n",
      "\"crane\"\t{\"necks\": 1}\r\n",
      "\"crawl\"\t{\"strove\": 1}\r\n",
      "\"creativity\"\t{\"clusters\": 1, \"ingenuity\": 1}\r\n",
      "\"cyrus\"\t{\"edict\": 2}\r\n",
      "\"desolate\"\t{\"wandering\": 1}\r\n",
      "\"distilled\"\t{\"sterile\": 1}\r\n",
      "\"donor\"\t{\"anastomosis\": 1}\r\n",
      "\"edict\"\t{\"preamble\": 1, \"cyrus\": 1}\r\n",
      "\"embody\"\t{\"accepts\": 1}\r\n",
      "\"englishmen\"\t{\"subjection\": 1}\r\n",
      "\"epsilon\"\t{\"sigma\": 1}\r\n",
      "\"erie\"\t{\"huron\": 1}\r\n",
      "\"excel\"\t{\"virtuous\": 1}\r\n",
      "\"falsehood\"\t{\"vanished\": 1, \"needful\": 1, \"mock\": 2}\r\n",
      "\"fin\"\t{\"caudal\": 1}\r\n",
      "\"fixation\"\t{\"atmospheric\": 1}\r\n",
      "\"gentry\"\t{\"constrained\": 1}\r\n",
      "\"goat\"\t{\"anti\": 1}\r\n",
      "\"graceful\"\t{\"compliment\": 2}\r\n",
      "\"grotesque\"\t{\"visions\": 1}\r\n",
      "\"heavier\"\t{\"rails\": 1}\r\n",
      "\"hepatic\"\t{\"inducing\": 1}\r\n",
      "\"hoover\"\t{\"coolidge\": 2, \"calvin\": 2}\r\n",
      "\"huron\"\t{\"erie\": 1}\r\n",
      "\"incentives\"\t{\"manipulation\": 1}\r\n",
      "\"inducing\"\t{\"hepatic\": 1}\r\n",
      "\"infinitesimal\"\t{\"calculus\": 2}\r\n",
      "\"ingenuity\"\t{\"creativity\": 2}\r\n",
      "\"inhibition\"\t{\"serotonin\": 2, \"conditioned\": 1}\r\n",
      "\"inhibitors\"\t{\"serotonin\": 2}\r\n",
      "\"inspector\"\t{\"sanitary\": 1}\r\n",
      "\"insulted\"\t{\"mock\": 2}\r\n",
      "\"intolerable\"\t{\"pangs\": 2}\r\n",
      "\"invaders\"\t{\"vigour\": 1}\r\n",
      "\"judicium\"\t{\"legale\": 3, \"parium\": 3, \"suorum\": 3}\r\n",
      "\"korean\"\t{\"unification\": 1}\r\n",
      "\"lee\"\t{\"albert\": 1}\r\n",
      "\"legale\"\t{\"parium\": 3, \"judicium\": 3, \"suorum\": 3}\r\n",
      "\"lois\"\t{\"recueil\": 2, \"anciennes\": 2}\r\n",
      "\"mahathir\"\t{\"mohamad\": 2, \"malaysian\": 2}\r\n",
      "\"malaysian\"\t{\"mahathir\": 2, \"mohamad\": 2}\r\n",
      "\"manipulation\"\t{\"incentives\": 1}\r\n",
      "\"maria\"\t{\"aunt\": 1}\r\n",
      "\"mock\"\t{\"falsehood\": 3, \"insulted\": 1}\r\n",
      "\"mohamad\"\t{\"mahathir\": 2, \"malaysian\": 2}\r\n",
      "\"monastic\"\t{\"vestiges\": 1}\r\n",
      "\"necks\"\t{\"crane\": 1}\r\n",
      "\"needful\"\t{\"falsehood\": 3}\r\n",
      "\"norfolk\"\t{\"presiding\": 1}\r\n",
      "\"ok\"\t{\"cancel\": 1}\r\n",
      "\"pangs\"\t{\"intolerable\": 1, \"parting\": 1}\r\n",
      "\"parium\"\t{\"legale\": 3, \"judicium\": 3, \"suorum\": 3}\r\n",
      "\"parting\"\t{\"pangs\": 2}\r\n",
      "\"pearl\"\t{\"sells\": 1}\r\n",
      "\"pernicious\"\t{\"tyrant\": 1}\r\n",
      "\"pilgrim\"\t{\"arrives\": 1}\r\n",
      "\"placement\"\t{\"adjustments\": 1}\r\n",
      "\"preamble\"\t{\"edict\": 2}\r\n",
      "\"presiding\"\t{\"norfolk\": 1}\r\n",
      "\"probabilities\"\t{\"calculus\": 2}\r\n",
      "\"prospective\"\t{\"seizure\": 1}\r\n",
      "\"que\"\t{\"se\": 1}\r\n",
      "\"rails\"\t{\"heavier\": 1}\r\n",
      "\"recueil\"\t{\"lois\": 2, \"anciennes\": 2}\r\n",
      "\"reinforcement\"\t{\"speedily\": 1}\r\n",
      "\"relish\"\t{\"compliment\": 2}\r\n",
      "\"rugged\"\t{\"brook\": 1}\r\n",
      "\"sanitary\"\t{\"inspector\": 1}\r\n",
      "\"scared\"\t{\"who's\": 1}\r\n",
      "\"se\"\t{\"que\": 1}\r\n",
      "\"seizing\"\t{\"cardinals\": 1}\r\n",
      "\"seizure\"\t{\"prospective\": 1}\r\n",
      "\"sells\"\t{\"pearl\": 1}\r\n",
      "\"serotonin\"\t{\"inhibition\": 2, \"inhibitors\": 1}\r\n",
      "\"sigma\"\t{\"epsilon\": 1}\r\n",
      "\"speedily\"\t{\"reinforcement\": 1}\r\n",
      "\"sterile\"\t{\"distilled\": 1}\r\n",
      "\"strand\"\t{\"triple\": 1}\r\n",
      "\"strove\"\t{\"crawl\": 1}\r\n",
      "\"subjection\"\t{\"englishmen\": 1}\r\n",
      "\"suorum\"\t{\"parium\": 3, \"judicium\": 3, \"legale\": 3}\r\n",
      "\"thunder\"\t{\"cannon\": 1}\r\n",
      "\"triple\"\t{\"strand\": 1}\r\n",
      "\"tyrant\"\t{\"pernicious\": 1}\r\n",
      "\"unification\"\t{\"korean\": 1}\r\n",
      "\"vacant\"\t{\"brother's\": 1}\r\n",
      "\"vanished\"\t{\"falsehood\": 3}\r\n",
      "\"ventilation\"\t{\"automatic\": 1}\r\n",
      "\"vestiges\"\t{\"monastic\": 1}\r\n",
      "\"vie\"\t{\"au\": 1}\r\n",
      "\"vigour\"\t{\"invaders\": 1}\r\n",
      "\"virtuous\"\t{\"excel\": 1}\r\n",
      "\"visions\"\t{\"grotesque\": 1}\r\n",
      "\"wandering\"\t{\"desolate\": 1}\r\n",
      "\"who's\"\t{\"scared\": 1}\r\n",
      "\"zippor\"\t{\"balak\": 1}\r\n"
     ]
    }
   ],
   "source": [
    "!cat index5.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting similarity5_5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile similarity5_5.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "#import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import time\n",
    "import logging\n",
    "\n",
    "class similarity(MRJob):\n",
    "  \n",
    "    #START SUDENT CODE531_SIMILARITY\n",
    "\n",
    "    MRJob.SORT_VALUES = True\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        key, valuesJson = line.strip().split('\\t')\n",
    "        key = key.strip(\"\\\"\")\n",
    "        values = json.loads(valuesJson)\n",
    "\n",
    "        for pair in itertools.combinations(sorted(set(values)), 2):\n",
    "            yield pair, [values[pair[0]], values[pair[1]]]\n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "        intersection = 0\n",
    "        count1 = None\n",
    "        count2 = None\n",
    "        \n",
    "        cosine = 0.0\n",
    "        \n",
    "        # Iterate through the values\n",
    "        for value in values:\n",
    "            # Jaccard, get counts for the intersection, and for each set\n",
    "            intersection += 1\n",
    "            if count1 == None:\n",
    "                count1 = value[0]\n",
    "                count2 = value[1]\n",
    "        \n",
    "            # Cosine\n",
    "            a = 1 / math.sqrt(value[0])\n",
    "            b = 1 / math.sqrt(value[1])\n",
    "            cosine += a * b\n",
    "            \n",
    "        jaccard = float(intersection) / float(count1 + count2 - intersection)\n",
    "        \n",
    "        overlap_coefficient = float(intersection) / min(count1, count2)\n",
    "        \n",
    "        dice_coefficient = float(2 * intersection) / (count1 + count2)\n",
    "        \n",
    "        average = (cosine + jaccard + overlap_coefficient + dice_coefficient) / 4.0\n",
    "        \n",
    "        yield average, [key[0] + ' - ' + key[1], cosine, jaccard, overlap_coefficient, dice_coefficient, average]\n",
    "    \n",
    "    \n",
    "    def max_reducer(self, average, records):\n",
    "        for record in records:\n",
    "            yield average, record\n",
    "\n",
    "    def steps(self):\n",
    "        \n",
    "        custom_jobconf = {\n",
    "            'stream.num.map.output.key.fields':'2',\n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k1,1nr',\n",
    "            'mapred.reduce.tasks': '1'\n",
    "        }\n",
    "\n",
    "        return [\n",
    "                MRStep(mapper=self.mapper,\n",
    "                    reducer=self.reducer),\n",
    "                MRStep(jobconf=custom_jobconf,\n",
    "                       reducer=self.max_reducer)\n",
    "                 ]\n",
    "    \n",
    "    #END SUDENT CODE531_SIMILARITY\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    similarity.run()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    mins = elapsed_time/float(60)\n",
    "    a = \"\"\"Elapsed time: %s seconds\n",
    "    In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `similarity5.5': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/similarity5_5.root.20170616.225146.757743\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/similarity5_5.root.20170616.225146.757743/files/...\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 2...\n",
      "  mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob4594933984191735185.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497651454196_0015\n",
      "  Submitted application application_1497651454196_0015\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1497651454196_0015/\n",
      "  Running job: job_1497651454196_0015\n",
      "  Job job_1497651454196_0015 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497651454196_0015 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/similarity5_5.root.20170616.225146.757743/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5282\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2549\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1081\n",
      "\t\tFILE: Number of bytes written=357388\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=5602\n",
      "\t\tHDFS: Number of bytes written=2549\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6223872\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2456576\n",
      "\t\tTotal time spent by all map tasks (ms)=6078\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6078\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2399\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2399\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6078\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2399\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1150\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=69\n",
      "\t\tInput split bytes=320\n",
      "\t\tMap input records=123\n",
      "\t\tMap output bytes=1011\n",
      "\t\tMap output materialized bytes=1087\n",
      "\t\tMap output records=32\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=732094464\n",
      "\t\tReduce input groups=26\n",
      "\t\tReduce input records=32\n",
      "\t\tReduce output records=26\n",
      "\t\tReduce shuffle bytes=1087\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=64\n",
      "\t\tTotal committed heap usage (bytes)=794296320\n",
      "\t\tVirtual memory (bytes) snapshot=4099485696\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 2 of 2...\n",
      "  mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "  mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "  mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "  mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob6104676176069256590.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497651454196_0016\n",
      "  Submitted application application_1497651454196_0016\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1497651454196_0016/\n",
      "  Running job: job_1497651454196_0016\n",
      "  Job job_1497651454196_0016 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497651454196_0016 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/similarity5_5.root.20170616.225146.757743/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3824\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2549\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2633\n",
      "\t\tFILE: Number of bytes written=361332\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4170\n",
      "\t\tHDFS: Number of bytes written=2549\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6389760\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2485248\n",
      "\t\tTotal time spent by all map tasks (ms)=6240\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6240\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2427\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2427\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6240\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2427\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1160\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=44\n",
      "\t\tInput split bytes=346\n",
      "\t\tMap input records=26\n",
      "\t\tMap output bytes=2575\n",
      "\t\tMap output materialized bytes=2639\n",
      "\t\tMap output records=26\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=785276928\n",
      "\t\tReduce input groups=26\n",
      "\t\tReduce input records=26\n",
      "\t\tReduce output records=26\n",
      "\t\tReduce shuffle bytes=2639\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=52\n",
      "\t\tTotal committed heap usage (bytes)=756547584\n",
      "\t\tVirtual memory (bytes) snapshot=4079706112\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/similarity5_5.root.20170616.225146.757743/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/similarity5_5.root.20170616.225146.757743...\n",
      "Removing temp directory /tmp/similarity5_5.root.20170616.225146.757743...\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r similarity5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python similarity5_5.py -r hadoop index5.5 > similarity5.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\t[\"needful - vanished\", 1.0, 1.0, 1.0, 1.0, 1.0]\r\n",
      "1.0\t[\"clusters - ingenuity\", 1.0, 1.0, 1.0, 1.0, 1.0]\r\n",
      "1.0\t[\"intolerable - parting\", 1.0, 1.0, 1.0, 1.0, 1.0]\r\n",
      "1.0\t[\"infinitesimal - probabilities\", 1.0, 1.0, 1.0, 1.0, 1.0]\r\n",
      "1.0\t[\"graceful - relish\", 1.0, 1.0, 1.0, 1.0, 1.0]\r\n",
      "1.0\t[\"cyrus - preamble\", 1.0, 1.0, 1.0, 1.0, 1.0]\r\n",
      "0.7184433619633035\t[\"inhibition - inhibitors\", 0.70710678118654746, 0.5, 1.0, 0.66666666666666663, 0.7184433619633035]\r\n",
      "0.7184433619633035\t[\"conditioned - serotonin\", 0.70710678118654746, 0.5, 1.0, 0.66666666666666663, 0.7184433619633035]\r\n",
      "0.7184433619633035\t[\"mock - vanished\", 0.70710678118654746, 0.5, 1.0, 0.66666666666666663, 0.7184433619633035]\r\n",
      "0.7184433619633035\t[\"mock - needful\", 0.70710678118654746, 0.5, 1.0, 0.66666666666666663, 0.7184433619633035]\r\n",
      "0.625\t[\"parium - suorum\", 0.66666666666666685, 0.5, 0.66666666666666663, 0.66666666666666663, 0.625]\r\n",
      "0.625\t[\"legale - suorum\", 0.66666666666666685, 0.5, 0.66666666666666663, 0.66666666666666663, 0.625]\r\n",
      "0.625\t[\"legale - parium\", 0.66666666666666685, 0.5, 0.66666666666666663, 0.66666666666666663, 0.625]\r\n",
      "0.625\t[\"judicium - suorum\", 0.66666666666666685, 0.5, 0.66666666666666663, 0.66666666666666663, 0.625]\r\n",
      "0.625\t[\"judicium - parium\", 0.66666666666666685, 0.5, 0.66666666666666663, 0.66666666666666663, 0.625]\r\n",
      "0.625\t[\"judicium - legale\", 0.66666666666666685, 0.5, 0.66666666666666663, 0.66666666666666663, 0.625]\r\n",
      "0.6026709006307398\t[\"falsehood - insulted\", 0.57735026918962584, 0.33333333333333331, 1.0, 0.5, 0.6026709006307398]\r\n",
      "0.45833333333333331\t[\"coolidge - hoover\", 0.49999999999999989, 0.33333333333333331, 0.5, 0.5, 0.45833333333333331]\r\n",
      "0.45833333333333331\t[\"calvin - hoover\", 0.49999999999999989, 0.33333333333333331, 0.5, 0.5, 0.45833333333333331]\r\n",
      "0.45833333333333331\t[\"calvin - coolidge\", 0.49999999999999989, 0.33333333333333331, 0.5, 0.5, 0.45833333333333331]\r\n",
      "0.45833333333333331\t[\"anciennes - recueil\", 0.49999999999999989, 0.33333333333333331, 0.5, 0.5, 0.45833333333333331]\r\n",
      "0.45833333333333331\t[\"anciennes - lois\", 0.49999999999999989, 0.33333333333333331, 0.5, 0.5, 0.45833333333333331]\r\n",
      "0.45833333333333331\t[\"malaysian - mohamad\", 0.49999999999999989, 0.33333333333333331, 0.5, 0.5, 0.45833333333333331]\r\n",
      "0.45833333333333331\t[\"mahathir - mohamad\", 0.49999999999999989, 0.33333333333333331, 0.5, 0.5, 0.45833333333333331]\r\n",
      "0.45833333333333331\t[\"mahathir - malaysian\", 0.49999999999999989, 0.33333333333333331, 0.5, 0.5, 0.45833333333333331]\r\n",
      "0.45833333333333331\t[\"lois - recueil\", 0.49999999999999989, 0.33333333333333331, 0.5, 0.5, 0.45833333333333331]\r\n"
     ]
    }
   ],
   "source": [
    "!cat similarity5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sortedSims = []\n",
    "with open(\"similarity5.5\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        sortedSims.append(lisst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir sims2\n",
    "!head -1000 similarity5.5 > sims2/top1000sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> l\n",
      "\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "Hit Enter to continue: \n",
      "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
      "                           and Basque Subset)\n",
      "  [ ] crubadan............ Crubadan Corpus\n",
      "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
      "  [ ] dolch............... Dolch Word List\n",
      "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
      "                           Corpus\n",
      "  [ ] floresta............ Portuguese Treebank\n",
      "  [ ] framenet_v15........ FrameNet 1.5\n",
      "  [ ] framenet_v17........ FrameNet 1.7\n",
      "  [ ] gazetteers.......... Gazeteer Lists\n",
      "  [ ] genesis............. Genesis Corpus\n",
      "  [ ] gutenberg........... Project Gutenberg Selections\n",
      "  [ ] hmm_treebank_pos_tagger Treebank Part of Speech Tagger (HMM)\n",
      "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
      "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
      "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
      "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
      "                           ChaSen format)\n",
      "  [ ] kimmo............... PC-KIMMO Data Files\n",
      "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
      "Hit Enter to continue: \n",
      "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
      "                           for parser comparison\n",
      "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
      "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
      "                           part-of-speech tags\n",
      "  [ ] machado............. Machado de Assis -- Obra Completa\n",
      "  [ ] masc_tagged......... MASC Tagged Corpus\n",
      "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
      "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
      "  [ ] moses_sample........ Moses Sample Models\n",
      "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
      "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
      "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
      "                           2015) subset of the Paraphrase Database.\n",
      "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
      "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
      "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
      "  [ ] nps_chat............ NPS Chat\n",
      "  [ ] omw................. Open Multilingual Wordnet\n",
      "  [ ] opinion_lexicon..... Opinion Lexicon\n",
      "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
      "Hit Enter to continue: \n",
      "  [ ] paradigms........... Paradigm Corpus\n",
      "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
      "                           Evaluation Shared Task\n",
      "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
      "                           character properties in Perl\n",
      "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
      "  [ ] pl196x.............. Polish language of the XX century sixties\n",
      "  [ ] porter_test......... Porter Stemmer Test Files\n",
      "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
      "  [ ] problem_reports..... Problem Report Corpus\n",
      "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
      "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
      "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
      "  [ ] pros_cons........... Pros and Cons\n",
      "  [ ] ptb................. Penn Treebank\n",
      "  [ ] punkt............... Punkt Tokenizer Models\n",
      "  [ ] qc.................. Experimental Data for Question Classification\n",
      "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
      "                           version\n",
      "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
      "                           Portuguesa)\n",
      "Hit Enter to continue: \n",
      "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
      "  [ ] sample_grammars..... Sample Grammars\n",
      "  [ ] semcor.............. SemCor 3.0\n",
      "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
      "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
      "  [ ] sentiwordnet........ SentiWordNet\n",
      "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
      "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
      "  [ ] smultron............ SMULTRON Corpus Sample\n",
      "  [ ] snowball_data....... Snowball Data\n",
      "  [ ] spanish_grammars.... Grammars for Spanish\n",
      "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
      "  [ ] stopwords........... Stopwords Corpus\n",
      "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
      "  [ ] swadesh............. Swadesh Wordlists\n",
      "  [ ] switchboard......... Switchboard Corpus Sample\n",
      "  [ ] tagsets............. Help on Tagsets\n",
      "  [ ] timit............... TIMIT Corpus Sample\n",
      "  [ ] toolbox............. Toolbox Sample Files\n",
      "  [ ] treebank............ Penn Treebank Sample\n",
      "  [ ] twitter_samples..... Twitter Samples\n",
      "Hit Enter to continue: \n",
      "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
      "                           (Unicode Version)\n",
      "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
      "  [ ] unicode_samples..... Unicode Samples\n",
      "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
      "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
      "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
      "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
      "  [ ] webtext............. Web Text Corpus\n",
      "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
      "  [ ] word2vec_sample..... Word2Vec Sample\n",
      "  [ ] wordnet............. WordNet\n",
      "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
      "  [ ] words............... Word Lists\n",
      "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
      "                           English Prose\n",
      "\n",
      "Collections:\n",
      "  [ ] all-corpora......... All the corpora\n",
      "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
      "                           branch\n",
      "  [ ] all................. All packages\n",
      "Hit Enter to continue: \n",
      "  [ ] book................ Everything used in the NLTK Book\n",
      "  [ ] popular............. Popular packages\n",
      "  [ ] third-party......... Third-party data packages\n",
      "\n",
      "([*] marks installed packages)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> \n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> wordnet\n",
      "Command 'wordnet' unrecognized\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> download\n",
      "Command 'download' unrecognized\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> wordnet\n",
      "    Downloading package wordnet to /root/nltk_data...\n",
      "      Unzipping corpora/wordnet.zip.\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-fec50ed38a60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/lib/python2.7/site-packages/nltk/downloader.pyc\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error)\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;31m# function should make a new copy of self to use?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdownload_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interactive_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python2.7/site-packages/nltk/downloader.pyc\u001b[0m in \u001b[0;36m_interactive_download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    974\u001b[0m                 \u001b[0mDownloaderGUI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTclError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m                 \u001b[0mDownloaderShell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m             \u001b[0mDownloaderShell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python2.7/site-packages/nltk/downloader.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    996\u001b[0m             self._simple_interactive_menu(\n\u001b[1;32m    997\u001b[0m                 'd) Download', 'l) List', ' u) Update', 'c) Config', 'h) Help', 'q) Quit')\n\u001b[0;32m--> 998\u001b[0;31m             \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Downloader> '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m             \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.pyc\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.pyc\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    722\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# END STUDENT CODE 5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top/Bottom 20 results - Similarity measures - sorted by cosine\n",
      "(From the entire data set)\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "                          pair |         cosine |        jaccard |        overlap |           dice |        average\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "            needful - vanished |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "          clusters - ingenuity |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         intolerable - parting |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      " infinitesimal - probabilities |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "             graceful - relish |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "              cyrus - preamble |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "       inhibition - inhibitors |       0.707107 |       0.500000 |       1.000000 |       0.666667 |       0.718443\n",
      "       conditioned - serotonin |       0.707107 |       0.500000 |       1.000000 |       0.666667 |       0.718443\n",
      "               mock - vanished |       0.707107 |       0.500000 |       1.000000 |       0.666667 |       0.718443\n",
      "                mock - needful |       0.707107 |       0.500000 |       1.000000 |       0.666667 |       0.718443\n",
      "               parium - suorum |       0.666667 |       0.500000 |       0.666667 |       0.666667 |       0.625000\n",
      "               legale - suorum |       0.666667 |       0.500000 |       0.666667 |       0.666667 |       0.625000\n",
      "               legale - parium |       0.666667 |       0.500000 |       0.666667 |       0.666667 |       0.625000\n",
      "             judicium - suorum |       0.666667 |       0.500000 |       0.666667 |       0.666667 |       0.625000\n",
      "             judicium - parium |       0.666667 |       0.500000 |       0.666667 |       0.666667 |       0.625000\n",
      "             judicium - legale |       0.666667 |       0.500000 |       0.666667 |       0.666667 |       0.625000\n",
      "          falsehood - insulted |       0.577350 |       0.333333 |       1.000000 |       0.500000 |       0.602671\n",
      "             coolidge - hoover |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "               calvin - hoover |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "             calvin - coolidge |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "       inhibition - inhibitors |       0.707107 |       0.500000 |       1.000000 |       0.666667 |       0.718443\n",
      "       conditioned - serotonin |       0.707107 |       0.500000 |       1.000000 |       0.666667 |       0.718443\n",
      "               mock - vanished |       0.707107 |       0.500000 |       1.000000 |       0.666667 |       0.718443\n",
      "                mock - needful |       0.707107 |       0.500000 |       1.000000 |       0.666667 |       0.718443\n",
      "               parium - suorum |       0.666667 |       0.500000 |       0.666667 |       0.666667 |       0.625000\n",
      "               legale - suorum |       0.666667 |       0.500000 |       0.666667 |       0.666667 |       0.625000\n",
      "               legale - parium |       0.666667 |       0.500000 |       0.666667 |       0.666667 |       0.625000\n",
      "             judicium - suorum |       0.666667 |       0.500000 |       0.666667 |       0.666667 |       0.625000\n",
      "             judicium - parium |       0.666667 |       0.500000 |       0.666667 |       0.666667 |       0.625000\n",
      "             judicium - legale |       0.666667 |       0.500000 |       0.666667 |       0.666667 |       0.625000\n",
      "          falsehood - insulted |       0.577350 |       0.333333 |       1.000000 |       0.500000 |       0.602671\n",
      "             coolidge - hoover |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "               calvin - hoover |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "             calvin - coolidge |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           anciennes - recueil |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              anciennes - lois |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           malaysian - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            mahathir - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          mahathir - malaysian |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "                lois - recueil |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n"
     ]
    }
   ],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print '—'*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print '—'*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Top/Bottom 20 results - Similarity measures - sorted by cosine\n",
    "(From the entire data set)\n",
    "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "                          pair |         cosine |        jaccard |        overlap |           dice |        average\n",
    "---------------------------------------------------------------------------------------------------------------------\n",
    "                   cons - pros |       0.894427 |       0.800000 |       1.000000 |       0.888889 |       0.895829\n",
    "            forties - twenties |       0.816497 |       0.666667 |       1.000000 |       0.800000 |       0.820791\n",
    "                    own - time |       0.809510 |       0.670563 |       0.921168 |       0.802799 |       0.801010\n",
    "                 little - time |       0.784197 |       0.630621 |       0.926101 |       0.773473 |       0.778598\n",
    "                  found - time |       0.783434 |       0.636364 |       0.883788 |       0.777778 |       0.770341\n",
    "                 nova - scotia |       0.774597 |       0.600000 |       1.000000 |       0.750000 |       0.781149\n",
    "                   hong - kong |       0.769800 |       0.615385 |       0.888889 |       0.761905 |       0.758995\n",
    "                   life - time |       0.769666 |       0.608789 |       0.925081 |       0.756829 |       0.765091\n",
    "                  time - world |       0.755476 |       0.585049 |       0.937500 |       0.738209 |       0.754058\n",
    "                  means - time |       0.752181 |       0.587117 |       0.902597 |       0.739854 |       0.745437\n",
    "                   form - time |       0.749943 |       0.588418 |       0.876733 |       0.740885 |       0.738995\n",
    "       infarction - myocardial |       0.748331 |       0.560000 |       1.000000 |       0.717949 |       0.756570\n",
    "                 people - time |       0.745788 |       0.573577 |       0.923875 |       0.729010 |       0.743063\n",
    "                 angeles - los |       0.745499 |       0.586207 |       0.850000 |       0.739130 |       0.730209\n",
    "                  little - own |       0.739343 |       0.585834 |       0.767296 |       0.738834 |       0.707827\n",
    "                    life - own |       0.737053 |       0.582217 |       0.778502 |       0.735951 |       0.708430\n",
    "          anterior - posterior |       0.733388 |       0.576471 |       0.790323 |       0.731343 |       0.707881\n",
    "                  power - time |       0.719611 |       0.533623 |       0.933586 |       0.695898 |       0.720680\n",
    "              dearly - install |       0.707107 |       0.500000 |       1.000000 |       0.666667 |       0.718443\n",
    "                   found - own |       0.704802 |       0.544134 |       0.710949 |       0.704776 |       0.666165\n",
    "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "           arrival - essential |       0.008258 |       0.004098 |       0.009615 |       0.008163 |       0.007534\n",
    "         governments - surface |       0.008251 |       0.003534 |       0.014706 |       0.007042 |       0.008383\n",
    "                king - lesions |       0.008178 |       0.003106 |       0.017857 |       0.006192 |       0.008833\n",
    "              clinical - stood |       0.008178 |       0.003831 |       0.011905 |       0.007634 |       0.007887\n",
    "               till - validity |       0.008172 |       0.003367 |       0.015625 |       0.006711 |       0.008469\n",
    "            evidence - started |       0.008159 |       0.003802 |       0.012048 |       0.007576 |       0.007896\n",
    "               forces - record |       0.008152 |       0.003876 |       0.011364 |       0.007722 |       0.007778\n",
    "               primary - stone |       0.008146 |       0.004065 |       0.009091 |       0.008097 |       0.007350\n",
    "             beneath - federal |       0.008134 |       0.004082 |       0.008403 |       0.008130 |       0.007187\n",
    "                factors - rose |       0.008113 |       0.004032 |       0.009346 |       0.008032 |       0.007381\n",
    "           evening - functions |       0.008069 |       0.004049 |       0.008333 |       0.008065 |       0.007129\n",
    "                   bone - told |       0.008061 |       0.003704 |       0.012346 |       0.007380 |       0.007873\n",
    "             building - occurs |       0.008002 |       0.003891 |       0.010309 |       0.007752 |       0.007489\n",
    "                 company - fig |       0.007913 |       0.003257 |       0.015152 |       0.006494 |       0.008204\n",
    "               chronic - north |       0.007803 |       0.003268 |       0.014493 |       0.006515 |       0.008020\n",
    "             evaluation - king |       0.007650 |       0.003030 |       0.015625 |       0.006042 |       0.008087\n",
    "             resulting - stood |       0.007650 |       0.003663 |       0.010417 |       0.007299 |       0.007257\n",
    "                 agent - round |       0.007515 |       0.003289 |       0.012821 |       0.006557 |       0.007546\n",
    "         afterwards - analysis |       0.007387 |       0.003521 |       0.010204 |       0.007018 |       0.007032\n",
    "            posterior - spirit |       0.007156 |       0.002660 |       0.016129 |       0.005305 |       0.007812"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.6  <a name=\"5.6\"></a> Evaluation of synonyms that your discovered\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\n",
    "In this part of the assignment you will evaluate the success of you synonym detector (developed in response to HW5.4).\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined by your measure in HW5.4, and use the synonyms function in the accompanying python code:\n",
    "\n",
    "nltk_synonyms.py\n",
    "\n",
    "Note: This will require installing the python nltk package:\n",
    "\n",
    "http://www.nltk.org/install.html\n",
    "\n",
    "and downloading its data with nltk.download().\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Calculate performance measures:\n",
    "$$Precision (P) = \\frac{TP}{TP + FP} $$  \n",
    "$$Recall (R) = \\frac{TP}{TP + FN} $$  \n",
    "$$F1 = \\frac{2 * ( precision * recall )}{precision + recall}$$\n",
    "\n",
    "\n",
    "We calculate Precision by counting the number of hits and dividing by the number of occurances in our top1000 (opportunities)   \n",
    "We calculate Recall by counting the number of hits, and dividing by the number of synonyms in wordnet (syns)\n",
    "\n",
    "\n",
    "Other diagnostic measures not implemented here:  https://en.wikipedia.org/wiki/F1_score#Diagnostic_Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Number of Hits: 0 out of top 26\n",
      "Number of words without synonyms: 22\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Precision\t0.0\n",
      "Recall\t\t0.0\n",
      "F1\t\t0.0\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Words without synonyms:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[] parium\n",
      "[] suorum\n",
      "[] legale\n",
      "[] suorum\n",
      "[] legale\n",
      "[] parium\n",
      "[] judicium\n",
      "[] suorum\n",
      "[] judicium\n",
      "[] parium\n",
      "[] judicium\n",
      "[] legale\n",
      "[] anciennes\n",
      "[] recueil\n",
      "[] anciennes\n",
      "[] lois\n",
      "[] mohamad\n",
      "[] mahathir\n",
      "[] mohamad\n",
      "[] mahathir\n",
      "[] lois\n",
      "[] recueil\n"
     ]
    }
   ],
   "source": [
    "''' Performance measures '''\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "\n",
    "#print all the synset element of an element\n",
    "def synonyms(string):\n",
    "    syndict = {}\n",
    "    for i,j in enumerate(wn.synsets(string)):\n",
    "        syns = j.lemma_names()\n",
    "        for syn in syns:\n",
    "            syndict.setdefault(syn,1)\n",
    "    return syndict.keys()\n",
    "hits = []\n",
    "\n",
    "TP = 0\n",
    "FP = 0\n",
    "\n",
    "TOTAL = 0\n",
    "flag = False # so we don't double count, but at the same time don't miss hits\n",
    "start_time = time.time()\n",
    "top1000sims = []\n",
    "with open(\"sims2/top1000sims\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        top1000sims.append(lisst)\n",
    "    \n",
    "\n",
    "measures = {}\n",
    "not_in_wordnet = []\n",
    "\n",
    "for line in top1000sims:\n",
    "    TOTAL += 1\n",
    "\n",
    "    pair = line[0]\n",
    "    words = pair.split(\" - \")\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in measures:\n",
    "            measures[word] = {\"syns\":0,\"opps\": 0,\"hits\":0}\n",
    "        measures[word][\"opps\"] += 1 \n",
    "    \n",
    "    syns0 = synonyms(words[0])\n",
    "    measures[words[1]][\"syns\"] = len(syns0)\n",
    "    if len(syns0) == 0:\n",
    "        not_in_wordnet.append(words[0])\n",
    "        \n",
    "    if words[1] in syns0:\n",
    "        TP += 1\n",
    "        hits.append(line)\n",
    "        flag = True\n",
    "        measures[words[1]][\"hits\"] += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    syns1 = synonyms(words[1]) \n",
    "    measures[words[0]][\"syns\"] = len(syns1)\n",
    "    if len(syns1) == 0:\n",
    "        not_in_wordnet.append(words[1])\n",
    "\n",
    "    if words[0] in syns1:\n",
    "        if flag == False:\n",
    "            TP += 1\n",
    "            hits.append(line)\n",
    "            measures[words[0]][\"hits\"] += 1\n",
    "            \n",
    "    flag = False    \n",
    "\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for key in measures:\n",
    "    p,r,f = 0,0,0\n",
    "    if measures[key][\"hits\"] > 0 and measures[key][\"syns\"] > 0:\n",
    "        p = measures[key][\"hits\"]/measures[key][\"opps\"]\n",
    "        r = measures[key][\"hits\"]/measures[key][\"syns\"]\n",
    "        f = 2 * (p*r)/(p+r)\n",
    "    \n",
    "    # For calculating measures, only take into account words that have synonyms in wordnet\n",
    "    if measures[key][\"syns\"] > 0:\n",
    "        precision.append(p)\n",
    "        recall.append(r)\n",
    "        f1.append(f)\n",
    "\n",
    "    \n",
    "# Take the mean of each measure    \n",
    "print \"—\"*110    \n",
    "print \"Number of Hits:\",TP, \"out of top\",TOTAL\n",
    "print \"Number of words without synonyms:\",len(not_in_wordnet)\n",
    "print \"—\"*110 \n",
    "print \"Precision\\t\", np.mean(precision)\n",
    "print \"Recall\\t\\t\", np.mean(recall)\n",
    "print \"F1\\t\\t\", np.mean(f1)\n",
    "print \"—\"*110  \n",
    "\n",
    "print \"Words without synonyms:\"\n",
    "print \"-\"*100\n",
    "\n",
    "for word in not_in_wordnet:\n",
    "    print synonyms(word),word\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "mins = elapsed_time/float(60)\n",
    "a = \"\"\"\n",
    "Elapsed time: %s seconds\n",
    "In minutes: %s mins\"\"\" % (str(elapsed_time), str(mins))\n",
    "logging.warning(a)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "Number of Hits: 31 out of top 1000\n",
    "Number of words without synonyms: 67\n",
    "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "Precision\t0.0280214404967\n",
    "Recall\t\t0.0178598869579\n",
    "F1\t\t0.013965517619\n",
    "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "Words without synonyms:\n",
    "----------------------------------------------------------------------------------------------------\n",
    "[] scotia\n",
    "[] hong\n",
    "[] kong\n",
    "[] angeles\n",
    "[] los\n",
    "[] nor\n",
    "[] themselves\n",
    "[] \n",
    "......."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.7  <a name=\"5.7\"></a> OPTIONAL: using different vocabulary subsets\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\n",
    "Repeat HW5 using vocabulary words ranked from 8001,-10,000;  7001,-10,000; 6001,-10,000; 5001,-10,000; 3001,-10,000; and 1001,-10,000;\n",
    "Dont forget to report you Cluster configuration.\n",
    "\n",
    "Generate the following graphs:\n",
    "-- vocabulary size (X-Axis) versus CPU time for indexing\n",
    "-- vocabulary size (X-Axis) versus number of pairs processed\n",
    "-- vocabulary size (X-Axis) versus F1 measure, Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7: Vocabulary subset 8001-10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `frequencies5.5': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/frequencies5_5.root.20170617.070245.063231\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/frequencies5_5.root.20170617.070245.063231/files/...\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 2...\n",
      "  mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob4219690725143209880.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497651454196_0021\n",
      "  Submitted application application_1497651454196_0021\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1497651454196_0021/\n",
      "  Running job: job_1497651454196_0021\n",
      "  Job job_1497651454196_0021 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497651454196_0021 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/frequencies5_5.root.20170617.070245.063231/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=11448710\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=538130\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=901704\n",
      "\t\tFILE: Number of bytes written=2160242\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=11449116\n",
      "\t\tHDFS: Number of bytes written=538130\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=28674048\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3456000\n",
      "\t\tTotal time spent by all map tasks (ms)=28002\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=28002\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3375\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3375\n",
      "\t\tTotal vcore-seconds taken by all map tasks=28002\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3375\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=11540\n",
      "\t\tCombine input records=1558070\n",
      "\t\tCombine output records=54102\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=140\n",
      "\t\tInput split bytes=406\n",
      "\t\tMap input records=311614\n",
      "\t\tMap output bytes=18210060\n",
      "\t\tMap output materialized bytes=901710\n",
      "\t\tMap output records=1558070\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=775692288\n",
      "\t\tReduce input groups=36353\n",
      "\t\tReduce input records=54102\n",
      "\t\tReduce output records=36353\n",
      "\t\tReduce shuffle bytes=901710\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=108204\n",
      "\t\tTotal committed heap usage (bytes)=836763648\n",
      "\t\tVirtual memory (bytes) snapshot=4085370880\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 2 of 2...\n",
      "  mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "  mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "  mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "  mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob8051247399245576914.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497651454196_0022\n",
      "  Submitted application application_1497651454196_0022\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1497651454196_0022/\n",
      "  Running job: job_1497651454196_0022\n",
      "  Job job_1497651454196_0022 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497651454196_0022 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/frequencies5_5.root.20170617.070245.063231/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=542226\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=30033\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=647195\n",
      "\t\tFILE: Number of bytes written=1650609\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=542574\n",
      "\t\tHDFS: Number of bytes written=30033\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6435840\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2970624\n",
      "\t\tTotal time spent by all map tasks (ms)=6285\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6285\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2901\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2901\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6285\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2901\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2840\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=81\n",
      "\t\tInput split bytes=348\n",
      "\t\tMap input records=36353\n",
      "\t\tMap output bytes=574483\n",
      "\t\tMap output materialized bytes=647201\n",
      "\t\tMap output records=36353\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=757231616\n",
      "\t\tReduce input groups=36353\n",
      "\t\tReduce input records=36353\n",
      "\t\tReduce output records=2000\n",
      "\t\tReduce shuffle bytes=647201\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=72706\n",
      "\t\tTotal committed heap usage (bytes)=757596160\n",
      "\t\tVirtual memory (bytes) snapshot=4097490944\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/frequencies5_5.root.20170617.070245.063231/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/frequencies5_5.root.20170617.070245.063231...\n",
      "Removing temp directory /tmp/frequencies5_5.root.20170617.070245.063231...\n",
      "WARNING:root:Elapsed time: 62.8730959892 seconds\n",
      "    In minutes: 1.04788493315 mins\n",
      "rm: `stripes5.5': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/stripes5_5.root.20170617.070350.152019\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/stripes5_5.root.20170617.070350.152019/files/...\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob2492567313006951298.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497651454196_0023\n",
      "  Submitted application application_1497651454196_0023\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1497651454196_0023/\n",
      "  Running job: job_1497651454196_0023\n",
      "  Job job_1497651454196_0023 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497651454196_0023 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/stripes5_5.root.20170617.070350.152019/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=11448710\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=17958\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=23752\n",
      "\t\tFILE: Number of bytes written=403999\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=11449108\n",
      "\t\tHDFS: Number of bytes written=17958\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8050688\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2360320\n",
      "\t\tTotal time spent by all map tasks (ms)=7862\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7862\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2305\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2305\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7862\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2305\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1510\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=99\n",
      "\t\tInput split bytes=398\n",
      "\t\tMap input records=311614\n",
      "\t\tMap output bytes=22314\n",
      "\t\tMap output materialized bytes=23758\n",
      "\t\tMap output records=716\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=759828480\n",
      "\t\tReduce input groups=714\n",
      "\t\tReduce input records=716\n",
      "\t\tReduce output records=567\n",
      "\t\tReduce shuffle bytes=23758\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=1432\n",
      "\t\tTotal committed heap usage (bytes)=833617920\n",
      "\t\tVirtual memory (bytes) snapshot=4077256704\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/stripes5_5.root.20170617.070350.152019/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/stripes5_5.root.20170617.070350.152019...\n",
      "Removing temp directory /tmp/stripes5_5.root.20170617.070350.152019...\n",
      "rm: `index5.5': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/index5_5.root.20170617.070429.023253\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/index5_5.root.20170617.070429.023253/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob1476262892082415966.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497651454196_0024\n",
      "  Submitted application application_1497651454196_0024\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1497651454196_0024/\n",
      "  Running job: job_1497651454196_0024\n",
      "  Job job_1497651454196_0024 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497651454196_0024 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/index5_5.root.20170617.070429.023253/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=22054\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=16996\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=20128\n",
      "\t\tFILE: Number of bytes written=393781\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=22368\n",
      "\t\tHDFS: Number of bytes written=16996\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=5874688\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2323456\n",
      "\t\tTotal time spent by all map tasks (ms)=5737\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5737\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2269\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2269\n",
      "\t\tTotal vcore-seconds taken by all map tasks=5737\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2269\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1220\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=58\n",
      "\t\tInput split bytes=314\n",
      "\t\tMap input records=567\n",
      "\t\tMap output bytes=18690\n",
      "\t\tMap output materialized bytes=20134\n",
      "\t\tMap output records=716\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=728055808\n",
      "\t\tReduce input groups=567\n",
      "\t\tReduce input records=716\n",
      "\t\tReduce output records=567\n",
      "\t\tReduce shuffle bytes=20134\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=1432\n",
      "\t\tTotal committed heap usage (bytes)=794296320\n",
      "\t\tVirtual memory (bytes) snapshot=4082741248\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/index5_5.root.20170617.070429.023253/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/index5_5.root.20170617.070429.023253...\n",
      "Removing temp directory /tmp/index5_5.root.20170617.070429.023253...\n",
      "rm: `similarity5.5': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/similarity5_5.root.20170617.070503.839752\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/similarity5_5.root.20170617.070503.839752/files/...\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 2...\n",
      "  mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob3473701266537159735.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497651454196_0025\n",
      "  Submitted application application_1497651454196_0025\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1497651454196_0025/\n",
      "  Running job: job_1497651454196_0025\n",
      "  Job job_1497651454196_0025 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497651454196_0025 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/similarity5_5.root.20170617.070503.839752/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=21092\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=15390\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6089\n",
      "\t\tFILE: Number of bytes written=367404\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=21412\n",
      "\t\tHDFS: Number of bytes written=15390\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6180864\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2324480\n",
      "\t\tTotal time spent by all map tasks (ms)=6036\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6036\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2270\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2270\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6036\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2270\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1120\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=67\n",
      "\t\tInput split bytes=320\n",
      "\t\tMap input records=567\n",
      "\t\tMap output bytes=5727\n",
      "\t\tMap output materialized bytes=6095\n",
      "\t\tMap output records=178\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=733151232\n",
      "\t\tReduce input groups=166\n",
      "\t\tReduce input records=178\n",
      "\t\tReduce output records=166\n",
      "\t\tReduce shuffle bytes=6095\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=356\n",
      "\t\tTotal committed heap usage (bytes)=794296320\n",
      "\t\tVirtual memory (bytes) snapshot=4074246144\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 2 of 2...\n",
      "  mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "  mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "  mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "  mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob2226960022529566800.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497651454196_0026\n",
      "  Submitted application application_1497651454196_0026\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1497651454196_0026/\n",
      "  Running job: job_1497651454196_0026\n",
      "  Job job_1497651454196_0026 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497651454196_0026 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/similarity5_5.root.20170617.070503.839752/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=19486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=15390\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=15900\n",
      "\t\tFILE: Number of bytes written=387866\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=19832\n",
      "\t\tHDFS: Number of bytes written=15390\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=5295104\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2376704\n",
      "\t\tTotal time spent by all map tasks (ms)=5171\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5171\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2321\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2321\n",
      "\t\tTotal vcore-seconds taken by all map tasks=5171\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2321\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1180\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=67\n",
      "\t\tInput split bytes=346\n",
      "\t\tMap input records=166\n",
      "\t\tMap output bytes=15559\n",
      "\t\tMap output materialized bytes=15906\n",
      "\t\tMap output records=166\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=706113536\n",
      "\t\tReduce input groups=166\n",
      "\t\tReduce input records=166\n",
      "\t\tReduce output records=166\n",
      "\t\tReduce shuffle bytes=15906\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=332\n",
      "\t\tTotal committed heap usage (bytes)=681050112\n",
      "\t\tVirtual memory (bytes) snapshot=4101160960\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/similarity5_5.root.20170617.070503.839752/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/similarity5_5.root.20170617.070503.839752...\n",
      "Removing temp directory /tmp/similarity5_5.root.20170617.070503.839752...\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r frequencies5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5\n",
    "!python frequencies5_5.py --min_rank 8001 --max_rank 10000 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > frequencies5.5\n",
    "\n",
    "!hdfs dfs -rm -r stripes5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python stripes5_5.py --file=frequencies5.5 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > stripes5.5\n",
    "\n",
    "!hdfs dfs -rm -r index5.5\n",
    "!python index5_5.py -r hadoop stripes5.5 > index5.5\n",
    "\n",
    "!hdfs dfs -rm -r similarity5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python similarity5_5.py -r hadoop index5.5 > similarity5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sortedSims = []\n",
    "with open(\"similarity5.5\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        sortedSims.append(lisst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `sims2': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir sims2\n",
    "!head -1000 similarity5.5 > sims2/top1000sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top/Bottom 20 results - Similarity measures - sorted by cosine\n",
      "(From the entire data set)\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "                          pair |         cosine |        jaccard |        overlap |           dice |        average\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "           iliac - replacement |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         replacement - rupture |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           hampshire - hungary |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "       greenhouse - tomography |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           grazing - livestock |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            gracious - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           gracious - pendulum |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                   genre - que |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "        precaution - resultant |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         preamble - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           preamble - sweeping |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "               plunged - slain |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            planting - players |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "             pilgrim - proudly |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            pendulum - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            offender - schemes |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "          newborn - saturation |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         sweeping - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            needful - vanished |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                 nazis - swift |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "               alpha - epsilon |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - neuropathy |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "     subordination - vengeance |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              seas - wandering |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              pious - syllable |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "       neuropathy - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           malaysian - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            mahathir - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          mahathir - malaysian |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "                lois - recueil |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            ingenuity - wretch |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           infected - infusion |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "               hers - paternal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           fossil - resembling |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          sensory - trigeminal |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "           maxillary - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "        breathed - persistence |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "            diabetic - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "                anne - prussia |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n"
     ]
    }
   ],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print '—'*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print '—'*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7: Vocabulary subset 7001-10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r frequencies5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5\n",
    "!python frequencies5_5.py --min_rank 7001 --max_rank 10000 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > frequencies5.5\n",
    "\n",
    "!hdfs dfs -rm -r stripes5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python stripes5_5.py --file=frequencies5.5 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > stripes5.5\n",
    "\n",
    "!hdfs dfs -rm -r index5.5\n",
    "!python index5_5.py -r hadoop stripes5.5 > index5.5\n",
    "\n",
    "!hdfs dfs -rm -r similarity5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python similarity5_5.py -r hadoop index5.5 > similarity5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sortedSims = []\n",
    "with open(\"similarity5.5\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        sortedSims.append(lisst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `sims2': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir sims2\n",
    "!head -1000 similarity5.5 > sims2/top1000sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top/Bottom 20 results - Similarity measures - sorted by cosine\n",
      "(From the entire data set)\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "                          pair |         cosine |        jaccard |        overlap |           dice |        average\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "           iliac - replacement |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         replacement - rupture |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           hampshire - hungary |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "       greenhouse - tomography |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           grazing - livestock |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            gracious - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           gracious - pendulum |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                   genre - que |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "        precaution - resultant |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         preamble - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           preamble - sweeping |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "               plunged - slain |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            planting - players |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "             pilgrim - proudly |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            pendulum - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            offender - schemes |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "          newborn - saturation |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         sweeping - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            needful - vanished |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                 nazis - swift |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "               alpha - epsilon |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - neuropathy |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "     subordination - vengeance |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              seas - wandering |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              pious - syllable |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "       neuropathy - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           malaysian - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            mahathir - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          mahathir - malaysian |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "                lois - recueil |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            ingenuity - wretch |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           infected - infusion |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "               hers - paternal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           fossil - resembling |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          sensory - trigeminal |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "           maxillary - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "        breathed - persistence |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "            diabetic - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "                anne - prussia |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n"
     ]
    }
   ],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print '—'*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print '—'*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7: Vocabulary subset 6001-10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r frequencies5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5\n",
    "!python frequencies5_5.py --min_rank 6001 --max_rank 10000 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > frequencies5.5\n",
    "\n",
    "!hdfs dfs -rm -r stripes5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python stripes5_5.py --file=frequencies5.5 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > stripes5.5\n",
    "\n",
    "!hdfs dfs -rm -r index5.5\n",
    "!python index5_5.py -r hadoop stripes5.5 > index5.5\n",
    "\n",
    "!hdfs dfs -rm -r similarity5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python similarity5_5.py -r hadoop index5.5 > similarity5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sortedSims = []\n",
    "with open(\"similarity5.5\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        sortedSims.append(lisst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `sims2': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir sims2\n",
    "!head -1000 similarity5.5 > sims2/top1000sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top/Bottom 20 results - Similarity measures - sorted by cosine\n",
      "(From the entire data set)\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "                          pair |         cosine |        jaccard |        overlap |           dice |        average\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "           iliac - replacement |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         replacement - rupture |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           hampshire - hungary |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "       greenhouse - tomography |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           grazing - livestock |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            gracious - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           gracious - pendulum |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                   genre - que |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "        precaution - resultant |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         preamble - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           preamble - sweeping |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "               plunged - slain |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            planting - players |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "             pilgrim - proudly |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            pendulum - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            offender - schemes |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "          newborn - saturation |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         sweeping - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            needful - vanished |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                 nazis - swift |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "               alpha - epsilon |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - neuropathy |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "     subordination - vengeance |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              seas - wandering |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              pious - syllable |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "       neuropathy - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           malaysian - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            mahathir - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          mahathir - malaysian |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "                lois - recueil |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            ingenuity - wretch |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           infected - infusion |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "               hers - paternal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           fossil - resembling |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          sensory - trigeminal |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "           maxillary - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "        breathed - persistence |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "            diabetic - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "                anne - prussia |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n"
     ]
    }
   ],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print '—'*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print '—'*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7: Vocabulary subset 5001-10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r frequencies5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5\n",
    "!python frequencies5_5.py --min_rank 5001 --max_rank 10000 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > frequencies5.5\n",
    "\n",
    "!hdfs dfs -rm -r stripes5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python stripes5_5.py --file=frequencies5.5 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > stripes5.5\n",
    "\n",
    "!hdfs dfs -rm -r index5.5\n",
    "!python index5_5.py -r hadoop stripes5.5 > index5.5\n",
    "\n",
    "!hdfs dfs -rm -r similarity5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python similarity5_5.py -r hadoop index5.5 > similarity5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sortedSims = []\n",
    "with open(\"similarity5.5\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        sortedSims.append(lisst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `sims2': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir sims2\n",
    "!head -1000 similarity5.5 > sims2/top1000sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top/Bottom 20 results - Similarity measures - sorted by cosine\n",
      "(From the entire data set)\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "                          pair |         cosine |        jaccard |        overlap |           dice |        average\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "           iliac - replacement |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         replacement - rupture |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           hampshire - hungary |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "       greenhouse - tomography |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           grazing - livestock |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            gracious - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           gracious - pendulum |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                   genre - que |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "        precaution - resultant |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         preamble - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           preamble - sweeping |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "               plunged - slain |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            planting - players |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "             pilgrim - proudly |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            pendulum - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            offender - schemes |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "          newborn - saturation |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         sweeping - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            needful - vanished |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                 nazis - swift |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "               alpha - epsilon |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - neuropathy |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "     subordination - vengeance |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              seas - wandering |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              pious - syllable |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "       neuropathy - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           malaysian - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            mahathir - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          mahathir - malaysian |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "                lois - recueil |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            ingenuity - wretch |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           infected - infusion |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "               hers - paternal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           fossil - resembling |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          sensory - trigeminal |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "           maxillary - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "        breathed - persistence |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "            diabetic - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "                anne - prussia |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n"
     ]
    }
   ],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print '—'*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print '—'*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7: Vocabulary subset 4001-10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r frequencies5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5\n",
    "!python frequencies5_5.py --min_rank 4001 --max_rank 10000 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > frequencies5.5\n",
    "\n",
    "!hdfs dfs -rm -r stripes5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python stripes5_5.py --file=frequencies5.5 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > stripes5.5\n",
    "\n",
    "!hdfs dfs -rm -r index5.5\n",
    "!python index5_5.py -r hadoop stripes5.5 > index5.5\n",
    "\n",
    "!hdfs dfs -rm -r similarity5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python similarity5_5.py -r hadoop index5.5 > similarity5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sortedSims = []\n",
    "with open(\"similarity5.5\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        sortedSims.append(lisst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `sims2': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir sims2\n",
    "!head -1000 similarity5.5 > sims2/top1000sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top/Bottom 20 results - Similarity measures - sorted by cosine\n",
      "(From the entire data set)\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "                          pair |         cosine |        jaccard |        overlap |           dice |        average\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "           iliac - replacement |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         replacement - rupture |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           hampshire - hungary |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "       greenhouse - tomography |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           grazing - livestock |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            gracious - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           gracious - pendulum |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                   genre - que |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "        precaution - resultant |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         preamble - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           preamble - sweeping |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "               plunged - slain |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            planting - players |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "             pilgrim - proudly |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            pendulum - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            offender - schemes |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "          newborn - saturation |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         sweeping - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            needful - vanished |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                 nazis - swift |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "               alpha - epsilon |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - neuropathy |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "     subordination - vengeance |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              seas - wandering |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              pious - syllable |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "       neuropathy - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           malaysian - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            mahathir - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          mahathir - malaysian |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "                lois - recueil |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            ingenuity - wretch |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           infected - infusion |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "               hers - paternal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           fossil - resembling |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          sensory - trigeminal |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "           maxillary - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "        breathed - persistence |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "            diabetic - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "                anne - prussia |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n"
     ]
    }
   ],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print '—'*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print '—'*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7: Vocabulary subset 3001-10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r frequencies5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5\n",
    "!python frequencies5_5.py --min_rank 3001 --max_rank 10000 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > frequencies5.5\n",
    "\n",
    "!hdfs dfs -rm -r stripes5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python stripes5_5.py --file=frequencies5.5 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > stripes5.5\n",
    "\n",
    "!hdfs dfs -rm -r index5.5\n",
    "!python index5_5.py -r hadoop stripes5.5 > index5.5\n",
    "\n",
    "!hdfs dfs -rm -r similarity5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python similarity5_5.py -r hadoop index5.5 > similarity5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sortedSims = []\n",
    "with open(\"similarity5.5\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        sortedSims.append(lisst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `sims2': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir sims2\n",
    "!head -1000 similarity5.5 > sims2/top1000sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top/Bottom 20 results - Similarity measures - sorted by cosine\n",
      "(From the entire data set)\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "                          pair |         cosine |        jaccard |        overlap |           dice |        average\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "           iliac - replacement |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         replacement - rupture |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           hampshire - hungary |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "       greenhouse - tomography |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           grazing - livestock |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            gracious - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           gracious - pendulum |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                   genre - que |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "        precaution - resultant |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         preamble - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           preamble - sweeping |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "               plunged - slain |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            planting - players |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "             pilgrim - proudly |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            pendulum - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            offender - schemes |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "          newborn - saturation |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         sweeping - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            needful - vanished |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                 nazis - swift |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "               alpha - epsilon |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - neuropathy |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "     subordination - vengeance |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              seas - wandering |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              pious - syllable |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "       neuropathy - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           malaysian - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            mahathir - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          mahathir - malaysian |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "                lois - recueil |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            ingenuity - wretch |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           infected - infusion |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "               hers - paternal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           fossil - resembling |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          sensory - trigeminal |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "           maxillary - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "        breathed - persistence |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "            diabetic - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "                anne - prussia |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n"
     ]
    }
   ],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print '—'*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print '—'*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7: Vocabulary subset 2001-10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r frequencies5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5\n",
    "!python frequencies5_5.py --min_rank 2001 --max_rank 10000 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > frequencies5.5\n",
    "\n",
    "!hdfs dfs -rm -r stripes5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python stripes5_5.py --file=frequencies5.5 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > stripes5.5\n",
    "\n",
    "!hdfs dfs -rm -r index5.5\n",
    "!python index5_5.py -r hadoop stripes5.5 > index5.5\n",
    "\n",
    "!hdfs dfs -rm -r similarity5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python similarity5_5.py -r hadoop index5.5 > similarity5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sortedSims = []\n",
    "with open(\"similarity5.5\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        sortedSims.append(lisst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `sims2': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir sims2\n",
    "!head -1000 similarity5.5 > sims2/top1000sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top/Bottom 20 results - Similarity measures - sorted by cosine\n",
      "(From the entire data set)\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "                          pair |         cosine |        jaccard |        overlap |           dice |        average\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "           iliac - replacement |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         replacement - rupture |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           hampshire - hungary |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "       greenhouse - tomography |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           grazing - livestock |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            gracious - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           gracious - pendulum |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                   genre - que |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "        precaution - resultant |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         preamble - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           preamble - sweeping |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "               plunged - slain |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            planting - players |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "             pilgrim - proudly |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            pendulum - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            offender - schemes |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "          newborn - saturation |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         sweeping - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            needful - vanished |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                 nazis - swift |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "               alpha - epsilon |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - neuropathy |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "     subordination - vengeance |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              seas - wandering |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              pious - syllable |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "       neuropathy - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           malaysian - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            mahathir - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          mahathir - malaysian |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "                lois - recueil |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            ingenuity - wretch |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           infected - infusion |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "               hers - paternal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           fossil - resembling |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          sensory - trigeminal |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "           maxillary - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "        breathed - persistence |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "            diabetic - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "                anne - prussia |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n"
     ]
    }
   ],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print '—'*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print '—'*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7: Vocabulary subset 1001-10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r frequencies5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5\n",
    "!python frequencies5_5.py --min_rank 1001 --max_rank 10000 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > frequencies5.5\n",
    "\n",
    "!hdfs dfs -rm -r stripes5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python stripes5_5.py --file=frequencies5.5 -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered.txt > stripes5.5\n",
    "\n",
    "!hdfs dfs -rm -r index5.5\n",
    "!python index5_5.py -r hadoop stripes5.5 > index5.5\n",
    "\n",
    "!hdfs dfs -rm -r similarity5.5\n",
    "#!python frequencies5_5.py -r hadoop hdfs:///user/cendylin/filtered-5Grams/ > frequencies5.5'\n",
    "!python similarity5_5.py -r hadoop index5.5 > similarity5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sortedSims = []\n",
    "with open(\"similarity5.5\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        sortedSims.append(lisst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `sims2': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir sims2\n",
    "!head -1000 similarity5.5 > sims2/top1000sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top/Bottom 20 results - Similarity measures - sorted by cosine\n",
      "(From the entire data set)\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "                          pair |         cosine |        jaccard |        overlap |           dice |        average\n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "           iliac - replacement |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         replacement - rupture |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           hampshire - hungary |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "       greenhouse - tomography |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           grazing - livestock |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            gracious - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           gracious - pendulum |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                   genre - que |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "        precaution - resultant |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         preamble - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "           preamble - sweeping |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "               plunged - slain |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            planting - players |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "             pilgrim - proudly |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            pendulum - privacy |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            offender - schemes |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "          newborn - saturation |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "         sweeping - toleration |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "            needful - vanished |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "                 nazis - swift |       1.000000 |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "               alpha - epsilon |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - neuropathy |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "     subordination - vengeance |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              seas - wandering |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "              pious - syllable |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "       neuropathy - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "        maxillary - trigeminal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           malaysian - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            mahathir - mohamad |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          mahathir - malaysian |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "                lois - recueil |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "            ingenuity - wretch |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           infected - infusion |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "               hers - paternal |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "           fossil - resembling |       0.500000 |       0.333333 |       0.500000 |       0.500000 |       0.458333\n",
      "          sensory - trigeminal |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "           maxillary - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "        breathed - persistence |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "            diabetic - sensory |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n",
      "                anne - prussia |       0.408248 |       0.250000 |       0.500000 |       0.400000 |       0.389562\n"
     ]
    }
   ],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print '—'*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print '—'*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.8  <a name=\"5.8\"></a> OPTIONAL: filter stopwords\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "There is also a corpus of stopwords, that is, high-frequency words like \"the\", \"to\" and \"also\" that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts. Python's nltk comes with a prebuilt list of stopwords (see below). Using this stopword list filter out these tokens from your analysis and rerun the experiments in 5.5 and disucuss the results of using a stopword list and without using a stopword list.\n",
    "\n",
    "> from nltk.corpus import stopwords\n",
    ">> stopwords.words('english')\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.9 <a name=\"5.9\"></a> OPTIONAL \n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "There are many good ways to build our synonym detectors, so for this optional homework, \n",
    "measure co-occurrence by (left/right/all) consecutive words only, \n",
    "or make stripes according to word co-occurrences with the accompanying \n",
    "2-, 3-, or 4-grams (note here that your output will no longer \n",
    "be interpretable as a network) inside of the 5-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.10 <a name=\"5.10\"></a> OPTIONAL \n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Once again, benchmark your top 10,000 associations (as in 5.5), this time for your\n",
    "results from 5.6. Has your detector improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "511px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
